<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[如何简单的给静态博客网站用上HTTPS]]></title>
    <url>%2Fposts%2F51%2F</url>
    <content type="text"><![CDATA[简介谷歌浏览器已经把没有使用HTTPS协议的网站全部标记为不安全，为了让浏览器不把我们的网站标记为不安全，为了让访问者能更安心的浏览我们的网站，我们只能为我们的网站开启HTTPS协议支持，由于 Let’s Encrypt 可以为我们提供免费证书使用，我们就可以免费的为我们的网站申请证书，但是由于证书有效期只有3个月，当证书快过期时，我们需要重新续签。为此有不少工具能帮助我们完成自助的申请证书以及续签。但是这仍然需要不少的配置。这时一个名为 Caddy 的跨开台开源软件出现了，Caddy 能自动帮我们申请证书，当证书快过期时自动续签，只需一次配置，后续基本不需要再做其他配置，非常的方便，完全可以代替 Nginx 与 Apache 。 安装去 Caddy 的 Github 发布页下载对应平台的二进制压缩包 https://github.com/caddyserver/caddy/releases 解压安装 123tar xf caddy_v1.0.3_linux_amd64.tar.gzmv caddy /usr/local/bincaddy -version 配置 Caddy 把如下的配置文件写入名为 Caddyfile 的文件中。 12345678910mgxian.dev &#123; root /data/blog/mgxian gzip log ./access.log&#125;www.mgxian.dev &#123; redir https://mgxian.dev&#123;uri&#125; log ./access.log&#125; 1-5 为 mgxian.dev 域名的相关配置，指定根目录为 /data/blog/mgxian ，开启 gzip 压缩，并把访问日志记录在当前目录的 access.log 文件中。 7-10 为 www.mgxian.dev 域名的相关配置，表示当访问 www.mgxian.dev 域名的相关资源时，将会自动跳转到 mgxian.dev 域名。 启动 Caddy 启动 Caddy 之后会自动申请 HTTPS 的证书，并自动开启对 HTTP2 协议的支持，启动过程中可能会提示你输入邮箱接受证书相关的提醒。 1caddy -conf Caddyfile 注意事项 在配置使用 Caddy 之前请确保 DNS 解析配置正常，请把你需要配置的域名解析到你安装配置 Caddy 机器的外网 IP 上。 如果需要申请支持通配符的证书，如为 *.mgxian.dev 申请证书，需要使用 DNS Challenge 的方式来申请证书，具体详细文档可参考 DNS Challenge 。 Caddy 不仅可以作为一个像 Apache 与 Nginx 一样的 Web Server ，也可以配置为反向代理，代理后端 Apache 与 Nginx 等应用。 访问测试 启动完成后，使用 cURL 访问你的网站域名进行测试。 123456789curl -I https://mgxian.dev/HTTP/1.1 200 OKAccept-Ranges: bytesContent-Length: 64846Content-Type: text/html; charset=utf-8Etag: "pwsec41e1a"Last-Modified: Sun, 25 Aug 2019 09:57:40 GMTServer: CaddyDate: Sun, 25 Aug 2019 09:59:27 GMT 参考文档 https://github.com/caddyserver/caddy https://caddyserver.com/docs]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>proxy</tag>
        <tag>caddy</tag>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个可供参考的 git commit message 规范]]></title>
    <url>%2Fposts%2F50%2F</url>
    <content type="text"><![CDATA[一个可供参考的 git commit message 规范]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7升级内核]]></title>
    <url>%2Fposts%2F49%2F</url>
    <content type="text"><![CDATA[配置yum源1234567# 安装yum源rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.orgrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm# 查看列表yum --disablerepo=* --enablerepo=elrepo-kernel repolistyum --disablerepo=* --enablerepo=elrepo-kernel list kernel* 安装最新版本的kernel12# 安装yum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -y 设置为默认内核123# 设置生成新的grubgrub2-set-default 0grub2-mkconfig -o /etc/grub2.cfg 安装新版本工具包（可省略）12345# 移除旧版本yum remove kernel-tools-libs.x86_64 kernel-tools.x86_64# 安装新版本yum --disablerepo=* --enablerepo=elrepo-kernel install -y kernel-ml-tools.x86_64 重启查看内核版本12345# 重启reboot# 查看内核版本uname -sr 参考文档 https://www.tecmint.com/install-upgrade-kernel-version-in-centos-7/ https://www.centos.bz/2017/08/upgrade-centos-7-6-kernel-to-4-12-4/]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>kernel</tag>
        <tag>centos</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s使用ceph实现动态持久化存储]]></title>
    <url>%2Fposts%2F48%2F</url>
    <content type="text"><![CDATA[简介本文章介绍如何使用ceph为k8s提供动态申请pv的功能。ceph提供底层存储功能，cephfs方式支持k8s的pv的3种访问模式ReadWriteOnce，ReadOnlyMany ，ReadWriteMany ，RBD支持ReadWriteOnce，ReadOnlyMany两种模式访问模式只是能力描述，并不是强制执行的，对于没有按pvc声明的方式使用pv，存储提供者应该负责访问时的运行错误。例如如果设置pvc的访问模式为ReadOnlyMany ，pod挂载后依然可写，如果需要真正的不可写，申请pvc是需要指定 readOnly: true 参数 部署部署k8scentos7使用kubeadm安装k8s-1.11版本 部署cephcentos7安装ceph分布式存储集群 在k8s集群中配置使用ceph使用Ceph RBD使用kubeadm安装集群的额外配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103# 如果使用kubeadm部署的集群需要这些额外的步骤# 由于使用动态存储时 controller-manager 需要使用 rbd 命令创建 image# 所以 controller-manager 需要使用 rbd 命令# 由于官方controller-manager镜像里没有rbd命令# 如果没使用如下方式会报错无法成功创建pvc# 相关 issue https://github.com/kubernetes/kubernetes/issues/38923cat &gt;external-storage-rbd-provisioner.yaml&lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: rbd-provisioner namespace: kube-system---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: rbd-provisionerrules: - apiGroups: [""] resources: ["persistentvolumes"] verbs: ["get", "list", "watch", "create", "delete"] - apiGroups: [""] resources: ["persistentvolumeclaims"] verbs: ["get", "list", "watch", "update"] - apiGroups: ["storage.k8s.io"] resources: ["storageclasses"] verbs: ["get", "list", "watch"] - apiGroups: [""] resources: ["events"] verbs: ["create", "update", "patch"] - apiGroups: [""] resources: ["endpoints"] verbs: ["get", "list", "watch", "create", "update", "patch"] - apiGroups: [""] resources: ["services"] resourceNames: ["kube-dns"] verbs: ["list", "get"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: rbd-provisionersubjects: - kind: ServiceAccount name: rbd-provisioner namespace: kube-systemroleRef: kind: ClusterRole name: rbd-provisioner apiGroup: rbac.authorization.k8s.io---apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: rbd-provisioner namespace: kube-systemrules:- apiGroups: [""] resources: ["secrets"] verbs: ["get"]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: rbd-provisioner namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: rbd-provisionersubjects:- kind: ServiceAccount name: rbd-provisioner namespace: kube-system---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: rbd-provisioner namespace: kube-systemspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: rbd-provisioner spec: containers: - name: rbd-provisioner image: "quay.io/external_storage/rbd-provisioner:v2.1.1-k8s1.11" env: - name: PROVISIONER_NAME value: ceph.com/rbd serviceAccount: rbd-provisionerEOFkubectl apply -f external-storage-rbd-provisioner.yaml# 查看状态 等待running之后 再进行后续的操作kubectl get pod -n kube-system 配置 storageclass12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# 在k8s集群中所有节点安装 ceph-common# 需要使用kubelet使用rdb命令map附加rbd创建的imageyum install -y ceph-common# 创建 osd pool 在ceph的mon或者admin节点ceph osd pool create kube 4096ceph osd pool ls# 创建k8s访问ceph的用户 在ceph的mon或者admin节点ceph auth get-or-create client.kube mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=kube' -o ceph.client.kube.keyring# 查看key 在ceph的mon或者admin节点ceph auth get-key client.adminceph auth get-key client.kube# 创建 admin secret# CEPH_ADMIN_SECRET 替换为 client.admin 获取到的keyexport CEPH_ADMIN_SECRET='AQBBAnRbSiSOFxAAEZXNMzYV6hsceccYLhzdWw=='kubectl create secret generic ceph-secret --type="kubernetes.io/rbd" \--from-literal=key=$CEPH_ADMIN_SECRET \--namespace=kube-system# 在 default 命名空间创建pvc用于访问ceph的 secret# CEPH_KUBE_SECRET 替换为 client.kube 获取到的keyexport CEPH_KUBE_SECRET='AQBZK3VbTN/QOBAAIYi6CRLQcVevW5HM8lunOg=='kubectl create secret generic ceph-user-secret --type="kubernetes.io/rbd" \--from-literal=key=$CEPH_KUBE_SECRET \--namespace=default# 查看 secretkubectl get secret ceph-user-secret -o yamlkubectl get secret ceph-secret -n kube-system -o yaml# 配置 StorageClass# 如果使用kubeadm创建的集群 provisioner 使用如下方式# provisioner: ceph.com/rbdcat &gt;storageclass-ceph-rdb.yaml&lt;&lt;EOFkind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: dynamic-ceph-rdbprovisioner: ceph.com/rbd# provisioner: kubernetes.io/rbdparameters: monitors: 11.11.11.111:6789,11.11.11.112:6789,11.11.11.113:6789 adminId: admin adminSecretName: ceph-secret adminSecretNamespace: kube-system pool: kube userId: kube userSecretName: ceph-user-secret fsType: ext4 imageFormat: "2" imageFeatures: "layering"EOF# 创建kubectl apply -f storageclass-ceph-rdb.yaml# 查看kubectl get sc 测试使用12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# 创建pvc测试cat &gt;ceph-rdb-pvc-test.yaml&lt;&lt;EOFkind: PersistentVolumeClaimapiVersion: v1metadata: name: ceph-rdb-claimspec: accessModes: - ReadWriteOnce storageClassName: dynamic-ceph-rdb resources: requests: storage: 2GiEOFkubectl apply -f ceph-rdb-pvc-test.yaml # 查看kubectl get pvckubectl get pv # 创建 nginx pod 挂载测试cat &gt;nginx-pod.yaml&lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: nginx-pod1 labels: name: nginx-pod1spec: containers: - name: nginx-pod1 image: nginx:alpine ports: - name: web containerPort: 80 volumeMounts: - name: ceph-rdb mountPath: /usr/share/nginx/html volumes: - name: ceph-rdb persistentVolumeClaim: claimName: ceph-rdb-claimEOFkubectl apply -f nginx-pod.yaml # 查看kubectl get pods -o wide # 修改文件内容kubectl exec -ti nginx-pod1 -- /bin/sh -c 'echo Hello World from Ceph RBD!!! &gt; /usr/share/nginx/html/index.html' # 访问测试POD_ID=$(kubectl get pods -o wide | grep nginx-pod1 | awk '&#123;print $(NF-1)&#125;')curl http://$POD_ID# 清理kubectl delete -f nginx-pod.yamlkubectl delete -f ceph-rdb-pvc-test.yaml 使用 CephFS linux内核需要4.10+，否则会出现无法正常使用的问题，详细issue信息 https://github.com/kubernetes-incubator/external-storage/issues/345 centos7升级内核 在ceph集群创建CephFS123456789101112# 如下操作在ceph的mon或者admin节点# CephFS需要使用两个Pool来分别存储数据和元数据ceph osd pool create fs_data 128ceph osd pool create fs_metadata 128ceph osd lspools# 创建一个CephFSceph fs new cephfs fs_metadata fs_data# 查看ceph fs ls 部署cephfs-provisioner123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102# 官方没有cephfs动态卷支持# 使用社区提供的cephfs-provisionercat &gt;external-storage-cephfs-provisioner.yaml&lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: cephfs-provisioner namespace: kube-system---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: cephfs-provisionerrules: - apiGroups: [""] resources: ["persistentvolumes"] verbs: ["get", "list", "watch", "create", "delete"] - apiGroups: [""] resources: ["persistentvolumeclaims"] verbs: ["get", "list", "watch", "update"] - apiGroups: ["storage.k8s.io"] resources: ["storageclasses"] verbs: ["get", "list", "watch"] - apiGroups: [""] resources: ["events"] verbs: ["create", "update", "patch"] - apiGroups: [""] resources: ["endpoints"] verbs: ["get", "list", "watch", "create", "update", "patch"] - apiGroups: [""] resources: ["secrets"] verbs: ["create", "get", "delete"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: cephfs-provisionersubjects: - kind: ServiceAccount name: cephfs-provisioner namespace: kube-systemroleRef: kind: ClusterRole name: cephfs-provisioner apiGroup: rbac.authorization.k8s.io---apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: cephfs-provisioner namespace: kube-systemrules: - apiGroups: [""] resources: ["secrets"] verbs: ["create", "get", "delete"]---apiVersion: rbac.authorization.k8s.io/v1kind: RoleBindingmetadata: name: cephfs-provisioner namespace: kube-systemroleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: cephfs-provisionersubjects:- kind: ServiceAccount name: cephfs-provisioner namespace: kube-system---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: cephfs-provisioner namespace: kube-systemspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: cephfs-provisioner spec: containers: - name: cephfs-provisioner image: "quay.io/external_storage/cephfs-provisioner:v2.0.0-k8s1.11" env: - name: PROVISIONER_NAME value: ceph.com/cephfs command: - "/usr/local/bin/cephfs-provisioner" args: - "-id=cephfs-provisioner-1" serviceAccount: cephfs-provisionerEOFkubectl apply -f external-storage-cephfs-provisioner.yaml# 查看状态 等待running之后 再进行后续的操作kubectl get pod -n kube-system #####配置 storageclass 12345678910111213141516171819202122232425262728293031323334# 查看key 在ceph的mon或者admin节点ceph auth get-key client.admin# 创建 admin secret# CEPH_ADMIN_SECRET 替换为 client.admin 获取到的key# 如果在测试 ceph rbd 方式已经添加 可以略过此步骤export CEPH_ADMIN_SECRET='AQBBAnRbSiSOFxAAEZXNMzYV6hsceccYLhzdWw=='kubectl create secret generic ceph-secret --type="kubernetes.io/rbd" \--from-literal=key=$CEPH_ADMIN_SECRET \--namespace=kube-system# 查看 secretkubectl get secret ceph-secret -n kube-system -o yaml# 配置 StorageClasscat &gt;storageclass-cephfs.yaml&lt;&lt;EOFkind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: dynamic-cephfsprovisioner: ceph.com/cephfsparameters: monitors: 11.11.11.111:6789,11.11.11.112:6789,11.11.11.113:6789 adminId: admin adminSecretName: ceph-secret adminSecretNamespace: "kube-system" claimRoot: /volumes/kubernetesEOF# 创建kubectl apply -f storageclass-cephfs.yaml# 查看kubectl get sc 测试使用12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# 创建pvc测试cat &gt;cephfs-pvc-test.yaml&lt;&lt;EOFkind: PersistentVolumeClaimapiVersion: v1metadata: name: cephfs-claimspec: accessModes: - ReadWriteOnce storageClassName: dynamic-cephfs resources: requests: storage: 2GiEOFkubectl apply -f cephfs-pvc-test.yaml # 查看kubectl get pvckubectl get pv # 创建 nginx pod 挂载测试cat &gt;nginx-pod.yaml&lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: nginx-pod1 labels: name: nginx-pod1spec: containers: - name: nginx-pod1 image: nginx:alpine ports: - name: web containerPort: 80 volumeMounts: - name: cephfs mountPath: /usr/share/nginx/html volumes: - name: cephfs persistentVolumeClaim: claimName: cephfs-claimEOFkubectl apply -f nginx-pod.yaml # 查看kubectl get pods -o wide # 修改文件内容kubectl exec -ti nginx-pod1 -- /bin/sh -c 'echo Hello World from CephFS!!! &gt; /usr/share/nginx/html/index.html' # 访问测试POD_ID=$(kubectl get pods -o wide | grep nginx-pod1 | awk '&#123;print $(NF-1)&#125;')curl http://$POD_ID# 清理kubectl delete -f nginx-pod.yamlkubectl delete -f cephfs-pvc-test.yaml 参考文档 https://kubernetes.io/docs/concepts/storage/storage-classes/ https://docs.openshift.com/container-platform/3.5/install_config/storage_examples/ceph_rbd_dynamic_example.html https://ieevee.com/tech/2018/05/17/k8s-cephfs.html https://github.com/kubernetes-incubator/external-storage/tree/master/ceph/rbd https://github.com/kubernetes-incubator/external-storage/blob/master/ceph/rbd/deploy/README.md https://github.com/heketi/heketi/blob/master/docs/admin/install-kubernetes.md https://github.com/gluster/gluster-kubernetes/blob/master/docs/setup-guide.md https://github.com/gluster/gluster-kubernetes/blob/master/docs/examples/hello_world/README.md https://jimmysong.io/kubernetes-handbook/practice/using-heketi-gluster-for-persistent-storage.html https://kubernetes.io/docs/concepts/storage/persistent-volumes/ https://docs.openshift.com/enterprise/3.1/architecture/additional_concepts/storage.html#pv-access-modes]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
        <tag>storage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ceph安装配置]]></title>
    <url>%2Fposts%2F47%2F</url>
    <content type="text"><![CDATA[简介ceph是一个开源分布式存储系统，支持PB级别的存储，支持对象存储，块存储和文件存储，高性能，高可用，可扩展。 部署网络建议架构图 部署 部署架构图，本次实验部署jewel版本 实验环境的Vagrantfile lab1节点既作admin节点又作node节点，lab2，lab3只作为node节点，lab4作为作测试使用ceph的节点 123456789101112131415161718192021222324252627# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| (1..4).each do |i| config.vm.define "lab#&#123;i&#125;" do |node| node.vm.box = "centos-7.4-docker-17" node.ssh.insert_key = false node.vm.hostname = "lab#&#123;i&#125;" node.vm.network "private_network", ip: "11.11.11.11#&#123;i&#125;" node.vm.provision "shell", inline: "echo hello from node #&#123;i&#125;" node.vm.provider "virtualbox" do |v| v.cpus = 3 v.customize ["modifyvm", :id, "--name", "lab#&#123;i&#125;", "--memory", "3096"] file_to_disk = "lab#&#123;i&#125;_vdb.vdi" unless File.exist?(file_to_disk) # 50GB v.customize ['createhd', '--filename', file_to_disk, '--size', 50 * 1024] end v.customize ['storageattach', :id, '--storagectl', 'IDE', '--port', 1, '--device', 0, '--type', 'hdd', '--medium', file_to_disk] end end endend 配置阿里ceph源 在所有节点执行如下操作 1234567891011121314151617181920212223cat &gt;/etc/yum.repos.d/ceph.repo&lt;&lt;EOF[ceph]name=cephbaseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/x86_64/gpgcheck=0priority=1[ceph-noarch]name=cephnoarchbaseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/noarch/gpgcheck=0priority=1[ceph-source]name=Ceph source packagesbaseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/SRPMSenabled=0gpgcheck=1type=rpm-mdgpgkey=http://mirrors.163.com/ceph/keys/release.ascpriority=1EOFyum makecache 在admin节点安装ceph-deploy lab1 节点 12345678910111213141516171819# 官方源# 如果已经配置了上面的阿里源，不需要再配置如下的源# 推荐使用阿里源，因为官方源速度太慢cat &gt;/etc/yum.repos.d/ceph.repo&lt;&lt;EOF[ceph-noarch]name=Ceph noarch packagesbaseurl=https://download.ceph.com/rpm-jewel/el7/noarchenabled=1gpgcheck=1type=rpm-mdgpgkey=https://download.ceph.com/keys/release.ascEOF# 更新系统软件# 此操作可省略# yum update -y# 安装 ceph-deployyum install -y ceph-deploy 配置admin节点连接node节点 安装之后需要配置admin节点可以ssh无密码登录每个node节点和测试节点，用户需要有sudo权限 123456789101112131415161718192021222324252627282930313233343536# 在每一个node节点执行useradd cephecho 'ceph' | passwd --stdin cephecho "ceph ALL = (root) NOPASSWD:ALL" &gt; /etc/sudoers.d/cephchmod 0440 /etc/sudoers.d/ceph# 配置sshd可以使用password登录sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/' /etc/ssh/sshd_configsystemctl reload sshd# 配置sudo不需要ttysed -i 's/Default requiretty/#Default requiretty/' /etc/sudoers# 在所有节点配置hosts# 包括要进行ceph测试的机器# 使用vagrant实验时注意# 由于vagrant会自动把主机名解析为 127.0.0.1# 所以在实验时如果在ceph集群内任意一台机器进行实验时# 注意把本机名解析为 127.0.0.1 的行注释，如下所示# 127.0.0.1 lab1 lab1cat &gt;&gt;/etc/hosts&lt;&lt;EOF11.11.11.111 lab111.11.11.112 lab211.11.11.113 lab311.11.11.113 lab4EOF# 在admin节点执行# 创建ceph用户，配置sshkey登录# 由于lab1节点作为node节点时已经创建过ceph用户# 第一条命令可能会出错，忽略即可useradd cephsu - cephssh-keygenssh-copy-id ceph@lab1ssh-copy-id ceph@lab2ssh-copy-id ceph@lab3ssh-copy-id ceph@lab4 在admin节点创建集群 在lab1节点执行如下操作，node的主机名一定要设置正确 lab1, lab2, lab3。否则可能会无法实验成功 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 不要使用sudo也不要使用root用户运行如下的命令su - cephmkdir my-clustercd my-cluster# 创建lab1为monitorceph-deploy new lab1# 查看配置文件ls -l# 配置ceph.conf[global]...# 如果有多个网卡，应该配置如下选项，# public network是公共网络，负责集群对外提供服务的流量# cluster network是集群网络，负载集群中数据复制传输通信等# 本次实验使用同一块网卡，生境环境建议分别使用一块网卡public network = 11.11.11.0/24cluster network = 11.11.11.0/24# 安装 ceph 包# 如果按照官方文档安装方法 会重新配置安装官方ceph源# 由于网络问题，安装可能会出错，需要多次执行# ceph-deploy install 其实只是会安装 ceph ceph-radosgw 两个包# ceph-deploy install lab1 lab2 lab3# 推荐使用阿里源安装，因为使用ceph-deploy安装会很慢# 使用如下命令手动安装包，替代官方的 ceph-deploy install 命令# 如下操作在所有node节点上执行yum install -y ceph ceph-radosgw# 部署monitor和生成keysceph-deploy mon create-initialls -l *.keyring# 复制文件到node节点ceph-deploy admin lab1 lab2 lab3# 部署manager （luminous+）12及以后的版本需要部署# 本次部署 jewel 版本 ，不需要执行如下命令# ceph-deploy mgr create lab1# 添加osd 以磁盘方式# 本次实验采用此种方法# sdb 为虚拟机添加的磁盘设置名ceph-deploy osd create lab1:sdb lab2:sdb lab3:sdb# 在node节点创建目录rm -rf /data/osd1mkdir -pv /data/osd1chmod 777 -R /data/osd1chown ceph.ceph -R /data/osd1# 添加osd 以文件目录方式ceph-deploy osd prepare lab1:/data/osd1 lab2:/data/osd1 lab3:/data/osd1ceph-deploy osd activate lab1:/data/osd1 lab2:/data/osd1 lab3:/data/osd1# 查看状态ssh lab1 sudo ceph healthssh lab1 sudo ceph -s 清理集群12345# 如果安装过程出错，使用如下命令清理之后重新开始ceph-deploy purge lab1 lab2 lab3ceph-deploy purgedata lab1 lab2 lab3ceph-deploy forgetkeysrm ceph.* 扩展集群提高可用性 在lab1上运行metadata server 为后续使用cephfs 在lab2,lab3运行monitor和manager提高集群可用性 1234567891011121314# 为了使用CephFS，必须启动 metadata serverceph-deploy mds create lab1# 添加monitorceph-deploy mon add lab2ceph-deploy mon add lab3ssh lab1 sudo ceph -s# 在monitor节点查看状态（需要以root用户或者sudo查看）ceph quorum_status --format json-pretty# 添加manager （luminous+）12及以后的版本需要部署# 本次部署 jewel 版本 ，不需要执行如下命令# ceph-deploy mgr create lab2 lab3 部署RGW使用Ceph Object Gateway 提供S3/Swift存储功能，实现S3和Swift兼容的接口，可以使用S3或Swift的命令行工具或SDK来使用ceph1234567891011121314# 启动 rgwceph-deploy rgw create lab1# 修改配置 /etc/ceph/ceph.conf# 使用 rgw 监听在 80 端口# lab1 为启动 rgw 的主机名[client.rgw.lab1]rgw_frontends = "civetweb port=80"# 重启 rgwsystemctl restart ceph-radosgw@rgw.lab1# 访问测试curl -I http://11.11.11.111/ 使用ceph存储 应用存储使用架构图 对象存储12345678910111213141516171819202122232425# 安装cephyum install -y ceph# 复制相关文件到要使用ceph-client的机器ceph-deploy admin lab4# 测试# 存储文件echo 'hello ceph oject storage' &gt; testfile.txtceph osd pool create mytest 8rados put test-object-1 testfile.txt --pool=mytest# 查看读取文件rados -p mytest lsrados get test-object-1 testfile.txt.1 --pool=mytestcat testfile.txt.1# 查看文件位置ceph osd map mytest test-object-1# 删除文件rados rm test-object-1 --pool=mytest# 删除poolceph osd pool rm mytest mytest --yes-i-really-really-mean-it 块存储1234567891011121314151617181920212223242526272829# 安装cephyum install -y ceph# 复制相关文件到要使用ceph-client的机器ceph-deploy admin lab4# 创建块设备镜像rbd create foo --size 4096 --image-feature layeringrbd info foorados -p rbd ls# 映射镜像到块设备sudo rbd map foo --name client.admin# 使用块设备创建文件系统sudo mkfs.ext4 -m0 /dev/rbd/rbd/foo# 挂载使用sudo mkdir /mnt/ceph-block-devicesudo mount /dev/rbd/rbd/foo /mnt/ceph-block-devicecd /mnt/ceph-block-deviceecho 'hello ceph block storage' &gt; testfile.txt# 清理cd ~sudo umount -lf /mnt/ceph-block-devicesudo rbd unmap foorbd remove foorados -p rbd ls S3对象存储 11.11.11.111 为安装了 RGW 的机器 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596# 安装yum install -y ceph ceph-radosgw# 复制相关文件到要使用ceph-client的机器ceph-deploy admin lab4# 创建S3所需要的poolceph osd pool create .rgw 128 128ceph osd pool create .rgw.root 128 128ceph osd pool create .rgw.control 128 128ceph osd pool create .rgw.gc 128 128ceph osd pool create .rgw.buckets 128 128ceph osd pool create .rgw.buckets.index 128 128ceph osd pool create .rgw.buckets.extra 128 128ceph osd pool create .log 128 128ceph osd pool create .intent-log 128 128ceph osd pool create .usage 128 128ceph osd pool create .users 128 128ceph osd pool create .users.email 128 128ceph osd pool create .users.swift 128 128ceph osd pool create .users.uid 128 128# 查看rados lspools# 访问测试curl -I http://11.11.11.111/# 创建S3用户# 保存如下命令返回的 user access_key secret_keyradosgw-admin user create --uid=foo --display-name=foo --email=foo@foo.com# 创建admin用户radosgw-admin user create --uid=admin --display-name=admin# 允许admin读写users信息radosgw-admin caps add --uid=admin --caps="users=*"# 允许admin读写所有的usage信息radosgw-admin caps add --uid=admin --caps="usage=read,write"# 安装s3测试工具yum install -y s3cmd# 配置s3cmd, 只需指定Access Key和Secret Key，其他默认即可s3cmd --configure# 修该生成的配置文件vim $HOME/.s3cfghost_base = 11.11.11.111host_bucket = 11.11.11.111/%(bucket)use_https = False# 创建Buckets3cmd mb s3://mybuckets3cmd ls# 上传Objectecho 'hello ceph block storage s3' &gt; hello.txts3cmd put hello.txt s3://mybucket# 查看Objects3cmd ls s3://mybucket# 下载Objectcd /tmps3cmd get s3://mybucket/hello.txtcd ~# 删除bucket下所有对象s3cmd del -rf s3://mybucket/s3cmd ls -r s3://mybucket# 删除Buckets3cmd mb s3://mybucket1s3cmd rb s3://mybucket1# 删除S3用户radosgw-admin user rm --uid=fooradosgw-admin user rm --uid=admin# 删除poolceph osd pool delete .rgw .rgw --yes-i-really-really-mean-itceph osd pool delete .rgw.root .rgw.root --yes-i-really-really-mean-itceph osd pool delete .rgw.control .rgw.control --yes-i-really-really-mean-itceph osd pool delete .rgw.gc .rgw.gc --yes-i-really-really-mean-itceph osd pool delete .rgw.buckets .rgw.buckets --yes-i-really-really-mean-itceph osd pool delete .rgw.buckets.index .rgw.buckets.index --yes-i-really-really-mean-itceph osd pool delete .rgw.buckets.extra .rgw.buckets.extra --yes-i-really-really-mean-itceph osd pool delete .log .log --yes-i-really-really-mean-itceph osd pool delete .intent-log .intent-log --yes-i-really-really-mean-itceph osd pool delete .usage .usage --yes-i-really-really-mean-itceph osd pool delete .users .users --yes-i-really-really-mean-itceph osd pool delete .users.email .users.email --yes-i-really-really-mean-itceph osd pool delete .users.swift .users.swift --yes-i-really-really-mean-itceph osd pool delete .users.uid .users.uid --yes-i-really-really-mean-it CephFS存储123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# 安装cephyum install -y ceph ceph-fuse# 复制相关文件到要使用ceph-client的机器ceph-deploy admin lab4# CephFS需要使用两个Pool来分别存储数据和元数据ceph osd pool create fs_data 128ceph osd pool create fs_metadata 128ceph osd lspools# 创建一个CephFSceph fs new cephfs fs_metadata fs_data# 查看ceph fs ls# 使用内核提供的功能 挂载CephFS# 由于可能会有bug，建议使用 4.0 以上的内核# 优点是性能比使用ceph-fuse更好# name，secret 为 /etc/ceph/ceph.client.admin.keyring 里的内容mkdir /mnt/mycephfsmount -t ceph lab1:6789,lab2:6789,lab3:6789:/ /mnt/mycephfs -o name=admin,secret=AQBoclRaiilZJBAACLjqg2OUOOB/FNa20UJXYA==df -hcd /mnt/mycephfsecho 'hello ceph CephFS' &gt; hello.txtcd ~umount -lf /mnt/mycephfsrm -rf /mnt/mycephfs# 使用 ceph-fuse 挂载CephFSmkdir /mnt/mycephfsceph-fuse -m lab1:6789 /mnt/mycephfsdf -hcd /mnt/mycephfsecho 'hello ceph CephFS' &gt; hello.txtcd ~umount -lf /mnt/mycephfsrm -rf /mnt/mycephfs# 清理# 停止 metadata server# 本次部署在lab1，去lab1停止服务systemctl stop ceph-mds@lab1ceph fs rm cephfs --yes-i-really-mean-itceph osd pool delete fs_data fs_data --yes-i-really-really-mean-itceph osd pool delete fs_metadata fs_metadata --yes-i-really-really-mean-it# 开启 metadata server# 方便以后使用 cephfssystemctl start ceph-mds@lab1 参考文档 http://docs.ceph.com/docs/master/cephfs/best-practices/ http://docs.ceph.com/docs/master/start/ http://docs.ceph.org.cn/start/ http://docs.ceph.com/docs/master/start/quick-rbd/ http://www.xuxiaopang.com/2016/10/13/easy-ceph-RBD/ http://docs.ceph.com/docs/master/start/quick-rgw/ https://blog.frognew.com/tags/ceph.html https://www.centos.bz/2017/10/%E7%94%A8ceph-deploy%E5%AE%89%E8%A3%85ceph%E5%B9%B6%E9%83%A8%E7%BD%B2%E9%9B%86%E7%BE%A4/]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>ceph</tag>
        <tag>storage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[istio-1.0微服务实验]]></title>
    <url>%2Fposts%2F46%2F</url>
    <content type="text"><![CDATA[由于1.0版本和0.8版本没有改动api，可以直接使用0.8版本的微服务实验]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>service mesh</tag>
        <tag>microservice</tag>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s日志收集实战]]></title>
    <url>%2Fposts%2F45%2F</url>
    <content type="text"><![CDATA[简介本文主要介绍在k8s中收集应用的日志方案，应用运行中日志，一般情况下都需要收集存储到一个集中的日志管理系统中，可以方便对日志进行分析统计，监控，甚至用于机器学习，智能分析应用系统问题，及时修复应用所存在的问题。 在k8s集群中应用一般有如下日志输出方式 直接遵循docker官方建议把日志输出到标准输出或者标准错误输出 输出日志到容器内指定目录中 应用直接发送日志给日志收集系统 本文会综合部署上述日志收集方案。日志收集组件说明 elastisearch 存储收集到的日志 kibana 可视化收集到的日志 logstash 汇总处理日志发送给elastisearch 存储 filebeat 读取容器或者应用日志文件处理发送给elastisearch或者logstash，也可用于汇总日志 fluentd 读取容器或者应用日志文件处理发送给elastisearch，也可用于汇总日志 fluent-bit 读取容器或者应用日志文件处理发送给elastisearch或者fluentd 部署 本次实验使用了3台虚拟机做k8s集群，每台虚拟机3G内存 部署前的准备1234567# 拉取文件git clone https://github.com/mgxian/k8s-log.gitcd k8s-loggit checkout v1# 创建 logging namespacekubectl apply -f logging-namespace.yaml 部署elastisearch1234567891011121314151617181920# 本次部署虽然使用 StatefulSet 但是没有使用pv进行持久化数据存储# pod重启之后，数据会丢失，生产环境一定要使用pv持久化存储数据# 部署kubectl apply -f elasticsearch.yaml# 查看状态kubectl get pods,svc -n logging -o wide# 等待所有pod变成running状态 # 访问测试# 如果测试都有数据返回代表部署成功kubectl run curl -n logging --image=radial/busyboxplus:curl -i --ttynslookup elasticsearch-loggingcurl 'http://elasticsearch-logging:9200/_cluster/health?pretty'curl 'http://elasticsearch-logging:9200/_cat/nodes'exit# 清理测试kubectl delete deploy curl -n logging 部署kibana1234567891011# 部署kubectl apply -f kibana.yaml# 查看状态kubectl get pods,svc -n logging -o wide# 访问测试# 浏览器访问下面输出的地址 看到 kibana 界面代表正常# 11.11.11.112 为集群中某个 node 节点ipKIBANA_NODEPORT=$(kubectl get svc -n logging | grep kibana-logging | awk '&#123;print $(NF-1)&#125;' | awk -F[:/] '&#123;print $2&#125;')echo "http://11.11.11.112:$KIBANA_NODEPORT/" 部署fluentd收集日志12345678910111213# fluentd 以 daemoset 方式部署# 在每个节点上启动fluentd容器，收集k8s组件，docker以及容器的日志# 给每个需要启动fluentd的节点打相关label# kubectl label node lab1 beta.kubernetes.io/fluentd-ds-ready=truekubectl label nodes --all beta.kubernetes.io/fluentd-ds-ready=true# 部署kubectl apply -f fluentd-es-configmap.yamlkubectl apply -f fluentd-es-ds.yaml# 查看状态kubectl get pods,svc -n logging -o wide kibana查看日志 创建index fluentd-k8s-*，由于需要拉取镜像启动容器，可能需要等待几分钟才能看到索引和数据 查看日志 应用日志收集测试应用日志输出到标准输出测试1234567891011# 启动测试日志输出kubectl run echo-test --image=radial/busyboxplus:curl -- sh -c 'count=1;while true;do echo log to stdout $count;sleep 1;count=$(($count+1));done'# 查看状态kubectl get pods -o wide# 命令行查看日志ECHO_TEST_POD=$(kubectl get pods | grep echo-test | awk '&#123;print $1&#125;')kubectl logs -f $ECHO_TEST_POD# 刷新 kibana 查看是否有新日志进入 应用日志输出到容器指定目录(filebeat收集)12345# 部署kubectl apply -f log-contanier-file-filebeat.yaml# 查看kubectl get pods -o wide 添加index filebeat-k8s-* 查看日志 应用日志输出到容器指定目录(fluent-bit收集)12345# 部署kubectl apply -f log-contanier-file-fluentbit.yaml# 查看kubectl get pods -o wide 添加index fluentbit-k8s-* 查看日志 应用直接发送日志到日志系统1234567# 本次测试应用直接输出日志到 elasticsearch# 部署kubectl apply -f log-contanier-es.yaml# 查看kubectl get pods -o wide 添加index k8s-app-* 查看日志 清理1234kubectl delete -f log-contanier-es.yamlkubectl delete -f log-contanier-file-fluentbit.yamlkubectl delete -f log-contanier-file-filebeat.yamlkubectl delete deploy echo-test 日志收集系统总结 本小节的图表以ELK技术栈展示说明，实际使用过程中可以使用EFK技术栈，使用fluentd代替logstash，使用fluent-bit代替filebeat。由于fluentd在内存占用和性能上有更好的优势，推荐使用fluentd替代logstash ，fluent-bit和filebeat性能和内存占用相差不大 k8s集群日志通用收集方案 集群内相关组件日志使用fluentd/filebeat收集 应用输出到标准输出或标准错误输出的日志使用fluentd/filebeat收集 应用输出到容器中指定文件日志使用fluent-bit/filebeat收集 通用日志收集系统 通用日志收集系统架构 架构说明 日志收集与处理解耦 由于收集和处理过程间加入了队列，当日志出现暴增时，可以避免分析处理节点被打垮，给分析处理节点足够时间消化日志数据 日志分析处理节点可以动态伸缩 大流量日志收集系统 大流量日志收集系统架构图 架构说明 当日志流量过大时，如果每一个日志收集节点都直连队列写数据，由于有很多分散的连接及写请求，会给队列造成压力。如果日志都发送到logstash收集节点，再集中写入队列，会减轻队列压力。 应用日志收集实验(ELK技术栈)以收集nginx日志为例，进行日志收集分析实验， 复用之前实验创建的elasticsearch，kibana应用。实验采用大流量日志收集架构 部署redis队列12345# 部署kubectl apply -f redis.yaml# 查看kubectl get pods -n logging 部署indexer分析日志12345# 部署kubectl apply -f logstash-indexer.yaml# 查看kubectl get pods -n logging 部署shipper集中日志12345# 部署kubectl apply -f logstash-shipper.yaml# 查看kubectl get pods -n logging 部署nginx测试日志收集12345# 部署kubectl apply -f nginx-log-filebeat.yaml# 查看kubectl get pods 持续访问nginx生成日志12345# 部署kubectl run curl-test --image=radial/busyboxplus:curl -- sh -c 'count=1;while true;do curl -s -H "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.89 Safari/537.36 $count" http://nginx-log-filebeat/ &gt;/dev/null;sleep 1;count=$(($count+1));done'# 查看kubectl get pods 访问kibana查看日志 添加index k8s-logging-elk-* 由于 logstash 启动较慢，可能需要等待数分钟才能看到数据 清理12345kubectl delete -f redis.yamlkubectl delete -f logstash-indexer.yamlkubectl delete -f logstash-shipper.yamlkubectl delete -f nginx-log-filebeat.yamlkubectl delete deploy curl-test 应用日志收集实验(EFK技术栈)由于fluentd官方不提供redis队列的支持，本次实验移除了redis队列。 部署indexer分析日志12345# 部署kubectl apply -f fluentd-indexer.yaml# 查看kubectl get pods -n logging 部署shipper集中日志12345# 部署kubectl apply -f fluentd-shipper.yaml# 查看kubectl get pods -n logging 部署nginx测试日志收集12345# 部署kubectl apply -f nginx-log-fluentbit.yaml# 查看kubectl get pods 持续访问nginx生成日志12345# 部署kubectl run curl-test --image=radial/busyboxplus:curl -- sh -c 'count=1;while true;do curl -s -H "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.89 Safari/537.36 $count" http://nginx-log-fluentbit/ &gt;/dev/null;sleep 1;count=$(($count+1));done'# 查看kubectl get pod 访问kibana查看日志 添加index k8s-logging-efk-* 清理1234kubectl delete -f fluentd-indexer.yamlkubectl delete -f fluentd-shipper.yamlkubectl delete -f nginx-log-fluentbit.yamlkubectl delete deploy curl-test 应用日志可视化部署日志收集需要的组件12345678# 部署 indexer shipper fluentbitkubectl apply -f fluentd-indexer.yamlkubectl apply -f fluentd-shipper.yamlkubectl apply -f nginx-log-fluentbit.yaml# 查看kubectl get podskubectl get pods -n logging 模拟用户访问12345# 部署kubectl apply -f web-load-gen.yaml# 查看kubectl get pods 访问kibana查看日志 添加index k8s-logging-efk-* 创建图表创建 Search制作 Visualize 的时候需要使用 按指定条件搜索日志 保存 Search 创建 Visualize创建好的 Visualize 可以添加到 Dashboard 中 选择制作 Visualize 选择 Visualize 类型 选择使用上面步骤保存的 Search 选择指定的 bucket 选择 code 字段进行统计 保存 Visualize 使用如上的步骤创建多个 Visualize 创建 Dashboard选择创建 Dashboard 把 Visualize 添加到 Dashboard 保存 Dashboard 编辑调整位置和大小 最终图表展示 如果快速体验可以在 菜单 Managerment 的 Saved Ojects 标签直接使用导入功能，导入本次实验下载目录k8s-log下的k8s-kibana-all.json文件 参考文档 https://kubernetes.io/docs/concepts/cluster-administration/logging/ https://banzaicloud.com/blog/k8s-logging/ https://docs.fluentd.org/v0.12/articles/kubernetes-fluentd https://jimmysong.io/kubernetes-handbook/practice/app-log-collection.html https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/fluentd-elasticsearch/README.md https://www.elastic.co/blog/shipping-kubernetes-logs-to-elasticsearch-with-filebeat https://github.com/elastic/beats/blob/master/deploy/kubernetes/filebeat/README.md https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-input-docker.html https://www.elastic.co/guide/en/beats/filebeat/current/add-kubernetes-metadata.html https://github.com/fluent/fluentd-kubernetes-daemonset https://github.com/fluent/fluent-bit-kubernetes-logging https://github.com/fluent/fluent-bit https://www.docker.elastic.co/ https://fluentbit.io/documentation/0.13/ https://docs.fluentd.org/v1.0/articles/quickstart https://www.elastic.co/guide/en/logstash/6.3/deploying-and-scaling.html]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
        <tag>log</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s使用openebs实现动态持久化存储]]></title>
    <url>%2Fposts%2F44%2F</url>
    <content type="text"><![CDATA[简介本文章介绍如何使用openebs为k8s提供动态申请pv的功能。iscsi提供底层存储功能，openebs管理iscsi。目前只支持pv的ReadWriteOnce访问模式 访问模式只是能力描述，并不是强制执行的，对于没有按pvc声明的方式使用pv，存储提供者应该负责访问时的运行错误。例如如果设置pvc的访问模式为ReadOnlyMany ，pod挂载后依然可写，如果需要真正的不可写，申请pvc是需要指定 readOnly: true 参数 安装实验用的Vagrantfile123456789101112131415161718192021222324252627# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| (1..3).each do |i| config.vm.define "lab#&#123;i&#125;" do |node| node.vm.box = "centos-7.4-docker-17" node.ssh.insert_key = false node.vm.hostname = "lab#&#123;i&#125;" node.vm.network "private_network", ip: "11.11.11.11#&#123;i&#125;" node.vm.provision "shell", inline: "echo hello from node #&#123;i&#125;" node.vm.provider "virtualbox" do |v| v.cpus = 2 v.customize ["modifyvm", :id, "--name", "lab#&#123;i&#125;", "--memory", "3096"] file_to_disk = "lab#&#123;i&#125;_vdb.vdi" unless File.exist?(file_to_disk) # 50GB v.customize ['createhd', '--filename', file_to_disk, '--size', 50 * 1024] end v.customize ['storageattach', :id, '--storagectl', 'IDE', '--port', 1, '--device', 0, '--type', 'hdd', '--medium', file_to_disk] end end endend 安装配置iscsi123456789101112# 安装 iscsiyum install iscsi-initiator-utils -y# 查看 InitiatorName 是否正常配置cat /etc/iscsi/initiatorname.iscsi# 启动查看状态systemctl start iscsid.servicesystemctl status iscsid.servicesystemctl start iscsi.servicesystemctl status iscsi.service 安装openebs1234567891011# 部署mkdir openebs &amp;&amp; cd openebswget https://raw.githubusercontent.com/openebs/openebs/v0.6/k8s/openebs-operator.yamlwget https://raw.githubusercontent.com/openebs/openebs/v0.6/k8s/openebs-storageclasses.yamlkubectl apply -f openebs-operator.yamlkubectl apply -f openebs-storageclasses.yaml# 查看 openebs 状态kubectl get pods -n openebs -o widekubectl get svc -n openebskubectl get crd 测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 查看 storage classkubectl get sc# 创建pvc测试cat &gt;openebs-pvc-test.yaml&lt;&lt;EOFapiVersion: v1kind: PersistentVolumeClaimmetadata: name: openebs1spec: storageClassName: openebs-standard accessModes: - ReadWriteOnce resources: requests: storage: 5GiEOFkubectl apply -f openebs-pvc-test.yaml # 查看kubectl get pvckubectl get pv # 创建 nginx pod 挂载测试cat &gt;nginx-pod.yaml&lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: nginx-pod1 labels: name: nginx-pod1spec: containers: - name: nginx-pod1 image: nginx:alpine ports: - name: web containerPort: 80 volumeMounts: - name: openebs1-vol1 mountPath: /usr/share/nginx/html volumes: - name: openebs1-vol1 persistentVolumeClaim: claimName: openebs1EOFkubectl apply -f nginx-pod.yaml # 查看kubectl get pods -o wide # 修改文件内容kubectl exec -ti nginx-pod1 -- /bin/sh -c 'echo Hello World from Openebs!!! &gt; /usr/share/nginx/html/index.html' # 访问测试POD_ID=$(kubectl get pods -o wide | grep nginx-pod1 | awk '&#123;print $(NF-1)&#125;')curl http://$POD_ID 参考文档 https://github.com/heketi/heketi/blob/master/docs/admin/install-kubernetes.md https://github.com/gluster/gluster-kubernetes/blob/master/docs/setup-guide.md https://github.com/gluster/gluster-kubernetes/blob/master/docs/examples/hello_world/README.md https://jimmysong.io/kubernetes-handbook/practice/using-heketi-gluster-for-persistent-storage.html https://kubernetes.io/docs/concepts/storage/persistent-volumes/ https://docs.openshift.com/enterprise/3.1/architecture/additional_concepts/storage.html#pv-access-modes]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
        <tag>storage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s使用glusterfs实现动态持久化存储]]></title>
    <url>%2Fposts%2F43%2F</url>
    <content type="text"><![CDATA[简介本文章介绍如何使用glusterfs为k8s提供动态申请pv的功能。glusterfs提供底层存储功能，heketi为glusterfs提供restful风格的api，方便管理glusterfs。支持k8s的pv的3种访问模式ReadWriteOnce，ReadOnlyMany ，ReadWriteMany 访问模式只是能力描述，并不是强制执行的，对于没有按pvc声明的方式使用pv，存储提供者应该负责访问时的运行错误。例如如果设置pvc的访问模式为ReadOnlyMany ，pod挂载后依然可写，如果需要真正的不可写，申请pvc是需要指定 readOnly: true 参数 安装实验用的Vagrantfile123456789101112131415161718192021222324252627# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| (1..3).each do |i| config.vm.define "lab#&#123;i&#125;" do |node| node.vm.box = "centos-7.4-docker-17" node.ssh.insert_key = false node.vm.hostname = "lab#&#123;i&#125;" node.vm.network "private_network", ip: "11.11.11.11#&#123;i&#125;" node.vm.provision "shell", inline: "echo hello from node #&#123;i&#125;" node.vm.provider "virtualbox" do |v| v.cpus = 2 v.customize ["modifyvm", :id, "--name", "lab#&#123;i&#125;", "--memory", "3096"] file_to_disk = "lab#&#123;i&#125;_vdb.vdi" unless File.exist?(file_to_disk) # 50GB v.customize ['createhd', '--filename', file_to_disk, '--size', 50 * 1024] end v.customize ['storageattach', :id, '--storagectl', 'IDE', '--port', 1, '--device', 0, '--type', 'hdd', '--medium', file_to_disk] end end endend 环境配置说明12345678910# 安装 glusterfs 每节点需要提前加载 dm_thin_pool 模块modprobe dm_thin_pool# 配置开启自加载cat &gt;/etc/modules-load.d/glusterfs.conf&lt;&lt;EOFdm_thin_poolEOF# 安装 glusterfs-fuseyum install -y glusterfs-fuse 安装glusterfs与heketi123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148# 安装 heketi client# https://github.com/heketi/heketi/releases# 去github下载相关的版本wget https://github.com/heketi/heketi/releases/download/v7.0.0/heketi-client-v7.0.0.linux.amd64.tar.gztar xf heketi-client-v7.0.0.linux.amd64.tar.gzcp heketi-client/bin/heketi-cli /usr/local/bin# 查看版本heketi-cli -v# 如下部署步骤都在如下目录执行cd heketi-client/share/heketi/kubernetes# 在k8s中部署 glusterfskubectl create -f glusterfs-daemonset.json# 查看 node 节点kubectl get nodes# 给提供存储 node 节点打 labelkubectl label node lab1 lab2 lab3 storagenode=glusterfs# 查看 glusterfs 状态kubectl get pods -o wide# 部署 heketi server # 配置 heketi server 的权限kubectl create -f heketi-service-account.jsonkubectl create clusterrolebinding heketi-gluster-admin --clusterrole=edit --serviceaccount=default:heketi-service-account# 创建 cofig secretkubectl create secret generic heketi-config-secret --from-file=./heketi.json# 初始化部署kubectl create -f heketi-bootstrap.json# 查看 heketi bootstrap 状态kubectl get pods -o widekubectl get svc# 配置端口转发 heketi serverHEKETI_BOOTSTRAP_POD=$(kubectl get pods | grep deploy-heketi | awk '&#123;print $1&#125;')kubectl port-forward $HEKETI_BOOTSTRAP_POD 58080:8080# 测试访问# 另起一终端curl http://localhost:58080/hello# 配置 glusterfs# hostnames/manage 字段里必须和 kubectl get node 一致# hostnames/storage 指定存储网络 ip 本次实验使用与k8s集群同一个ipcat &gt;topology.json&lt;&lt;EOF&#123; "clusters": [ &#123; "nodes": [ &#123; "node": &#123; "hostnames": &#123; "manage": [ "lab1" ], "storage": [ "11.11.11.111" ] &#125;, "zone": 1 &#125;, "devices": [ &#123; "name": "/dev/sdb", "destroydata": false &#125; ] &#125;, &#123; "node": &#123; "hostnames": &#123; "manage": [ "lab2" ], "storage": [ "11.11.11.112" ] &#125;, "zone": 1 &#125;, "devices": [ &#123; "name": "/dev/sdb", "destroydata": false &#125; ] &#125;, &#123; "node": &#123; "hostnames": &#123; "manage": [ "lab3" ], "storage": [ "11.11.11.113" ] &#125;, "zone": 1 &#125;, "devices": [ &#123; "name": "/dev/sdb", "destroydata": false &#125; ] &#125; ] &#125; ]&#125;EOFexport HEKETI_CLI_SERVER=http://localhost:58080heketi-cli topology load --json=topology.json# 使用 Heketi 创建一个用于存储 Heketi 数据库的 volumeheketi-cli setup-openshift-heketi-storagekubectl create -f heketi-storage.json# 查看状态# 等所有job完成 即状态为 Completed# 才能进行如下的步骤kubectl get podskubectl get job# 删除部署时产生的相关资源kubectl delete all,service,jobs,deployment,secret --selector="deploy-heketi"# 部署 heketi serverkubectl create -f heketi-deployment.json# 查看 heketi server 状态kubectl get pods -o widekubectl get svc# 查看 heketi 状态信息# 配置端口转发 heketi serverHEKETI_BOOTSTRAP_POD=$(kubectl get pods | grep heketi | awk '&#123;print $1&#125;')kubectl port-forward $HEKETI_BOOTSTRAP_POD 58080:8080export HEKETI_CLI_SERVER=http://localhost:58080heketi-cli cluster listheketi-cli volume list 测试1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# 创建 StorageClass# 由于没有开启认证# restuser restuserkey 可以随意写HEKETI_SERVER=$(kubectl get svc | grep heketi | head -1 | awk '&#123;print $3&#125;')echo $HEKETI_SERVERcat &gt;storageclass-glusterfs.yaml&lt;&lt;EOFkind: StorageClassapiVersion: storage.k8s.io/v1metadata: name: gluster-heketiprovisioner: kubernetes.io/glusterfsparameters: resturl: "http://$HEKETI_SERVER:8080" restauthenabled: "false" restuser: "will" restuserkey: "will" gidMin: "40000" gidMax: "50000" volumetype: "replicate:3"EOFkubectl create -f storageclass-glusterfs.yaml# 查看kubectl get sc# 创建pvc测试cat &gt;gluster-pvc-test.yaml&lt;&lt;EOFapiVersion: v1kind: PersistentVolumeClaimmetadata: name: gluster1 annotations: volume.beta.kubernetes.io/storage-class: gluster-heketispec: accessModes: - ReadWriteOnce resources: requests: storage: 5GiEOFkubectl apply -f gluster-pvc-test.yaml # 查看kubectl get pvckubectl get pv # 创建 nginx pod 挂载测试cat &gt;nginx-pod.yaml&lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: nginx-pod1 labels: name: nginx-pod1spec: containers: - name: nginx-pod1 image: nginx:alpine ports: - name: web containerPort: 80 volumeMounts: - name: gluster-vol1 mountPath: /usr/share/nginx/html volumes: - name: gluster-vol1 persistentVolumeClaim: claimName: gluster1EOFkubectl apply -f nginx-pod.yaml # 查看kubectl get pods -o wide # 修改文件内容kubectl exec -ti nginx-pod1 -- /bin/sh -c 'echo Hello World from GlusterFS!!! &gt; /usr/share/nginx/html/index.html' # 访问测试POD_ID=$(kubectl get pods -o wide | grep nginx-pod1 | awk '&#123;print $(NF-1)&#125;')curl http://$POD_ID # node 节点查看文件内容GLUSTERFS_POD=$(kubectl get pod | grep glusterfs | head -1 | awk '&#123;print $1&#125;')kubectl exec -ti $GLUSTERFS_POD /bin/shmount | grep heketicat /var/lib/heketi/mounts/vg_56033aa8a9131e84faa61a6f4774d8c3/brick_1ac5f3a0730457cf3fcec6d881e132a2/brick/index.html 参考文档 https://github.com/heketi/heketi/blob/master/docs/admin/install-kubernetes.md https://github.com/gluster/gluster-kubernetes/blob/master/docs/setup-guide.md https://github.com/gluster/gluster-kubernetes/blob/master/docs/examples/hello_world/README.md https://jimmysong.io/kubernetes-handbook/practice/using-heketi-gluster-for-persistent-storage.html https://kubernetes.io/docs/concepts/storage/persistent-volumes/ https://docs.openshift.com/enterprise/3.1/architecture/additional_concepts/storage.html#pv-access-modes]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
        <tag>storage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s全栈监控]]></title>
    <url>%2Fposts%2F42%2F</url>
    <content type="text"><![CDATA[简介整体概括本文章主要介绍如何全面监控k8s 使用metric-server收集数据给k8s集群内使用，如kubectl,hpa,scheduler等 使用prometheus-operator部署prometheus，存储监控数据 使用kube-state-metrics收集k8s集群内资源对象数据 使用node_exporter收集集群中各节点的数据 使用prometheus收集apiserver，scheduler，controller-manager，kubelet组件数据 使用alertmanager实现监控报警 使用grafana实现数据可视化 prometheus-operator简介prometheus-operator是一个整合prometheus和operator的项目，prometheus是一个集数据收集存储，数据查询，数据图表显示于一身的开源监控组件。operator是由coreos开源一套在k8s上管理应用的软件，通过operator可以方便的实现部署，扩容，删除应用等功能。 prometheus-operator利用k8s的CustomResourceDefinitions功能实现了只需要像写原生kubectl支持的yaml文件一样，轻松收集应用数据，配置报警规则等，包含如下CRDs ： Prometheus 用于部署Prometheus 实例 ServiceMonitor 用于配置数据收集，创建之后会根据DNS自动发现并收集数据 PrometheusRule 用于配置Prometheus 规则，处理规整数据和配置报警规则 Alertmanager 用于部署报警实例 安装环境说明 收集kube-controller-manager，kube-scheduler数据，需要配置组件监听0.0.0.0地址 二进制安装启动时添加如下参数 –address=0.0.0.0 如果使用kubeadm启动的集群，初始化时加入如下参数1234controllerManagerExtraArgs: address: 0.0.0.0schedulerExtraArgs: address: 0.0.0.0 如果是已经启动之后的集群，可以使用如下命令修改12sed -e "s/- --address=127.0.0.1/- --address=0.0.0.0/" -i /etc/kubernetes/manifests/kube-controller-manager.yamlsed -e "s/- --address=127.0.0.1/- --address=0.0.0.0/" -i /etc/kubernetes/manifests/kube-scheduler.yaml 收集kubelet相关数据时需要配置kubelet使用如下认证方式。使用kubeadm默认情况下已经开启 12--authentication-token-webhook=true--authorization-mode=Webhook 部署metric-server1234567891011121314151617181920212223242526# 下载mkdir k8s-monitor &amp;&amp; cd k8s-monitorgit clone https://github.com/kubernetes-incubator/metrics-server.gitcd metrics-server &amp;&amp; git checkout v0.2.1 &amp;&amp; cd ../# 修改配置（当前版本有bug）sed -ri 's@gcr.io/google_containers/metrics-server-amd64:(.*)@mirrorgooglecontainers/metrics-server-amd64:\1@g' metrics-server/deploy/1.8+/metrics-server-deployment.yamlsed -ri 's@--source=kubernetes.summary_api:.*@--source=kubernetes.summary_api:https://kubernetes.default?kubeletHttps=true\&amp;kubeletPort=10250\&amp;insecure=true@' metrics-server/deploy/1.8+/metrics-server-deployment.yaml# 部署kubectl create -f metrics-server/deploy/1.8+/# 查看状态kubectl get pods -n kube-system# 测试获取数据# 由于采集数据间隔为1分钟# 等待数分钟后查看数据NODE=$(kubectl get nodes | grep 'Ready' | head -1 | awk '&#123;print $1&#125;')METRIC_SERVER_POD=$(kubectl get pods -n kube-system | grep 'metrics-server' | awk '&#123;print $1&#125;')kubectl get --raw /apis/metrics.k8s.io/v1beta1/nodeskubectl get --raw /apis/metrics.k8s.io/v1beta1/podskubectl get --raw /apis/metrics.k8s.io/v1beta1/nodes/$NODEkubectl top node $NODEkubectl top pod $METRIC_SERVER_POD -n kube-system 下载相关部署文件12git clone https://github.com/mgxian/k8s-monitor.gitcd k8s-monitor 部署prometheus-operator12345678910# 创建 namespacekubectl apply -f monitoring-namespace.yaml# 部署kubectl apply -f prometheus-operator.yaml# 查看kubectl get pods -n monitoringkubectl get svc -n monitoringkubectl get crd 部署k8s组件服务12345# 部署kubectl apply -f kube-k8s-service.yaml# 查看kubectl get svc -n kube-system 部署node_exporter123456# 部署kubectl apply -f node_exporter.yaml# 查看kubectl get pods -n monitoringkubectl get svc -n monitoring 部署kube-state-metrics123456# 部署kubectl apply -f kube-state-metrics.yaml# 查看kubectl get pods -n monitoringkubectl get svc -n monitoring 部署prometheus123456# 部署kubectl apply -f prometheus.yaml# 查看kubectl get pods -n monitoringkubectl get svc -n monitoring 配置数据收集12345# 部署kubectl apply -f kube-servicemonitor.yaml# 查看kubectl get servicemonitors -n monitoring 查看prometheus中的数据12345678# 查看 nodeportkubectl get svc -n monitoring | grep prometheus-k8s# 获取访问链接# 11.11.11.111 为其中一个node ipNODE_IP='11.11.11.112'PROMETHEUS_NODEPORT=$(kubectl get svc -n monitoring | grep prometheus-k8s | awk '&#123;print $(NF-1)&#125;' | cut -d ':' -f 2 | cut -d '/' -f 1)echo "http://$NODE_IP:$PROMETHEUS_NODEPORT/" prometheus主页 生成图表container_network_receive_bytes_total{namespace=”monitoring”, name=~”.prometheus.“} 查看收集数据的端点 查看数据收集服务发现 部署grafana123456789101112131415# 部署kubectl apply -f grafana.yaml# 查看kubectl get pods -n monitoringkubectl get svc -n monitoring# 查看 nodeportkubectl get svc -n monitoring | grep grafana# 获取访问链接# 11.11.11.111 为其中一个node ipNODE_IP='11.11.11.112'GRAFANA_NODEPORT=$(kubectl get svc -n monitoring | grep grafana | awk '&#123;print $(NF-1)&#125;' | cut -d ':' -f 2 | cut -d '/' -f 1)echo "http://$NODE_IP:$GRAFANA_NODEPORT/" 部署alertmanager123456789101112131415# 部署kubectl apply -f alertmanager.yaml# 查看kubectl get pods -n monitoringkubectl get svc -n monitoring# 查看 nodeportkubectl get svc -n monitoring | grep alertmanager-main# 获取访问链接# 11.11.11.111 为其中一个node ipNODE_IP='11.11.11.112'ALERTMANAGER_MAIN_NODEPORT=$(kubectl get svc -n monitoring | grep alertmanager-main | awk '&#123;print $(NF-1)&#125;' | cut -d ':' -f 2 | cut -d '/' -f 1)echo "http://$NODE_IP:$ALERTMANAGER_MAIN_NODEPORT/" 查看图表 集群状态 集群状态以命名空间视角 POD状态 参考文档 https://github.com/coreos/prometheus-operator https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/cluster-monitoring.md https://github.com/coreos/prometheus-operator/tree/master/contrib/kube-prometheus]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
        <tag>monitor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重构手法]]></title>
    <url>%2Fposts%2F41%2F</url>
    <content type="text"><![CDATA[重构 改善既有代码的设计]]></content>
      <categories>
        <category>开发</category>
      </categories>
      <tags>
        <tag>编程</tag>
        <tag>重构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[helm安装配置]]></title>
    <url>%2Fposts%2F40%2F</url>
    <content type="text"><![CDATA[简介helm是kubernetes的包管理工具，用于简化部署和管理 Kubernetes 应用。用来管理charts——预先配置好的安装包资源。Helm和charts的主要作用： 应用程序封装 版本管理 依赖检查 便于应用程序分发 helm是一个C/S框架的软件，helm相当于一个客户端，tiller是一个服务端 Helm CLI 是 Helm 客户端，可以在本地执行 Tiller 是服务器端组件，在 Kubernetes 群集上运行，并管理 Kubernetes 应用程序的生命周期 Repository 是 Chart 仓库，Helm客户端通过HTTP协议来访问仓库中Chart的索引文件和压缩包 安装 安装之前需要先配置好kubeconfig，kubectl可以正常使用就表示kubeconfig已经配置正确，也可以通过环境变量KUBECONFIG指定helm使用的kubeconfig 安装helm1234567891011# 如下地址下载helm二进制文件 # 解压之后放在自己的PATH路径下# https://github.com/helm/helm/releasesmkdir -pv helm &amp;&amp; cd helmwget https://storage.googleapis.com/kubernetes-helm/helm-v2.9.1-linux-amd64.tar.gztar xf helm-v2.9.1-linux-amd64.tar.gzsudo mv linux-amd64/helm /usr/local/binrm -rf linux-amd64# 查看版本，不显示出server版本，因为还没有安装serverhelm version 简单安装tiller(本次实验采用)1234567# 部署 tillerhelm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.9.1 \--stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts# 查看kubectl get pods -n kube-system -l app=helmkubectl get svc -n kube-system -l app=helm 安全性更高的安装tiller(tls)1234567891011helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.9.1 \--stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts \--tiller-tls \--tiller-tls-verify \--tiller-tls-cert=cert.pem \--tiller-tls-key=key.pem \--tls-ca-cert=ca.pem \--service-account=tiller# 其他命令都需要使用 tls# --tls 配置rbac123456789101112131415161718192021222324cat &gt;helm-rbac-config.yaml&lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: tiller namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: tillerroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: ServiceAccount name: tiller namespace: kube-systemEOFkubectl create -f helm-rbac-config.yaml# 配置tiller使用创建的ServiceAccountkubectl patch deploy --namespace kube-system tiller-deploy -p '&#123;"spec":&#123;"template":&#123;"spec":&#123;"serviceAccount":"tiller"&#125;&#125;&#125;&#125;' 查看状态12345# 查看pod启动情况kubectl get pod -n kube-system -l app=helm# 再次查看版本，显示出server版本helm version 简单使用查看可用charts123456# 更新charts列表helm repo update # 搜索可用chartshelm searchhelm search mysql 查看已经安装的charts12helm list# helm ls 安装测试 以下两种部署方式任选一种 使用命令行模式部署1234# 使用命令行模式部署helm install -n mariadb-test \--set persistence.enabled=false,mariadbRootPassword=will,mariadbUser=will,mariadbPassword=will,mariadbDatabase=will \stable/mariadb 使用配置文件定制部署1234567891011121314151617181920# 使用配置文件方式部署# 查看可配置项helm inspect values stable/mariadb# 获取所有可配置项（去行空行和注释）helm inspect values stable/mariadb | egrep -v '^\s*#|^$'# 配置cat &gt;config.yml&lt;&lt;EOFusePassword: truemariadbRootPassword: willmariadbUser: willmariadbPassword: willmariadbDatabase: willpersistence: enabled: falseEOF# 部署helm install -n mariadb-test -f config.yml stable/mariadb 测试1234567891011121314151617# 查看状态helm lskubectl get podskubectl get svc# 连接测试kubectl run mariadb-cli --image=bitnami/mariadb:10.1.28-r1 -i --tty bashmysql -hmariadb-test-mariadb -uwill -pwill willshow databases;select version();select user();# 删除 mariadb-clikubectl delete deploy mariadb-cli# 删除部署helm delete mariadb-test --purge 自定义chart创建chart1234567891011121314151617# 创建helm create hello# chart目录结构介绍hello├── charts # 本chart依赖的chart├── Chart.yaml # 描述chart的基本信息，如名称版本等├── templates # kubernetes manifest文件模板目录│ ├── deployment.yaml│ ├── _helpers.tpl│ ├── ingress.yaml│ ├── NOTES.txt # 纯文本文件，可在其中填写chart的使用说明│ └── service.yaml└── values.yaml # chart配置的默认值# 对chart的模板和配置进行测试helm install --dry-run --debug ./ 测试安装chart1234567891011121314151617# 安装helm install -n hello-test ./# 查看helm lskubectl get podskubectl get svc# 端口转发export POD_NAME=$(kubectl get pods --namespace default -l "app=hello,release=hello-test" -o jsonpath="&#123;.items[0].metadata.name&#125;")kubectl port-forward $POD_NAME 8080:80# 访问curl http://127.0.0.1:8080# 删除helm delete hello-test --purge chart分发12# 打包为压缩包helm package ./ 参考文档 https://blog.frognew.com/2017/12/its-time-to-use-helm.html https://jimmysong.io/posts/manage-kubernetes-native-app-with-helm/ https://yq.aliyun.com/articles/159601 https://docs.helm.sh/]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
        <tag>helm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[istio-1.0安装测试]]></title>
    <url>%2Fposts%2F39%2F</url>
    <content type="text"><![CDATA[简介istio是一个service mesh开源实现，由Google/IBM/Lyft共同开发。架构图如下： 安装安装k8s集群参考文章 安装istioctl1234567891011121314# 本次实验时使用 vagrant 用户登录 具有 sudo 权限# vagrant 用户配置了连接管理 k8s 集群# 去下面的地址下载压缩包# https://github.com/istio/istio/releaseswget https://github.com/istio/istio/releases/download/1.0.0/istio-1.0.0-linux.tar.gztar xf istio-1.0.0-linux.tar.gz# 安装配置环境变量sudo mv istio-1.0.0 /usr/local/sudo ln -sv /usr/local/istio-1.0.0 /usr/local/istioecho 'export PATH=/usr/local/istio/bin:$PATH' | sudo tee /etc/profile.d/istio.shsource /etc/profile.d/istio.shistioctl version 在k8s集群中安装istio123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# 如果环境不是云环境，不支持LoadBalancer# 作如下修改，使得 ingressgateway 监听在80和443端口# 修改使用主机端口映射# 使用此修改版本之后，每台机器只能运行单个实例# 大概在3027行左右cd /usr/local/istiosudo cp install/kubernetes/istio-demo.yaml install/kubernetes/istio-demo.yaml.orisudo vim install/kubernetes/istio-demo.yaml...apiVersion: extensions/v1beta1# kind: Deployment# 使用DaemonSet部署方式kind: DaemonSetmetadata: name: istio-ingressgateway namespace: istio-system labels: app: ingressgateway chart: gateways-1.0.0 release: RELEASE-NAME heritage: Tiller app: istio-ingressgateway istio: ingressgatewayspec: # DaemonSet不支持replicas # replicas: 1 template: metadata: labels: app: istio-ingressgateway istio: ingressgateway annotations: sidecar.istio.io/inject: "false" scheduler.alpha.kubernetes.io/critical-pod: "" spec: serviceAccountName: istio-ingressgateway-service-account containers: - name: ingressgateway image: "gcr.io/istio-release/proxyv2:1.0.0" imagePullPolicy: IfNotPresent ports: - containerPort: 80 # 主机80端口映射 hostPort: 80 - containerPort: 443 # 主机443端口映射 hostPort: 443...# 替换镜像地址sudo sed -i 's@gcr.io/istio-release@docker.io/istio@g' install/kubernetes/istio-demo.yamlsudo sed -i 's@quay.io/coreos/hyperkube:v1.7.6_coreos.0@registry.cn-shanghai.aliyuncs.com/gcr-k8s/hyperkube:v1.7.6_coreos.0@g' install/kubernetes/istio-demo.yaml# 查看镜像地址grep 'image:' install/kubernetes/istio-demo.yaml# 安装 CRDs# 等待数秒kubectl apply -f install/kubernetes/helm/istio/templates/crds.yaml -n istio-systemkubectl get crd# 安装不使用认证（不使用tls）# 如果机器内存过小会无法成功启动# 实验使用3台虚拟机每台3G内存kubectl apply -f install/kubernetes/istio-demo.yaml# 查看状态kubectl get svc -n istio-systemkubectl get pods -n istio-system 注意 istio-1.0.0 默认已经开启了自动注入功能以及其他日志监控和追踪的相关组件如 istio-tracing istio-telemetry grafana prometheus servicegraph 启用自动注入 sidecar 不开启自动注入部署应用需要使用如下方式的命令kubectl apply -f &lt;(istioctl kube-inject -f samples/bookinfo/kube/bookinfo.yaml) 开启自动注入后，使用正常命令即可部署应用kubectl apply -f samples/bookinfo/kube/bookinfo.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041# istio-1.0.0 默认已经开启了自动注入功能# k8s 1.9 及之后的版本才能使用自动注入功能# 查看是否支持kubectl api-versions | grep admissionregistration# 除了要满足以上条件外还需要检查kube-apiserver启动的参数# k8s 1.9 版本要确保 --admission-control 里有 MutatingAdmissionWebhook,ValidatingAdmissionWebhook# k8s 1.9 之后的版本要确保 --enable-admission-plugins 里有MutatingAdmissionWebhook,ValidatingAdmissionWebhook# 测试自动注入# 创建kubectl apply -f samples/sleep/sleep.yaml kubectl get deployment -o widekubectl get pod# 设置 default namespace 开启自动注入kubectl label namespace default istio-injection=enabledkubectl get namespace -L istio-injection# 删除创建的pod，等待重建kubectl delete pod $(kubectl get pod | grep sleep | cut -d ' ' -f 1)# 查看重建后的pod# 查看是否有istio-proxy容器kubectl get podkubectl describe pod $(kubectl get pod | grep sleep | cut -d ' ' -f 1)# 清理kubectl delete -f samples/sleep/sleep.yaml # 关闭自动注入kubectl label namespace default istio-injection-# 关闭部分pod的自动注入功能... template: metadata: annotations: sidecar.istio.io/inject: "false"... 部署官方测试用例12345678910111213# default开启自动注入kubectl label namespace default istio-injection=enabled# 部署 bookinfokubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml# 创建 gatewaykubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml# 查看状态kubectl get serviceskubectl get podsistioctl get gateway 访问测试123456789101112131415161718# 命令行访问测试export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='&#123;.spec.ports[?(@.name=="http2")].nodePort&#125;')NODE_NAME=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')NODE_IP=$(ping -c 1 $NODE_NAME | grep PING | awk '&#123;print $3&#125;' | tr -d '()')export GATEWAY_URL=$NODE_IP:$INGRESS_PORTecho $GATEWAY_URLcurl -o /dev/null -s -w "%&#123;http_code&#125;\n" http://$&#123;GATEWAY_URL&#125;/productpage# 浏览器访问测试echo "http://$&#123;GATEWAY_URL&#125;/productpage"# 使用daemonset方式部署可以使用如下方式访问# 11.11.11.112为其中一个node节点的ipcurl http://11.11.11.112/productpage# 清理samples/bookinfo/platform/kube/cleanup.sh 清理12345# 清理istiokubectl delete -f install/kubernetes/helm/istio/templates/crds.yaml -n istio-systemkubectl delete -f install/kubernetes/istio-demo.yaml# kubectl delete -f install/kubernetes/istio-demo-auth.yaml 使用helm安装istio安装helm参考文章 安装istio123456789101112131415161718192021222324252627# 查看配置cd /usr/local/istioegrep -v "^$|#" install/kubernetes/helm/istio/values.yaml# 安装 CRDskubectl apply -f install/kubernetes/helm/istio/templates/crds.yamlkubectl get crd# 根据上面查看的配置和需求配置相关参数# 部署helm install install/kubernetes/helm/istio --name istio --namespace istio-system \--set ingress.enabled=false \--set global.hub="docker.io/istio" \--set global.hyperkube.hub="registry.cn-shanghai.aliyuncs.com/gcr-k8s" \--set gateways.istio-ingressgateway.type=NodePort \--set gateways.istio-egressgateway.type=NodePort# 查看helm lskubectl get pods -n istio-systemkubectl get svc -n istio-system# 运行之前的测试# 清理helm delete --purge istiokubectl delete -f install/kubernetes/helm/istio/templates/crds.yaml -n istio-system 参考文档 https://istio.io/docs/setup/kubernetes/quick-start.html https://istio.io/docs/guides/bookinfo.html https://istio.io/docs/setup/kubernetes/sidecar-injection.html#automatic-sidecar-injection]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
        <tag>istio</tag>
        <tag>servicemesh</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s使用kube-router构建高可用可扩展ingress]]></title>
    <url>%2Fposts%2F38%2F</url>
    <content type="text"><![CDATA[简介使用kube-router实现k8s集群的ingress功能，高可用易扩展 环境说明本实验在已经安装配置好k8s集群基础之上进行实验，k8s安装参考博客其他文章。lab4作为一个路由器，转发lab5的请求 实验架构12345lab1: master 11.11.11.111lab2: node 11.11.11.112lab3: node 11.11.11.113lab4: router 11.11.11.114lab5: client 11.11.11.115 安装12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 本次实验重新创建了集群，使用之前测试其他网络插件的集群环境没有成功# 可能是由于环境干扰，实验时需要注意# 创建kube-router目录下载相关文件mkdir kube-router &amp;&amp; cd kube-routerrm -f generic-kuberouter-all-features-dsr.yamlwget https://raw.githubusercontent.com/mgxian/kube-router/master/generic-kuberouter-all-features-dsr.yaml# 启用pod网络通信，网络隔离策略，服务代理所有功能# CLUSTERCIDR kube-controller-manager 启动参数 --cluster-cidr 的值# APISERVER kube-apiserver 启动参数 --advertise-address 值CLUSTERCIDR='10.244.0.0/16'APISERVER='https://11.11.11.111:6443'sed -i "s;%APISERVER%;$APISERVER;g" generic-kuberouter-all-features-dsr.yamlsed -i "s;%CLUSTERCIDR%;$CLUSTERCIDR;g" generic-kuberouter-all-features-dsr.yaml# 修改配置 containers: - name: kube-router image: cloudnativelabs/kube-router imagePullPolicy: Always args: ... - --peer-router-ips=11.11.11.114 - --peer-router-asns=64513 - --cluster-asn=64512 - --advertise-external-ip=true ...# 部署kubectl apply -f generic-kuberouter-all-features-dsr.yaml# 删除kube-proxykubectl -n kube-system delete ds kube-proxy# 在每个节点上执行# 如果是二进制安装使用如下命令systemctl stop kube-proxy# 在每个节点上执行# 清理kube-proxy留下的规则docker run --privileged --net=host registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64:v1.10.2 kube-proxy --cleanup# 查看kubectl get pods -n kube-systemkubectl get svc -n kube-system 测试12345678910111213141516# 测试之前请先安装配置好kube-dns或者coredns# 启动用于测试的deploymentkubectl run nginx --replicas=2 --image=nginx:alpine --port=80kubectl expose deployment nginx --type=NodePort --name=example-service-nodeportkubectl expose deployment nginx --name=example-service# 查看kubectl get pods -o widekubectl get svc -o wide# dns及访问测试kubectl run curl --image=radial/busyboxplus:curl -i --ttynslookup kubernetesnslookup example-servicecurl example-service 在lab4配置quagga123456789101112131415161718192021222324252627282930# 安装yum install -y quagga# 配置cat &gt;/etc/quagga/bgpd.conf&lt;&lt;EOF! -*- bgp -*-!! BGPd sample configuratin file!! $Id: bgpd.conf.sample,v 1.1 2002/12/13 20:15:29 paul Exp $!hostname lab4password password!router bgp 64513 bgp router-id 11.11.11.114 maximum-paths 4 neighbor 11.11.11.111 remote-as 64512 neighbor 11.11.11.112 remote-as 64512 neighbor 11.11.11.113 remote-as 64512log stdoutEOF# 启动systemctl start bgpdsystemctl status bgpdsystemctl enable bgpd# 查看路由信息ip route 在lab4测试12345678910111213141516171819# 在lab1上修改 example-service 配置 external ipkubectl edit svc example-service...spec: clusterIP: 10.111.34.147 externalIPs: - 11.11.111.111...# 在lab1上查看svc信息# 可以看到 example-service 有了 external ipkubectl get svc# 查看lab4路由# 可以看到有 11.11.111.111 相关的路由ip route# 在lab4上访问测试curl 11.11.111.111 在lab5测试123456789# 在lab5添加路由ip route add 11.11.111.111 via 11.11.11.114ip route# 在lab5访问测试curl 11.11.111.111# 在lab1查看ipvsipvsadm -L -n 使用DSR1234567891011121314# DSR实验没有成功，实验环境是vagrant配合virtualbox# 在lab1设置 example-service 使用 DSR 模式# 服务的响应直接发送到客户端不经过lvs中转kubectl annotate svc example-service "kube-router.io/service.dsr=tunnel"# 在lab1查看ipvs# 可以看到 Tunnel 转发类型ipvsadm -L -n# 在lab5访问测试curl 11.11.111.111# 在集群中的节点抓包分析tcpdump -i kube-bridge proto 4 清理123# 清理kubectl delete svc example-service example-service-nodeportkubectl delete deploy nginx curl 参考文档 https://cloudnativelabs.github.io/post/2017-11-01-kube-high-available-ingress/ https://github.com/cloudnativelabs/kube-router/blob/master/docs/generic.md]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s使用kube-router暴露集群中的pod和svc到外部]]></title>
    <url>%2Fposts%2F37%2F</url>
    <content type="text"><![CDATA[简介使用kube-router把k8s集群中的pod ip和cluter i暴露集群外部，实现集群外的节点直接访问k8s的pod和svc 环境说明本实验在已经安装配置好k8s集群基础之上进行实验，k8s安装参考博客其他文章。 实验架构1234lab1: master 11.11.11.111lab2: node 11.11.11.112lab3: node 11.11.11.113lab4: external 11.11.11.114 安装12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 本次实验重新创建了集群，使用之前测试其他网络插件的集群环境没有成功# 可能是由于环境干扰，实验时需要注意# 创建kube-router目录下载相关文件mkdir kube-router &amp;&amp; cd kube-routerrm -f generic-kuberouter-all-features.yamlwget https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/generic-kuberouter-all-features-advertise-routes.yaml# 启用pod网络通信，网络隔离策略，服务代理所有功能# CLUSTERCIDR kube-controller-manager 启动参数 --cluster-cidr 的值# APISERVER kube-apiserver 启动参数 --advertise-address 值CLUSTERCIDR='10.244.0.0/16'APISERVER='https://11.11.11.111:6443'sed -i "s;%APISERVER%;$APISERVER;g" generic-kuberouter-all-features-advertise-routes.yamlsed -i "s;%CLUSTERCIDR%;$CLUSTERCIDR;g" generic-kuberouter-all-features-advertise-routes.yaml# 修改配置 containers: - name: kube-router image: cloudnativelabs/kube-router imagePullPolicy: Always args: ... - "--peer-router-ips=11.11.11.114" - "--peer-router-asns=64513" - "--cluster-asn=64512" - "--advertise-cluster-ip=true" ...# 部署kubectl apply -f generic-kuberouter-all-features-advertise-routes.yaml# 删除kube-proxykubectl -n kube-system delete ds kube-proxy# 在每个节点上执行# 如果是二进制安装使用如下命令systemctl stop kube-proxy# 在每个节点上执行# 清理kube-proxy留下的规则docker run --privileged --net=host registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64:v1.10.2 kube-proxy --cleanup# 查看kubectl get pods -n kube-systemkubectl get svc -n kube-system 测试12345678910111213141516# 测试之前请先安装配置好kube-dns或者coredns# 启动用于测试的deploymentkubectl run nginx --replicas=2 --image=nginx:alpine --port=80kubectl expose deployment nginx --type=NodePort --name=example-service-nodeportkubectl expose deployment nginx --name=example-service# 查看kubectl get pods -o widekubectl get svc -o wide# dns及访问测试kubectl run curl --image=radial/busyboxplus:curl -i --ttynslookup kubernetesnslookup example-servicecurl example-service 在lab4配置quagga1234567891011121314151617181920212223242526272829# 安装yum install -y quagga# 配置cat &gt;/etc/quagga/bgpd.conf&lt;&lt;EOF! -*- bgp -*-!! BGPd sample configuratin file!! $Id: bgpd.conf.sample,v 1.1 2002/12/13 20:15:29 paul Exp $!hostname lab4password zebra!router bgp 64513 bgp router-id 11.11.11.114 neighbor 11.11.11.111 remote-as 64512 neighbor 11.11.11.112 remote-as 64512 neighbor 11.11.11.113 remote-as 64512log stdoutEOF# 启动systemctl start bgpdsystemctl status bgpdsystemctl enable bgpd# 查看路由信息ip route 在lab4测试访问k8s集群中的pod和svc123456789# 在lab1上获取pod和svc信息kubectl get pods -o widekubectl get svc# 在lab4上访问# 10.244.2.11 其中一个 nginx pod 的ip# 10.106.123.190 为 example-service 的 cluster ipcurl 10.244.2.11curl 10.106.123.190 清理123# 清理kubectl delete svc example-service example-service-nodeportkubectl delete deploy nginx curl 参考文档 https://cloudnativelabs.github.io/post/2017-05-22-kube-pod-networking/ https://github.com/cloudnativelabs/kube-router/blob/master/docs/generic.md]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s使用kube-router网络组件并实现网络隔离]]></title>
    <url>%2Fposts%2F36%2F</url>
    <content type="text"><![CDATA[简介本文章主要介绍k8s如何使用kube-router实现pod通信，服务代理，网络策略隔离等功能 kube-router是一个新的k8s的网络插件，使用lvs做服务的代理及负载均衡，使用iptables来做网络的隔离策略。部署简单，只需要在每个节点部署一个daemonset即可，高性能，易维护。支持pod间通信，以及服务的代理。 环境说明本实验在已经安装配置好k8s集群基础之上进行实验，k8s安装参考博客其他文章。 实验架构123lab1: master 11.11.11.111lab2: node 11.11.11.112lab3: node 11.11.11.113 安装12345678910111213141516171819202122232425262728293031# 本次实验重新创建了集群，使用之前测试其他网络插件的集群环境没有成功# 可能是由于环境干扰，实验时需要注意# 创建kube-router目录下载相关文件mkdir kube-router &amp;&amp; cd kube-routerrm -f generic-kuberouter-all-features.yamlwget https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/generic-kuberouter-all-features.yaml# 启用pod网络通信，网络隔离策略，服务代理所有功能# CLUSTERCIDR kube-controller-manager 启动参数 --cluster-cidr 的值# APISERVER kube-apiserver 启动参数 --advertise-address 值CLUSTERCIDR='10.244.0.0/16'APISERVER='https://11.11.11.111:6443'sed -i "s;%APISERVER%;$APISERVER;g" generic-kuberouter-all-features.yamlsed -i "s;%CLUSTERCIDR%;$CLUSTERCIDR;g" generic-kuberouter-all-features.yamlkubectl apply -f generic-kuberouter-all-features.yaml# 删除kube-proxykubectl -n kube-system delete ds kube-proxy# 在每个节点上执行# 如果是二进制安装使用如下命令systemctl stop kube-proxy# 在每个节点上执行# 清理kube-proxy留下的规则docker run --privileged --net=host registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64:v1.10.2 kube-proxy --cleanup# 查看kubectl get pods -n kube-systemkubectl get svc -n kube-system 测试1234567891011121314151617181920# 测试之前请先安装配置好kube-dns或者coredns# 启动用于测试的deploymentkubectl run nginx --replicas=2 --image=nginx:alpine --port=80kubectl expose deployment nginx --type=NodePort --name=example-service-nodeportkubectl expose deployment nginx --name=example-service# 查看kubectl get pods -o widekubectl get svc -o wide# dns及访问测试kubectl run curl --image=radial/busyboxplus:curl -i --ttynslookup kubernetesnslookup example-servicecurl example-service# 清理kubectl delete svc example-service example-service-nodeportkubectl delete deploy nginx curl 网络隔离策略部署应用123456789101112131415# 创建 production staging 命名空间kubectl create namespace productionkubectl create namespace staging# 在每个命名空间各部署一套服务cd kube-routerwget https://raw.githubusercontent.com/mgxian/istio-test/master/service/node/v1/node-v1.ymlwget https://raw.githubusercontent.com/mgxian/istio-test/master/service/go/v1/go-v1.ymlkubectl apply -f node-v1.yml -n productionkubectl apply -f go-v1.yml -n productionkubectl apply -f node-v1.yml -n stagingkubectl apply -f go-v1.yml -n staging# 查看状态kubectl get pods --all-namespaces -o wide 测试pod通信123456789101112131415161718# 获取相关POD信息PRODUCTION_NODE_NAME=$(kubectl get pods -n production | grep Running | grep service-node | awk '&#123;print $1&#125;')STAGING_NODE_NAME=$(kubectl get pods -n staging | grep Running | grep service-node | awk '&#123;print $1&#125;')PRODUCTION_GO_IP=$(kubectl get pods -n production -o wide | grep Running | grep service-go | awk '&#123;print $6&#125;')STAGING_GO_IP=$(kubectl get pods -n staging -o wide | grep Running | grep service-go | awk '&#123;print $6&#125;')echo $PRODUCTION_NODE_NAME $PRODUCTION_GO_IPecho $STAGING_NODE_NAME $STAGING_GO_IP# 同namespace的pod通信kubectl exec -it $PRODUCTION_NODE_NAME --namespace=production -- ping -c4 $PRODUCTION_GO_IP kubectl exec -it $STAGING_NODE_NAME --namespace=staging -- ping -c4 $STAGING_GO_IP # 不同namespace的pod通信kubectl exec -it $PRODUCTION_NODE_NAME --namespace=production -- ping -c4 $STAGING_GO_IPkubectl exec -it $STAGING_NODE_NAME --namespace=staging -- ping -c4 $PRODUCTION_GO_IP# 结论：任何namespace的任何pod间都可以直接通信 设置默认策略测试123456789101112131415161718192021222324# 设置默认策略为拒绝所有流量cat &gt;default-deny.yml&lt;&lt;EOFapiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: default-denyspec: podSelector: &#123;&#125; policyTypes: - IngressEOFkubectl apply -f default-deny.yml -n productionkubectl apply -f default-deny.yml -n staging# 测试通信# 同namespace的pod通信kubectl exec -it $PRODUCTION_NODE_NAME --namespace=production -- ping -c4 $PRODUCTION_GO_IP kubectl exec -it $STAGING_NODE_NAME --namespace=staging -- ping -c4 $STAGING_GO_IP # 不同namespace的pod通信kubectl exec -it $PRODUCTION_NODE_NAME --namespace=production -- ping -c4 $STAGING_GO_IPkubectl exec -it $STAGING_NODE_NAME --namespace=staging -- ping -c4 $PRODUCTION_GO_IP# 结论：所有pod间都不能通信 设置允许规则1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# 设置 service-go 允许从 service-node 访问cat &gt;service-go-allow-service-node.yml&lt;&lt;EOFapiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: service-go-allow-service-nodespec: podSelector: matchLabels: app: service-go ingress: - from: - podSelector: matchLabels: app: service-nodeEOFkubectl apply -f service-go-allow-service-node.yml -n productionkubectl apply -f service-go-allow-service-node.yml -n staging# 设置 service-node 允许 访问 tcp 80 端口cat &gt;service-node-allow-tcp-80.yml&lt;&lt;EOFapiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: service-node-allow-tcp-80spec: podSelector: matchLabels: app: service-node ingress: - from: ports: - protocol: TCP port: 80EOFkubectl apply -f service-node-allow-tcp-80.yml -n productionkubectl apply -f service-node-allow-tcp-80.yml -n staging# 测试通信# 同namespace的pod通信kubectl exec -it $PRODUCTION_NODE_NAME --namespace=production -- ping -c4 $PRODUCTION_GO_IP kubectl exec -it $STAGING_NODE_NAME --namespace=staging -- ping -c4 $STAGING_GO_IP # 不同namespace的pod通信kubectl exec -it $PRODUCTION_NODE_NAME --namespace=production -- ping -c4 $STAGING_GO_IPkubectl exec -it $STAGING_NODE_NAME --namespace=staging -- ping -c4 $PRODUCTION_GO_IP# 通过service测试PRODUCTION_GO_SVC=$(kubectl get svc -n production | grep service-go | awk '&#123;print $3&#125;')STAGING_GO_SVC=$(kubectl get svc -n staging | grep service-go | awk '&#123;print $3&#125;')echo $PRODUCTION_GO_SVC $STAGING_GO_SVCcurl $PRODUCTION_GO_SVCcurl $STAGING_GO_SVC# 结论：同一namespace的pod间可以通信，不同namespace的pod间不可以通信，只允许配置了网络规则的pod间通信# 通过 service 也无法绕过网络隔离策略 清理123# 删除 namespace 自动删除相关资源kubectl delete ns productionkubectl delete ns staging 参考文档 https://github.com/cloudnativelabs/kube-router/blob/master/docs/generic.md https://kubernetes.io/docs/concepts/services-networking/network-policies/ https://cloudnativelabs.github.io/post/2017-05-1-kube-network-policies/]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7手动安装k8s-1.11版本]]></title>
    <url>%2Fposts%2F35%2F</url>
    <content type="text"><![CDATA[简介本文章主要介绍如何通过使用官方提供的二进制包安装配置k8s集群 实验环境说明实验架构123lab1: master 11.11.11.111lab2: node 11.11.11.112lab3: node 11.11.11.113 实验使用的Vagrantfile123456789101112131415161718192021# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| (1..3).each do |i| config.vm.define "lab#&#123;i&#125;" do |node| node.vm.box = "centos-7.4-docker-17" node.ssh.insert_key = false node.vm.hostname = "lab#&#123;i&#125;" node.vm.network "private_network", ip: "11.11.11.11#&#123;i&#125;" node.vm.provision "shell", inline: "echo hello from node #&#123;i&#125;" node.vm.provider "virtualbox" do |v| v.cpus = 2 v.customize ["modifyvm", :id, "--name", "lab#&#123;i&#125;", "--memory", "2048"] end end endend 安装关闭防火墙12systemctl stop firewalldsystemctl disable firewalld 配置系统相关参数 如下操作在所有节点操作 1234567891011121314151617181920212223242526272829303132333435363738394041# 临时禁用selinux# 永久关闭 修改/etc/sysconfig/selinux文件设置sed -i 's/SELINUX=permissive/SELINUX=disabled/' /etc/sysconfig/selinuxsetenforce 0# 临时关闭swap# 永久关闭 注释/etc/fstab文件里swap相关的行swapoff -a# 开启forward# Docker从1.13版本开始调整了默认的防火墙规则# 禁用了iptables filter表中FOWARD链# 这样会引起Kubernetes集群中跨Node的Pod无法通信iptables -P FORWARD ACCEPT# 配置转发相关参数，否则可能会出错cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1vm.swappiness=0EOFsysctl --system# 加载ipvs相关内核模块# 如果重新开机，需要重新加载modprobe ip_vsmodprobe ip_vs_rrmodprobe ip_vs_wrrmodprobe ip_vs_shmodprobe nf_conntrack_ipv4lsmod | grep ip_vs# 配置开启自加载cat &gt;/etc/modules-load.d/k8s-ipvs.conf&lt;&lt;EOFip_vsip_vs_rrip_vs_wrrip_vs_shnf_conntrack_ipv4EOF 配置hosts解析 如下操作在所有节点操作 12345cat &gt;&gt;/etc/hosts&lt;&lt;EOF11.11.11.111 lab111.11.11.112 lab211.11.11.113 lab3EOF 安装配置docker v1.11.0版本推荐使用docker v17.03,v1.11,v1.12,v1.13, 也可以使用，再高版本的docker可能无法正常使用。测试发现17.09无法正常使用，不能使用资源限制(内存CPU) 如下操作在所有节点操作 安装docker12345# 卸载安装指定版本docker-ceyum remove -y docker-ce docker-ce-selinux container-selinuxyum install -y --setopt=obsoletes=0 \docker-ce-17.03.1.ce-1.el7.centos \docker-ce-selinux-17.03.1.ce-1.el7.centos 启动docker1systemctl enable docker &amp;&amp; systemctl restart docker 安装CFSSL 只在lab1节点操作 1234567891011121314# 下载# 百度云链接：https://pan.baidu.com/s/1kgV40nwHy1IKnnLD6zH4cQ 密码：alyjmkdir -pv /server/software/k8scd /server/software/k8syum install -y wgetwget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64# 安装mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfomv cfssl_linux-amd64 /usr/local/bin/cfsslmv cfssljson_linux-amd64 /usr/local/bin/cfssljsonchmod +x /usr/local/bin/cfssl* 配置CA 只在lab1节点操作 此处的CA配置，后面配置etcd和k8s时都需要使用 123456789101112131415161718192021mkdir -pv $HOME/ssl &amp;&amp; cd $HOME/sslcat &gt;ca-config.json&lt;&lt;EOF&#123; "signing": &#123; "default": &#123; "expiry": "87600h" &#125;, "profiles": &#123; "kubernetes": &#123; "usages": [ "signing", "key encipherment", "server auth", "client auth" ], "expiry": "87600h" &#125; &#125; &#125;&#125;EOF 配置etcd集群生成etcd-ca 只在lab1节点操作 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# 写入配置cat &gt;etcd-ca-csr.json&lt;&lt;EOF&#123; "CN": "etcd", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "etcd", "OU": "Etcd Security" &#125; ]&#125;EOF# 生成 etcd root cacfssl gencert -initca etcd-ca-csr.json | cfssljson -bare etcd-cacat &gt;etcd-csr.json&lt;&lt;EOF&#123; "CN": "etcd", "hosts": [ "127.0.0.1", "11.11.11.111", "11.11.11.112", "11.11.11.113" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "etcd", "OU": "Etcd Security" &#125; ]&#125;EOF# 生成 etcd cacfssl gencert -ca=etcd-ca.pem -ca-key=etcd-ca-key.pem -config=ca-config.json \-profile=kubernetes etcd-csr.json | cfssljson -bare etcdmkdir -pv /etc/etcd/sslcp etcd*.pem /etc/etcd/sslls /etc/etcd/ssl/etcd*.pem# 复制到其他节点cd /etc/etcd &amp;&amp; tar cvzf etcd-ssl.tgz ssl/scp /etc/etcd/etcd-ssl.tgz lab2:~/scp /etc/etcd/etcd-ssl.tgz lab3:~/ssh lab2 'mkdir -pv /etc/etcd &amp;&amp; tar xf etcd-ssl.tgz -C /etc/etcd &amp;&amp; ls -l /etc/etcd/ssl'ssh lab3 'mkdir -pv /etc/etcd &amp;&amp; tar xf etcd-ssl.tgz -C /etc/etcd &amp;&amp; ls -l /etc/etcd/ssl' 安装启动etcd 如下操作在所有节点操作 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758# 安装# 百度云链接：https://pan.baidu.com/s/1IVHyMqiJrlq9gmbF49Ly3Q 密码：w5nxmkdir -pv /server/software/k8scd /server/software/k8syum install -y wgetwget https://github.com/coreos/etcd/releases/download/v3.2.18/etcd-v3.2.18-linux-amd64.tar.gztar xf etcd-v3.2.18-linux-amd64.tar.gzmv etcd-v3.2.18-linux-amd64 /usr/local/etcd-v3.2.18ln -sv /usr/local/etcd-v3.2.18 /usr/local/etcdcd /usr/local/etcd &amp;&amp; mkdir bin &amp;&amp; mv etcd etcdctl bin/usr/local/etcd/bin/etcd --versioncd $HOME# 配置启动脚本export ETCD_NAME=$(hostname)export INTERNAL_IP=$(hostname -i | awk '&#123;print $NF&#125;')export ECTD_CLUSTER='lab1=https://11.11.11.111:2380,lab2=https://11.11.11.112:2380,lab3=https://11.11.11.113:2380'mkdir -pv /data/etcdcat &gt; /etc/systemd/system/etcd.service &lt;&lt;EOF[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.targetDocumentation=https://github.com/coreos[Service]Type=notifyWorkingDirectory=/data/etcdEnvironmentFile=-/etc/etcd/etcd.confExecStart=/usr/local/etcd/bin/etcd \\ --name $&#123;ETCD_NAME&#125; \\ --cert-file=/etc/etcd/ssl/etcd.pem \\ --key-file=/etc/etcd/ssl/etcd-key.pem \\ --peer-cert-file=/etc/etcd/ssl/etcd.pem \\ --peer-key-file=/etc/etcd/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/etcd/ssl/etcd-ca.pem \\ --peer-trusted-ca-file=/etc/etcd/ssl/etcd-ca.pem \\ --initial-advertise-peer-urls https://$&#123;INTERNAL_IP&#125;:2380 \\ --listen-peer-urls https://$&#123;INTERNAL_IP&#125;:2380 \\ --listen-client-urls https://$&#123;INTERNAL_IP&#125;:2379,https://127.0.0.1:2379 \\ --advertise-client-urls https://$&#123;INTERNAL_IP&#125;:2379 \\ --initial-cluster-token my-etcd-token \\ --initial-cluster $ECTD_CLUSTER \\ --initial-cluster-state new \\ --data-dir=/data/etcdRestart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF# 启动并设置开机启动systemctl daemon-reloadsystemctl start etcdsystemctl enable etcd 查看etcd集群状态12345/usr/local/etcd/bin/etcdctl --endpoints "https://127.0.0.1:2379" \ --ca-file=/etc/etcd/ssl/etcd-ca.pem \ --cert-file=/etc/etcd/ssl/etcd.pem \ --key-file=/etc/etcd/ssl/etcd-key.pem \ cluster-health 生成k8s集群的CA123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189# 进入相关目录cd $HOME/ssl# 配置 root cacat &gt;ca-csr.json&lt;&lt;EOF&#123; "CN": "kubernetes", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "System" &#125; ], "ca": &#123; "expiry": "87600h" &#125;&#125;EOF# 生成 root cacfssl gencert -initca ca-csr.json | cfssljson -bare cals ca*.pem# 配置 kube-apiserver ca# 10.96.0.1 是 kube-apiserver 指定的 service-cluster-ip-range 网段的第一个IPcat &gt;kube-apiserver-csr.json&lt;&lt;EOF&#123; "CN": "kube-apiserver", "hosts": [ "127.0.0.1", "11.11.11.111", "11.11.11.112", "11.11.11.113", "10.96.0.1", "kubernetes", "kubernetes.default", "kubernetes.default.svc", "kubernetes.default.svc.cluster", "kubernetes.default.svc.cluster.local" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "System" &#125; ]&#125;EOF# 生成 kube-apiserver cacfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json \-profile=kubernetes kube-apiserver-csr.json | cfssljson -bare kube-apiserverls kube-apiserver*.pem# 配置 kube-controller-manager cacat &gt;kube-controller-manager-csr.json&lt;&lt;EOF&#123; "CN": "system:kube-controller-manager", "hosts": [ "127.0.0.1", "11.11.11.111", "11.11.11.112", "11.11.11.113" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "system:kube-controller-manager", "OU": "System" &#125; ]&#125;EOF# 生成 kube-controller-manager cacfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json \-profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-managerls kube-controller-manager*.pem# 配置 kube-scheduler cacat &gt;kube-scheduler-csr.json&lt;&lt;EOF&#123; "CN": "system:kube-scheduler", "hosts": [ "127.0.0.1", "11.11.11.111", "11.11.11.112", "11.11.11.113" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "system:kube-scheduler", "OU": "System" &#125; ]&#125;EOF# 生成 kube-scheduler cacfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json \-profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-schedulerls kube-scheduler*.pem# 配置 kube-proxy cacat &gt;kube-proxy-csr.json&lt;&lt;EOF&#123; "CN": "system:kube-proxy", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "system:kube-proxy", "OU": "System" &#125; ]&#125;EOF# 生成 kube-proxy cacfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json \-profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxyls kube-proxy*.pem# 配置 admin cacat &gt;admin-csr.json&lt;&lt;EOF&#123; "CN": "admin", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "system:masters", "OU": "System" &#125; ]&#125;EOF# 生成 admin cacfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json \-profile=kubernetes admin-csr.json | cfssljson -bare adminls admin*.pem# 复制生成的camkdir -pv /etc/kubernetes/pkicp ca*.pem admin*.pem kube-proxy*.pem kube-scheduler*.pem kube-controller-manager*.pem kube-apiserver*.pem /etc/kubernetes/pkicd /etc/kubernetes &amp;&amp; tar cvzf pki.tgz pki/scp /etc/kubernetes/pki.tgz lab2:~/scp /etc/kubernetes/pki.tgz lab3:~/ssh lab2 'mkdir -pv /etc/kubernetes &amp;&amp; tar xf pki.tgz -C /etc/kubernetes &amp;&amp; ls -l /etc/kubernetes/pki'ssh lab3 'mkdir -pv /etc/kubernetes &amp;&amp; tar xf pki.tgz -C /etc/kubernetes &amp;&amp; ls -l /etc/kubernetes/pki'cd $HOME 安装k8s文件12345678910111213# 下载文件# 需要翻墙，如果不能翻墙使用如下链接下载# 链接：https://pan.baidu.com/s/1OI9Q4BRp7jNJUmsA8IAkbA 密码：tnx5cd /server/software/k8swget https://dl.k8s.io/v1.11.0/kubernetes-server-linux-amd64.tar.gztar xf kubernetes-server-linux-amd64.tar.gzcd kubernetes/server/binmkdir -pv /usr/local/kubernetes-v1.11.0/bincp kube-apiserver kube-controller-manager kube-scheduler kube-proxy kubelet kubectl /usr/local/kubernetes-v1.11.0/binln -sv /usr/local/kubernetes-v1.11.0 /usr/local/kubernetescp /usr/local/kubernetes/bin/kubectl /usr/local/bin/kubectlkubectl versioncd $HOME 生成kubeconfig123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899# 使用 TLS Bootstrapping export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')cat &gt; /etc/kubernetes/token.csv &lt;&lt;EOF$&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,"system:kubelet-bootstrap"EOF# 创建 kubelet bootstrapping kubeconfigcd /etc/kubernetesexport KUBE_APISERVER="https://11.11.11.111:6443"kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/pki/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kubelet-bootstrap.confkubectl config set-credentials kubelet-bootstrap \ --token=$&#123;BOOTSTRAP_TOKEN&#125; \ --kubeconfig=kubelet-bootstrap.confkubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=kubelet-bootstrap.confkubectl config use-context default --kubeconfig=kubelet-bootstrap.conf# 创建 kube-controller-manager kubeconfigexport KUBE_APISERVER="https://11.11.11.111:6443"kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/pki/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-controller-manager.confkubectl config set-credentials kube-controller-manager \ --client-certificate=/etc/kubernetes/pki/kube-controller-manager.pem \ --client-key=/etc/kubernetes/pki/kube-controller-manager-key.pem \ --embed-certs=true \ --kubeconfig=kube-controller-manager.confkubectl config set-context default \ --cluster=kubernetes \ --user=kube-controller-manager \ --kubeconfig=kube-controller-manager.confkubectl config use-context default --kubeconfig=kube-controller-manager.conf# 创建 kube-scheduler kubeconfigexport KUBE_APISERVER="https://11.11.11.111:6443"kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/pki/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-scheduler.confkubectl config set-credentials kube-scheduler \ --client-certificate=/etc/kubernetes/pki/kube-scheduler.pem \ --client-key=/etc/kubernetes/pki/kube-scheduler-key.pem \ --embed-certs=true \ --kubeconfig=kube-scheduler.confkubectl config set-context default \ --cluster=kubernetes \ --user=kube-scheduler \ --kubeconfig=kube-scheduler.confkubectl config use-context default --kubeconfig=kube-scheduler.conf# 创建 kube-proxy kubeconfigexport KUBE_APISERVER="https://11.11.11.111:6443"kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/pki/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-proxy.confkubectl config set-credentials kube-proxy \ --client-certificate=/etc/kubernetes/pki/kube-proxy.pem \ --client-key=/etc/kubernetes/pki/kube-proxy-key.pem \ --embed-certs=true \ --kubeconfig=kube-proxy.confkubectl config set-context default \ --cluster=kubernetes \ --user=kube-proxy \ --kubeconfig=kube-proxy.confkubectl config use-context default --kubeconfig=kube-proxy.conf# 创建 admin kubeconfigexport KUBE_APISERVER="https://11.11.11.111:6443"kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/pki/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=admin.confkubectl config set-credentials admin \ --client-certificate=/etc/kubernetes/pki/admin.pem \ --client-key=/etc/kubernetes/pki/admin-key.pem \ --embed-certs=true \ --kubeconfig=admin.confkubectl config set-context default \ --cluster=kubernetes \ --user=admin \ --kubeconfig=admin.confkubectl config use-context default --kubeconfig=admin.conf# 把 kube-proxy.conf 复制到其他节点scp kubelet-bootstrap.conf kube-proxy.conf lab2:/etc/kubernetesscp kubelet-bootstrap.conf kube-proxy.conf lab3:/etc/kubernetescd $HOME 配置master相关组件 只在lab1节点操作 配置启动kube-apiserver123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# 复制 etcd camkdir -pv /etc/kubernetes/pki/etcdcd /etc/etcd/sslcp etcd-ca.pem etcd-key.pem etcd.pem /etc/kubernetes/pki/etcd# 生成 service account keyopenssl genrsa -out /etc/kubernetes/pki/sa.key 2048openssl rsa -in /etc/kubernetes/pki/sa.key -pubout -out /etc/kubernetes/pki/sa.publs /etc/kubernetes/pki/sa.*cd $HOME# 启动文件cat &gt;/etc/systemd/system/kube-apiserver.service&lt;&lt;EOF[Unit]Description=Kubernetes API ServiceDocumentation=https://github.com/kubernetes/kubernetesAfter=network.target[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/apiserverExecStart=/usr/local/kubernetes/bin/kube-apiserver \\ \$KUBE_LOGTOSTDERR \\ \$KUBE_LOG_LEVEL \\ \$KUBE_ETCD_ARGS \\ \$KUBE_API_ADDRESS \\ \$KUBE_SERVICE_ADDRESSES \\ \$KUBE_ADMISSION_CONTROL \\ \$KUBE_APISERVER_ARGSRestart=on-failureType=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF# 该配置文件同时被 kube-apiserver, kube-controller-manager# kube-scheduler, kubelet, kube-proxy 使用cat &gt;/etc/kubernetes/config&lt;&lt;EOFKUBE_LOGTOSTDERR="--logtostderr=true"KUBE_LOG_LEVEL="--v=2"EOFcat &gt;/etc/kubernetes/apiserver&lt;&lt;EOFKUBE_API_ADDRESS="--advertise-address=11.11.11.111"KUBE_ETCD_ARGS="--etcd-servers=https://11.11.11.111:2379,https://11.11.11.112:2379,https://11.11.11.113:2379 --etcd-cafile=/etc/kubernetes/pki/etcd/etcd-ca.pem --etcd-certfile=/etc/kubernetes/pki/etcd/etcd.pem --etcd-keyfile=/etc/kubernetes/pki/etcd/etcd-key.pem"KUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=10.96.0.0/12"KUBE_ADMISSION_CONTROL="--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"KUBE_APISERVER_ARGS="--allow-privileged=true --authorization-mode=Node,RBAC --enable-bootstrap-token-auth=true --token-auth-file=/etc/kubernetes/token.csv --service-node-port-range=30000-32767 --tls-cert-file=/etc/kubernetes/pki/kube-apiserver.pem --tls-private-key-file=/etc/kubernetes/pki/kube-apiserver-key.pem --client-ca-file=/etc/kubernetes/pki/ca.pem --service-account-key-file=/etc/kubernetes/pki/sa.pub --enable-swagger-ui=true --secure-port=6443 --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --anonymous-auth=false --kubelet-client-certificate=/etc/kubernetes/pki/admin.pem --kubelet-client-key=/etc/kubernetes/pki/admin-key.pem"EOF# 启动systemctl daemon-reloadsystemctl enable kube-apiserversystemctl start kube-apiserversystemctl status kube-apiserver# 浏览器访问测试https://11.11.11.111:6443/swaggerapi 配置启动kube-controller-manager123456789101112131415161718192021222324252627282930# 启动文件cat &gt;/etc/systemd/system/kube-controller-manager.service&lt;&lt;EOFDescription=Kubernetes Controller ManagerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/controller-managerExecStart=/usr/local/kubernetes/bin/kube-controller-manager \\ \$KUBE_LOGTOSTDERR \\ \$KUBE_LOG_LEVEL \\ \$KUBECONFIG \\ \$KUBE_CONTROLLER_MANAGER_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOFcat &gt;/etc/kubernetes/controller-manager&lt;&lt;EOFKUBECONFIG="--kubeconfig=/etc/kubernetes/kube-controller-manager.conf"KUBE_CONTROLLER_MANAGER_ARGS="--address=127.0.0.1 --cluster-cidr=10.244.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/pki/ca.pem --cluster-signing-key-file=/etc/kubernetes/pki/ca-key.pem --service-account-private-key-file=/etc/kubernetes/pki/sa.key --root-ca-file=/etc/kubernetes/pki/ca.pem --leader-elect=true --use-service-account-credentials=true --node-monitor-grace-period=10s --pod-eviction-timeout=10s --allocate-node-cidrs=true --controllers=*,bootstrapsigner,tokencleaner"EOF# 启动systemctl daemon-reloadsystemctl enable kube-controller-managersystemctl start kube-controller-managersystemctl status kube-controller-manager 配置启动kube-scheduler123456789101112131415161718192021222324252627282930cat &gt;/etc/systemd/system/kube-scheduler.service&lt;&lt;EOF[Unit]Description=Kubernetes Scheduler PluginDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/schedulerExecStart=/usr/local/kubernetes/bin/kube-scheduler \\ \$KUBE_LOGTOSTDERR \\ \$KUBE_LOG_LEVEL \\ \$KUBECONFIG \\ \$KUBE_SCHEDULER_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOFcat &gt;/etc/kubernetes/scheduler&lt;&lt;EOFKUBECONFIG="--kubeconfig=/etc/kubernetes/kube-scheduler.conf"KUBE_SCHEDULER_ARGS="--leader-elect=true --address=127.0.0.1"EOF# 启动systemctl daemon-reloadsystemctl enable kube-schedulersystemctl start kube-schedulersystemctl status kube-scheduler 配置kubectl使用12345rm -rf $HOME/.kubemkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/configkubectl get no 查看组件状态1kubectl get componentstatuses 配置kubelet使用bootstrap1234# 将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper cluster 角色kubectl create clusterrolebinding kubelet-bootstrap \--clusterrole=system:node-bootstrapper \--user=kubelet-bootstrap 配置node相关组件 如下操作在所有节点操作 安装cni12345678# 安装 cni# 百度云链接：https://pan.baidu.com/s/1-PputObLs5jouXLnuBCI6Q 密码：tzqmcd /server/software/k8swget https://github.com/containernetworking/plugins/releases/download/v0.7.1/cni-plugins-amd64-v0.7.1.tgzmkdir -pv /opt/cni/bintar xf cni-plugins-amd64-v0.7.1.tgz -C /opt/cni/binls -l /opt/cni/bincd $HOME 配置启动kubelet1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# 启动文件mkdir -pv /data/kubeletcat &gt;/etc/systemd/system/kubelet.service&lt;&lt;EOF[Unit]Description=Kubernetes Kubelet ServerDocumentation=https://github.com/kubernetes/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/data/kubeletEnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/kubeletExecStart=/usr/local/kubernetes/bin/kubelet \\ \$KUBE_LOGTOSTDERR \\ \$KUBE_LOG_LEVEL \\ \$KUBELET_CONFIG \\ \$KUBELET_HOSTNAME \\ \$KUBELET_POD_INFRA_CONTAINER \\ \$KUBELET_ARGSRestart=on-failure[Install]WantedBy=multi-user.targetEOFcat &gt;/etc/kubernetes/config&lt;&lt;EOFKUBE_LOGTOSTDERR="--logtostderr=true"KUBE_LOG_LEVEL="--v=2"EOF# 注意修改相关ipcat &gt;/etc/kubernetes/kubelet&lt;&lt;EOFKUBELET_HOSTNAME="--hostname-override=11.11.11.111"KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1"KUBELET_CONFIG="--config=/etc/kubernetes/kubelet-config.yml"KUBELET_ARGS="--bootstrap-kubeconfig=/etc/kubernetes/kubelet-bootstrap.conf --kubeconfig=/etc/kubernetes/kubelet.conf --cert-dir=/etc/kubernetes/pki --network-plugin=cni --cni-bin-dir=/opt/cni/bin --cni-conf-dir=/etc/cni/net.d"EOF# 注意修改相关ip# lab1 lab2 lab3 使用各自ipcat &gt;/etc/kubernetes/kubelet-config.yml&lt;&lt;EOFkind: KubeletConfigurationapiVersion: kubelet.config.k8s.io/v1beta1address: 11.11.11.111port: 10250cgroupDriver: cgroupfsclusterDNS: - 10.96.0.10clusterDomain: cluster.local.hairpinMode: promiscuous-bridgeserializeImagePulls: falseauthentication: x509: clientCAFile: /etc/kubernetes/pki/ca.pemEOF# 启动systemctl daemon-reloadsystemctl enable kubeletsystemctl start kubeletsystemctl status kubelet 通过证书请求123456789101112131415# 在配置了kubectl的节点上执行如下操作# 查看kubectl get csr# 通过kubectl certificate approve node-csr-Yiiv675wUCvQl3HH11jDr0cC9p3kbrXWrxvG3EjWGoE# 查看节点# 此时节点状态为 NotReadykubectl get nodes# 在node节点查看生成的文件ls -l /etc/kubernetes/kubelet.confls -l /etc/kubernetes/pki/kubelet* 配置启动kube-proxy12345678910111213141516171819202122232425262728293031323334353637383940# 安装yum install -y conntrack-tools# 启动文件cat &gt;/etc/systemd/system/kube-proxy.service&lt;&lt;EOF[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/kubernetes/kubernetesAfter=network.target[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/proxyExecStart=/usr/local/kubernetes/bin/kube-proxy \\ \$KUBE_LOGTOSTDERR \\ \$KUBE_LOG_LEVEL \\ \$KUBECONFIG \\ \$KUBE_PROXY_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF# 注意修改相关ip# lab1 lab2 lab3 使用各自ip# 由于 1.11.0 ipvs 在centos7上有bug无法正常使用# 实验使用 iptables 模式# 以后版本可以使用 ipvs 模式cat &gt;/etc/kubernetes/proxy&lt;&lt;EOFKUBECONFIG="--kubeconfig=/etc/kubernetes/kube-proxy.conf"KUBE_PROXY_ARGS="--bind-address=11.11.11.111 --proxy-mode=iptables --hostname-override=11.11.11.111 --cluster-cidr=10.244.0.0/16"EOF# 启动systemctl daemon-reloadsystemctl enable kube-proxysystemctl start kube-proxysystemctl status kube-proxy 设置集群角色12345678910111213# 设置 lab1 为 masterkubectl label nodes 11.11.11.111 node-role.kubernetes.io/master=# 设置 lab2 lab3 为 nodekubectl label nodes 11.11.11.112 node-role.kubernetes.io/node=kubectl label nodes 11.11.11.113 node-role.kubernetes.io/node=# 设置 master 一般情况下不接受负载kubectl taint nodes 11.11.11.111 node-role.kubernetes.io/master=true:NoSchedule# 查看节点# 此时节点状态为 NotReadykubectl get no 配置使用flannel网络 在lab1操作 123456789101112131415161718192021222324252627282930313233343536373839404142# 下载配置mkdir flannel &amp;&amp; cd flannelwget https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml# 修改配置# 此处的ip配置要与上面kubeadm的pod-network一致 net-conf.json: | &#123; "Network": "10.244.0.0/16", "Backend": &#123; "Type": "vxlan" &#125; &#125;# 修改镜像image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64# 如果Node有多个网卡的话，参考flannel issues 39701，# https://github.com/kubernetes/kubernetes/issues/39701# 目前需要在kube-flannel.yml中使用--iface参数指定集群主机内网网卡的名称，# 否则可能会出现dns无法解析。容器无法通信的情况，需要将kube-flannel.yml下载到本地，# flanneld启动参数加上--iface=&lt;iface-name&gt; containers: - name: kube-flannel image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=eth1# 启动kubectl apply -f kube-flannel.yml# 查看kubectl get pods -n kube-systemkubectl get svc -n kube-system# 查看节点状态# 当 flannel pod 全部启动之后，节点状态为 Readykubectl get no 配置使用coredns 在lab1操作 123456789101112# 安装# 10.96.0.10 kubelet中配置的dnscd $HOME &amp;&amp; mkdir coredns &amp;&amp; cd corednswget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sedwget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/deploy.shchmod +x deploy.sh./deploy.sh -i 10.96.0.10 &gt; coredns.ymlkubectl apply -f coredns.yml# 查看kubectl get pods -n kube-systemkubectl get svc -n kube-system 测试启动1234kubectl run nginx --replicas=2 --image=nginx:alpine --port=80kubectl expose deployment nginx --type=NodePort --name=example-service-nodeportkubectl expose deployment nginx --name=example-servicekubectl scale --replicas=3 deployment/nginx 查看状态1234kubectl get deploy -o widekubectl get pods -o widekubectl get svc -o widekubectl describe svc example-service DNS解析1234kubectl run curl --image=radial/busyboxplus:curl -i --ttynslookup kubernetesnslookup example-servicecurl example-service 访问测试1234567# 10.96.59.56 为查看svc时获取到的clusteripcurl "10.107.91.153:80"# 32223 为查看svc时获取到的 nodeporthttp://11.11.11.111:32223/http://11.11.11.112:32223/http://11.11.11.113:32223/ 清理12kubectl delete svc example-service example-service-nodeportkubectl delete deploy nginx curl 参考文档 https://jimmysong.io/kubernetes-handbook/practice/install-kubernetes-on-centos.html https://zhangguanzhang.github.io/2018/05/05/Kubernetes_install/ https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/ https://kairen.github.io/2018/04/05/kubernetes/deploy/manual-v1.10/ http://www.maogx.win/posts/7/ https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ https://kubernetes-v1-4.github.io/docs/user-guide/kubectl/kubectl_label/ https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ https://github.com/gjmzj/kubeasz]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes基础概念总结]]></title>
    <url>%2Fposts%2F34%2F</url>
    <content type="text"><![CDATA[简介kubernetes是由google主导开发的开源容器管理平台，提供多主机集群，容器编排，容器伸缩等功能 基础组件介绍docker容器运行环境的一种实现，封装底层容器 etcdetcd是kubernetes集群用来存储集群相关数据的数据仓库 master节点组件master节点是主集群中的大脑，负责处理外部的api请求，分配调度任务以及管理容器的副本数等 kube-apiserver kubernetes对外的服务入口，其他组件通信的纽带，服务无状态，可水平扩容 kube-scheduler 负责pod的任务调度 kube-controller-manager 处理node节点当机情况 负责保证pod的副本数 管理endpoint，连接service和pod 为新namespace创建默认api token和accounts node节点组件node节点负责干活，执行master节点指派的相关任务 kubelet 负责启动停止容器，保证容器运行。 kube-proxy 负责根据service生成网络规则，生成路由规则 组件通信 架构图 组件间的通信全依赖于kube-apiserver，其他组件通过http协议与kube-apiserver交互。 当使用kubectl或者直接调用kube-apiserver提供的api请求创建pod里和service时工作流程如下： kube-apiserver把相关的pod和service配置存储到etcd中 kube-scheduler从kube-apiserver获取到相关pod的配置，根据集群中的资源和条件限制把pod调度到相应的node节点上 kube-controller-manager从kube-apiserver获取到相关pod和service的配置，定期检查pod的状态，保证有用户配置的足够数量的pod副本在运行，生成service到pod的规则关系。 kubelet从kube-apiserver获取分配到本节点的相关pod配置，在本地启动容器并定期检查返回容器状态 kube-proxy从kube-apiserver获取service到pod的规则，在本节点维护iptable或者ipvs相关路由规则 基础概念pod kubernetes的最小调度单元，一个pod中可以有多个容器，多个容器共享网络和存储卷 service kubernetes抽象出来一个概念，可以理解为负载均衡器，后端接pod cluster ip service在集群中的ip，相当负载均衡器的ip ingress 对集群外部暴露集群内部service的一种方式 nodeport 对集群外部暴露服务的第二种方式，跟随service配置，让集群中的node节点都监听相应的端口，可以通过node节点访问集群内部service loadbalancer 对集群外部暴露集群内部service的一种方式，一般只有在云平台才能使用 deployment 封装了pod/replicaset，可以实现指定pod副本数量，滚动更新，扩容pod，一般一个应用（服务）一个deployment statefullSet 相当于有状态服务的deployment，重启后，主机名和pod的名称不会改变 daemonSet 在每个node节点都需要运行的pod可以使用daemonSet job 一次性任务 cronJob 类似于crontab定时执行任务 dns kube-dns/coredns提供集群中的dns服务，可以解析service到cluster ip，实现服务发现 pv 管理员用来提前创建好的存储空间，供用户申请使用 pvc 用户用来申请存储空间 storageClass 定义存储类供pvc使用，当用户通过pvc并指定storageClass请求pv时kubernetes可以根据storageClass动态创建pv 排错技巧查看日志查看pod日志 kubectl logs pod-name 查看事件 kubectl describe pod-name 查看docker日志 通过kubectl get pod -o wide找到容器运行的node节点 在node节点上通过docker ps -a找到出错的容器 docker logs container-id 查看kubelet及其他组件日志 journalctl -u kubelet]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7使用kubeadm安装k8s-1.11版本多主高可用]]></title>
    <url>%2Fposts%2F33%2F</url>
    <content type="text"><![CDATA[实验环境说明实验架构图12345678lab1: etcd master haproxy keepalived 11.11.11.111lab2: etcd master haproxy keepalived 11.11.11.112lab3: etcd master haproxy keepalived 11.11.11.113lab4: node 11.11.11.114lab5: node 11.11.11.115lab6: node 11.11.11.116vip(loadblancer ip): 11.11.11.110 实验使用的Vagrantfile123456789101112131415161718192021# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| (1..6).each do |i| config.vm.define "lab#&#123;i&#125;" do |node| node.vm.box = "centos-7.4-docker-17" node.ssh.insert_key = false node.vm.hostname = "lab#&#123;i&#125;" node.vm.network "private_network", ip: "11.11.11.11#&#123;i&#125;" node.vm.provision "shell", inline: "echo hello from node #&#123;i&#125;" node.vm.provider "virtualbox" do |v| v.cpus = 2 v.customize ["modifyvm", :id, "--name", "lab#&#123;i&#125;", "--memory", "2048"] end end endend 关闭防火墙12systemctl stop firewalldsystemctl disable firewalld 安装配置docker v1.11.0版本推荐使用docker v17.03,v1.11,v1.12,v1.13, 也可以使用，再高版本的docker可能无法正常使用。测试发现17.09无法正常使用，不能使用资源限制(内存CPU) 如下操作在所有节点操作 安装docker12345# 卸载安装指定版本docker-ceyum remove -y docker-ce docker-ce-selinux container-selinuxyum install -y --setopt=obsoletes=0 \docker-ce-17.03.1.ce-1.el7.centos \docker-ce-selinux-17.03.1.ce-1.el7.centos 启动docker1systemctl enable docker &amp;&amp; systemctl restart docker 安装 kubeadm, kubelet 和 kubectl 如下操作在所有节点操作 使用阿里镜像安装12345678910111213# 配置源cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF# 安装yum install -y kubelet-1.11.2 kubeadm-1.11.2 kubectl-1.11.2 ipvsadm 配置系统相关参数1234567891011121314151617181920212223242526272829303132# 临时禁用selinux# 永久关闭 修改/etc/sysconfig/selinux文件设置sed -i 's/SELINUX=permissive/SELINUX=disabled/' /etc/sysconfig/selinuxsetenforce 0# 临时关闭swap# 永久关闭 注释/etc/fstab文件里swap相关的行swapoff -a# 开启forward# Docker从1.13版本开始调整了默认的防火墙规则# 禁用了iptables filter表中FOWARD链# 这样会引起Kubernetes集群中跨Node的Pod无法通信iptables -P FORWARD ACCEPT# 配置转发相关参数，否则可能会出错cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1vm.swappiness=0EOFsysctl --system# 加载ipvs相关内核模块# 如果重新开机，需要重新加载modprobe ip_vsmodprobe ip_vs_rrmodprobe ip_vs_wrrmodprobe ip_vs_shmodprobe nf_conntrack_ipv4lsmod | grep ip_vs 配置hosts解析 如下操作在所有节点操作 12345678cat &gt;&gt;/etc/hosts&lt;&lt;EOF11.11.11.111 lab111.11.11.112 lab211.11.11.113 lab311.11.11.114 lab411.11.11.115 lab511.11.11.116 lab6EOF 配置haproxy代理和keepalived 如下操作在节点lab1,lab2,lab3操作 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293# 拉取haproxy镜像docker pull haproxy:1.7.8-alpinemkdir /etc/haproxycat &gt;/etc/haproxy/haproxy.cfg&lt;&lt;EOFglobal log 127.0.0.1 local0 err maxconn 50000 uid 99 gid 99 #daemon nbproc 1 pidfile haproxy.piddefaults mode http log 127.0.0.1 local0 err maxconn 50000 retries 3 timeout connect 5s timeout client 30s timeout server 30s timeout check 2slisten admin_stats mode http bind 0.0.0.0:1080 log 127.0.0.1 local0 err stats refresh 30s stats uri /haproxy-status stats realm Haproxy\ Statistics stats auth will:will stats hide-version stats admin if TRUEfrontend k8s-https bind 0.0.0.0:8443 mode tcp #maxconn 50000 default_backend k8s-httpsbackend k8s-https mode tcp balance roundrobin server lab1 11.11.11.111:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server lab2 11.11.11.112:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server lab3 11.11.11.113:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3EOF# 启动haproxydocker run -d --name my-haproxy \-v /etc/haproxy:/usr/local/etc/haproxy:ro \-p 8443:8443 \-p 1080:1080 \--restart always \haproxy:1.7.8-alpine# 查看日志docker logs my-haproxy# 浏览器查看状态http://11.11.11.111:1080/haproxy-statushttp://11.11.11.112:1080/haproxy-status# 拉取keepalived镜像docker pull osixia/keepalived:1.4.4# 启动# 载入内核相关模块lsmod | grep ip_vsmodprobe ip_vs# 启动keepalived# eth1为本次实验11.11.11.0/24网段的所在网卡docker run --net=host --cap-add=NET_ADMIN \-e KEEPALIVED_INTERFACE=eth1 \-e KEEPALIVED_VIRTUAL_IPS="#PYTHON2BASH:['11.11.11.110']" \-e KEEPALIVED_UNICAST_PEERS="#PYTHON2BASH:['11.11.11.111','11.11.11.112','11.11.11.113']" \-e KEEPALIVED_PASSWORD=hello \--name k8s-keepalived \--restart always \-d osixia/keepalived:1.4.4# 查看日志# 会看到两个成为backup 一个成为masterdocker logs k8s-keepalived# 此时会配置 11.11.11.110 到其中一台机器# ping测试ping -c4 11.11.11.110# 如果失败后清理后，重新实验docker rm -f k8s-keepalivedip a del 11.11.11.110/32 dev eth1 配置启动kubelet 如下操作在所有节点操作 123456789101112# 配置kubelet使用国内pause镜像# 配置kubelet的cgroups# 获取docker的cgroupsDOCKER_CGROUPS=$(docker info | grep 'Cgroup' | cut -d' ' -f3)echo $DOCKER_CGROUPScat &gt;/etc/sysconfig/kubelet&lt;&lt;EOFKUBELET_EXTRA_ARGS="--cgroup-driver=$DOCKER_CGROUPS --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1"EOF# 启动systemctl daemon-reloadsystemctl enable kubelet &amp;&amp; systemctl restart kubelet 配置master配置第一个master节点 如下操作在lab1节点操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 1.11.0 版本 centos 下使用 ipvs 模式会出问题# 参考 https://github.com/kubernetes/kubernetes/issues/65461# 生成配置文件CP0_IP="11.11.11.111"CP0_HOSTNAME="lab1"cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha2kind: MasterConfigurationkubernetesVersion: v1.11.2imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containersapiServerCertSANs:- "lab1"- "lab2"- "lab3"- "11.11.11.111"- "11.11.11.112"- "11.11.11.113"- "11.11.11.110"- "127.0.0.1"api: advertiseAddress: $CP0_IP controlPlaneEndpoint: 11.11.11.110:8443etcd: local: extraArgs: listen-client-urls: "https://127.0.0.1:2379,https://$CP0_IP:2379" advertise-client-urls: "https://$CP0_IP:2379" listen-peer-urls: "https://$CP0_IP:2380" initial-advertise-peer-urls: "https://$CP0_IP:2380" initial-cluster: "$CP0_HOSTNAME=https://$CP0_IP:2380" serverCertSANs: - $CP0_HOSTNAME - $CP0_IP peerCertSANs: - $CP0_HOSTNAME - $CP0_IPcontrollerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 10.244.0.0/16 kubeProxy: config: # mode: ipvs mode: iptablesEOF# 提前拉取镜像# 如果执行失败 可以多次执行kubeadm config images pull --config kubeadm-master.config# 初始化# 注意保存返回的 join 命令kubeadm init --config kubeadm-master.config# 打包ca相关文件上传至其他master节点cd /etc/kubernetes &amp;&amp; tar cvzf k8s-key.tgz admin.conf pki/ca.* pki/sa.* pki/front-proxy-ca.* pki/etcd/ca.*scp k8s-key.tgz lab2:~/scp k8s-key.tgz lab3:~/ssh lab2 'tar xf k8s-key.tgz -C /etc/kubernetes/'ssh lab3 'tar xf k8s-key.tgz -C /etc/kubernetes/' 配置第二个master节点 如下操作在lab2节点操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980# 1.11.0 版本 centos 下使用 ipvs 模式会出问题# 参考 https://github.com/kubernetes/kubernetes/issues/65461# 生成配置文件CP0_IP="11.11.11.111"CP0_HOSTNAME="lab1"CP1_IP="11.11.11.112"CP1_HOSTNAME="lab2"cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha2kind: MasterConfigurationkubernetesVersion: v1.11.2imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containersapiServerCertSANs:- "lab1"- "lab2"- "lab3"- "11.11.11.111"- "11.11.11.112"- "11.11.11.113"- "11.11.11.110"- "127.0.0.1"api: advertiseAddress: $CP1_IP controlPlaneEndpoint: 11.11.11.110:8443etcd: local: extraArgs: listen-client-urls: "https://127.0.0.1:2379,https://$CP1_IP:2379" advertise-client-urls: "https://$CP1_IP:2379" listen-peer-urls: "https://$CP1_IP:2380" initial-advertise-peer-urls: "https://$CP1_IP:2380" initial-cluster: "$CP0_HOSTNAME=https://$CP0_IP:2380,$CP1_HOSTNAME=https://$CP1_IP:2380" initial-cluster-state: existing serverCertSANs: - $CP1_HOSTNAME - $CP1_IP peerCertSANs: - $CP1_HOSTNAME - $CP1_IPcontrollerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 10.244.0.0/16 kubeProxy: config: # mode: ipvs mode: iptablesEOF# 配置kubeletkubeadm alpha phase certs all --config kubeadm-master.configkubeadm alpha phase kubelet config write-to-disk --config kubeadm-master.configkubeadm alpha phase kubelet write-env-file --config kubeadm-master.configkubeadm alpha phase kubeconfig kubelet --config kubeadm-master.configsystemctl restart kubelet# 添加etcd到集群中CP0_IP="11.11.11.111"CP0_HOSTNAME="lab1"CP1_IP="11.11.11.112"CP1_HOSTNAME="lab2"KUBECONFIG=/etc/kubernetes/admin.conf kubectl exec -n kube-system etcd-$&#123;CP0_HOSTNAME&#125; -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://$&#123;CP0_IP&#125;:2379 member add $&#123;CP1_HOSTNAME&#125; https://$&#123;CP1_IP&#125;:2380kubeadm alpha phase etcd local --config kubeadm-master.config# 提前拉取镜像# 如果执行失败 可以多次执行kubeadm config images pull --config kubeadm-master.config# 部署kubeadm alpha phase kubeconfig all --config kubeadm-master.configkubeadm alpha phase controlplane all --config kubeadm-master.configkubeadm alpha phase mark-master --config kubeadm-master.config 配置第三个master节点 如下操作在lab3节点操作 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# 1.11.0 版本 centos 下使用 ipvs 模式会出问题# 参考 https://github.com/kubernetes/kubernetes/issues/65461# 生成配置文件CP0_IP="11.11.11.111"CP0_HOSTNAME="lab1"CP1_IP="11.11.11.112"CP1_HOSTNAME="lab2"CP2_IP="11.11.11.113"CP2_HOSTNAME="lab3"cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha2kind: MasterConfigurationkubernetesVersion: v1.11.2imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containersapiServerCertSANs:- "lab1"- "lab2"- "lab3"- "11.11.11.111"- "11.11.11.112"- "11.11.11.113"- "11.11.11.110"- "127.0.0.1"api: advertiseAddress: $CP2_IP controlPlaneEndpoint: 11.11.11.110:8443etcd: local: extraArgs: listen-client-urls: "https://127.0.0.1:2379,https://$CP2_IP:2379" advertise-client-urls: "https://$CP2_IP:2379" listen-peer-urls: "https://$CP2_IP:2380" initial-advertise-peer-urls: "https://$CP2_IP:2380" initial-cluster: "$CP0_HOSTNAME=https://$CP0_IP:2380,$CP1_HOSTNAME=https://$CP1_IP:2380,$CP2_HOSTNAME=https://$CP2_IP:2380" initial-cluster-state: existing serverCertSANs: - $CP2_HOSTNAME - $CP2_IP peerCertSANs: - $CP2_HOSTNAME - $CP2_IPcontrollerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 10.244.0.0/16 kubeProxy: config: # mode: ipvs mode: iptablesEOF# 配置kubeletkubeadm alpha phase certs all --config kubeadm-master.configkubeadm alpha phase kubelet config write-to-disk --config kubeadm-master.configkubeadm alpha phase kubelet write-env-file --config kubeadm-master.configkubeadm alpha phase kubeconfig kubelet --config kubeadm-master.configsystemctl restart kubelet# 添加etcd到集群中CP0_IP="11.11.11.111"CP0_HOSTNAME="lab1"CP2_IP="11.11.11.113"CP2_HOSTNAME="lab3"KUBECONFIG=/etc/kubernetes/admin.conf kubectl exec -n kube-system etcd-$&#123;CP0_HOSTNAME&#125; -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://$&#123;CP0_IP&#125;:2379 member add $&#123;CP2_HOSTNAME&#125; https://$&#123;CP2_IP&#125;:2380kubeadm alpha phase etcd local --config kubeadm-master.config# 提前拉取镜像# 如果执行失败 可以多次执行kubeadm config images pull --config kubeadm-master.config# 部署kubeadm alpha phase kubeconfig all --config kubeadm-master.configkubeadm alpha phase controlplane all --config kubeadm-master.configkubeadm alpha phase mark-master --config kubeadm-master.config 配置使用kubectl 如下操作在任意master节点操作 123456789101112rm -rf $HOME/.kubemkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# 查看node节点kubectl get nodes# 只有网络插件也安装配置完成之后，才能会显示为ready状态# 设置master允许部署应用pod，参与工作负载，现在可以部署其他系统组件# 如 dashboard, heapster, efk等kubectl taint nodes --all node-role.kubernetes.io/master- 配置使用网络插件 如下操作在任意master节点操作 1234567891011121314151617181920212223242526272829303132333435363738# 下载配置mkdir flannel &amp;&amp; cd flannelwget https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml# 修改配置# 此处的ip配置要与上面kubeadm的pod-network一致 net-conf.json: | &#123; "Network": "10.244.0.0/16", "Backend": &#123; "Type": "vxlan" &#125; &#125;# 修改镜像image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64# 如果Node有多个网卡的话，参考flannel issues 39701，# https://github.com/kubernetes/kubernetes/issues/39701# 目前需要在kube-flannel.yml中使用--iface参数指定集群主机内网网卡的名称，# 否则可能会出现dns无法解析。容器无法通信的情况，需要将kube-flannel.yml下载到本地，# flanneld启动参数加上--iface=&lt;iface-name&gt; containers: - name: kube-flannel image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=eth1# 启动kubectl apply -f kube-flannel.yml# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system 配置node节点加入集群 如下操作在所有node节点操作 12# 此命令为初始化master成功后返回的结果kubeadm join 11.11.11.110:8443 --token yzb7v7.dy40mhlljt1d48i9 --discovery-token-ca-cert-hash sha256:61ec309e6f942305006e6622dcadedcc64420e361231eff23cb535a183c0e77a 基础测试测试容器间的通信和DNS 配置好网络之后，kubeadm会自动部署coredns 如下测试可以在配置kubectl的节点上操作 启动123kubectl run nginx --replicas=2 --image=nginx:alpine --port=80kubectl expose deployment nginx --type=NodePort --name=example-service-nodeportkubectl expose deployment nginx --name=example-service 查看状态1234kubectl get deploykubectl get podskubectl get svckubectl describe svc example-service DNS解析1234kubectl run curl --image=radial/busyboxplus:curl -i --ttynslookup kubernetesnslookup example-servicecurl example-service 访问测试123456# 10.96.59.56 为查看svc时获取到的clusteripcurl "10.96.59.56:80"# 32223 为查看svc时获取到的 nodeporthttp://11.11.11.112:32223/http://11.11.11.113:32223/ 清理删除12kubectl delete svc example-service example-service-nodeportkubectl delete deploy nginx curl 高可用测试关闭任一master节点测试集群是能否正常执行上一步的基础测试，查看相关信息，不能同时关闭两个节点，因为3个节点组成的etcd集群，最多只能有一个当机。 12345678910111213141516# 查看组件状态kubectl get pod --all-namespaces -o widekubectl get pod --all-namespaces -o wide | grep lab1kubectl get pod --all-namespaces -o wide | grep lab2kubectl get pod --all-namespaces -o wide | grep lab3kubectl get nodes -o widekubectl get deploykubectl get podskubectl get svc# 访问测试CURL_POD=$(kubectl get pods | grep curl | grep Running | cut -d ' ' -f1)kubectl exec -ti $CURL_POD -- sh --ttynslookup kubernetesnslookup example-servicecurl example-service 小技巧忘记初始master节点时的node节点加入集群命令怎么办 123456# 简单方法kubeadm token create --print-join-command# 第二种方法token=$(kubeadm token generate)kubeadm token create $token --print-join-command --ttl=0 参考文档 https://kubernetes.io/docs/setup/independent/install-kubeadm/ https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/ https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/ https://kubernetes.io/docs/setup/independent/high-availability/ https://sealyun.com/post/k8s-ipvs/]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7使用kubeadm安装k8s-1.11版本]]></title>
    <url>%2Fposts%2F32%2F</url>
    <content type="text"><![CDATA[实验环境说明实验架构123lab1: master 11.11.11.111lab2: node 11.11.11.112lab3: node 11.11.11.113 实验使用的Vagrantfile123456789101112131415161718192021# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| (1..3).each do |i| config.vm.define "lab#&#123;i&#125;" do |node| node.vm.box = "centos-7.4-docker-17" node.ssh.insert_key = false node.vm.hostname = "lab#&#123;i&#125;" node.vm.network "private_network", ip: "11.11.11.11#&#123;i&#125;" node.vm.provision "shell", inline: "echo hello from node #&#123;i&#125;" node.vm.provider "virtualbox" do |v| v.cpus = 2 v.customize ["modifyvm", :id, "--name", "lab#&#123;i&#125;", "--memory", "2048"] end end endend 关闭防火墙12systemctl stop firewalldsystemctl disable firewalld 安装配置docker v1.11.0版本推荐使用docker v17.03,v1.11,v1.12,v1.13, 也可以使用，再高版本的docker可能无法正常使用。测试发现17.09无法正常使用，不能使用资源限制(内存CPU) 如下操作在所有节点操作 安装docker12345# 卸载安装指定版本docker-ceyum remove -y docker-ce docker-ce-selinux container-selinuxyum install -y --setopt=obsoletes=0 \docker-ce-17.03.1.ce-1.el7.centos \docker-ce-selinux-17.03.1.ce-1.el7.centos 启动docker1systemctl enable docker &amp;&amp; systemctl restart docker 安装 kubeadm, kubelet 和 kubectl 如下操作在所有节点操作 使用阿里镜像安装12345678910111213# 配置源cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF# 安装yum install -y kubelet-1.11.2 kubeadm-1.11.2 kubectl-1.11.2 ipvsadm 配置系统相关参数1234567891011121314151617181920212223242526272829303132# 临时禁用selinux# 永久关闭 修改/etc/sysconfig/selinux文件设置sed -i 's/SELINUX=permissive/SELINUX=disabled/' /etc/sysconfig/selinuxsetenforce 0# 临时关闭swap# 永久关闭 注释/etc/fstab文件里swap相关的行swapoff -a# 开启forward# Docker从1.13版本开始调整了默认的防火墙规则# 禁用了iptables filter表中FOWARD链# 这样会引起Kubernetes集群中跨Node的Pod无法通信iptables -P FORWARD ACCEPT# 配置转发相关参数，否则可能会出错cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1vm.swappiness=0EOFsysctl --system# 加载ipvs相关内核模块# 如果重新开机，需要重新加载modprobe ip_vsmodprobe ip_vs_rrmodprobe ip_vs_wrrmodprobe ip_vs_shmodprobe nf_conntrack_ipv4lsmod | grep ip_vs 配置hosts解析 如下操作在所有节点操作 12345cat &gt;&gt;/etc/hosts&lt;&lt;EOF11.11.11.111 lab111.11.11.112 lab211.11.11.113 lab3EOF 配置启动kubelet 如下操作在所有节点操作 123456789101112# 配置kubelet使用国内pause镜像# 配置kubelet的cgroups# 获取docker的cgroupsDOCKER_CGROUPS=$(docker info | grep 'Cgroup' | cut -d' ' -f3)echo $DOCKER_CGROUPScat &gt;/etc/sysconfig/kubelet&lt;&lt;EOFKUBELET_EXTRA_ARGS="--cgroup-driver=$DOCKER_CGROUPS --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1"EOF# 启动systemctl daemon-reloadsystemctl enable kubelet &amp;&amp; systemctl start kubelet 配置master节点 如下操作在master节点操作 12345678910111213141516171819202122232425262728293031# 1.11.0 版本 centos 下使用 ipvs 模式会出问题# 参考 https://github.com/kubernetes/kubernetes/issues/65461# 生成配置文件cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha2kind: MasterConfigurationkubernetesVersion: v1.11.2imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containersapi: advertiseAddress: 11.11.11.111controllerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 10.244.0.0/16 kubeProxy: config: mode: ipvs # mode: iptablesEOF# 提前拉取镜像# 如果执行失败 可以多次执行kubeadm config images pull --config kubeadm-master.config# 初始化kubeadm init --config kubeadm-master.config 配置使用kubectl 如下操作在master节点操作 123456789101112rm -rf $HOME/.kubemkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# 查看node节点kubectl get nodes# 只有网络插件也安装配置完成之后，才能会显示为ready状态# 设置master允许部署应用pod，参与工作负载，现在可以部署其他系统组件# 如 dashboard, heapster, efk等kubectl taint nodes --all node-role.kubernetes.io/master- 配置使用网络插件 如下操作在master节点操作 1234567891011121314151617181920212223242526272829303132333435363738# 下载配置mkdir flannel &amp;&amp; cd flannelwget https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml# 修改配置# 此处的ip配置要与上面kubeadm的pod-network一致 net-conf.json: | &#123; "Network": "10.244.0.0/16", "Backend": &#123; "Type": "vxlan" &#125; &#125;# 修改镜像image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64# 如果Node有多个网卡的话，参考flannel issues 39701，# https://github.com/kubernetes/kubernetes/issues/39701# 目前需要在kube-flannel.yml中使用--iface参数指定集群主机内网网卡的名称，# 否则可能会出现dns无法解析。容器无法通信的情况，需要将kube-flannel.yml下载到本地，# flanneld启动参数加上--iface=&lt;iface-name&gt; containers: - name: kube-flannel image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=eth1# 启动kubectl apply -f kube-flannel.yml# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system 配置node节点加入集群 如下操作在所有node节点操作 12# 此命令为初始化master成功后返回的结果kubeadm join 11.11.11.111:6443 --token yl53pn.wpx4mvx6a6jfkjhw --discovery-token-ca-cert-hash sha256:17751fcda3e79da63f5d0c4a3586e97de8b8b1d017c1a6977c88136409af5240 测试容器间的通信和DNS 配置好网络之后，kubeadm会自动部署coredns 启动123kubectl run nginx --replicas=2 --image=nginx:alpine --port=80kubectl expose deployment nginx --type=NodePort --name=example-service-nodeportkubectl expose deployment nginx --name=example-service 查看状态1234kubectl get deploykubectl get podskubectl get svckubectl describe svc example-service DNS解析1234kubectl run curl --image=radial/busyboxplus:curl -i --ttynslookup kubernetesnslookup example-servicecurl example-service 访问测试123456# 10.96.59.56 为查看svc时获取到的clusteripcurl "10.96.59.56:80"# 32223 为查看svc时获取到的 nodeporthttp://11.11.11.112:32223/http://11.11.11.113:32223/ 清理删除12kubectl delete svc example-service example-service-nodeportkubectl delete deploy nginx curl 小技巧忘记初始master节点时的node节点加入集群命令怎么办 123456# 简单方法kubeadm token create --print-join-command# 第二种方法token=$(kubeadm token generate)kubeadm token create $token --print-join-command --ttl=0 参考文档 https://kubernetes.io/docs/setup/independent/install-kubeadm/ https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/ https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/ https://sealyun.com/post/k8s-ipvs/ https://blog.frognew.com/2017/12/kubeadm-install-kubernetes-1.9.html]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[istio-0.8长期支持版微服务实验]]></title>
    <url>%2Fposts%2F31%2F</url>
    <content type="text"><![CDATA[简介本实验通过在k8s上部署istio，实现微服务的基础功能。其中会涉及到服务的限流，超时，熔断，降级，流量分隔，A/B测试等功能。实验之前需要安装k8s和istio，请参考之前文章。注意开启istio的自动注入功能，并在 default namespace 启用自动注入功能。 本实验的服务间调用关系如下： 本实验采用时下流行的前后端分离模式 前端项目基于vue/react实现 前端调用python实现的API接口 python服务调用后端node实现的服务和lua实现的服务 node服务调用go实现的服务 —-&gt;service-js —-&gt;service-python —-&gt;service-lua —-&gt;service-node —-&gt;service-go 本实验使用的语言技术栈： vue/react python2/3 node8/10 openresty1.11 /1.13 go1.10/1.9 架构图如下： istio-0.8版本配置发生很大的变化，由原来的v1alpha1升级到了v1alpha3，主要变化如下 使用virtualservice和destinationrule 代替原来的routerule 使用gateway代替了原来的ingress 每个virtualservice都要指定要去向哪一个destinationrule ，virtualservice指定访问哪个地址时会使用这个路由，相当于nginx上配置的vhosts 下载实验仓库12git clone https://github.com/mgxian/istio-testcd istio-test &amp;&amp; git checkout v2 部署服务12345678910kubectl apply -f service/go/v1/go-v1.ymlkubectl apply -f service/go/v2/go-v2.ymlkubectl apply -f service/python/v1/python-v1.ymlkubectl apply -f service/python/v2/python-v2.ymlkubectl apply -f service/js/v1/js-v1.ymlkubectl apply -f service/js/v2/js-v2.ymlkubectl apply -f service/node/v1/node-v1.ymlkubectl apply -f service/node/v2/node-v2.ymlkubectl apply -f service/lua/v1/lua-v1.ymlkubectl apply -f service/lua/v2/lua-v2.yml 创建Gateway123456789101112131415161718# 使用istio提供的Gateway功能# 暴露js和python服务让k8s集群外部访问istioctl create -f istio/gateway.ymlistioctl create -f istio/gateway-virtualservice.yml# 查看istioctl get gatewayistioctl get virtualservice# 测试访问INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='&#123;.spec.ports[?(@.name=="http2")].nodePort&#125;')NODE_NAME=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')NODE_IP=$(ping -c 1 $NODE_NAME | grep PING | awk '&#123;print $3&#125;' | tr -d '()')export GATEWAY_URL=$NODE_IP:$INGRESS_PORTecho "curl -I http://$GATEWAY_URL/"echo "curl -I http://$NODE_IP/"# 访问返回404表示正确 配置测试访问环境123456789101112# 配置hosts解析# 11.11.11.112为其中一个node的ip11.11.11.112 istio-test.willcurl -I http://istio-test.will/# 使用curlcurl -I istio-test.willcurl -s istio-test.will | egrep "vue|React"# 此时如果用浏览器，可能会出会页面显示不正常的情况。# 因为此时请求会轮流分发到后端js服务的v1/v2版本，因此css/js并不能正常加载 流量管理根据请求的信息，把流量路由到服务的不同版本。实验过程如果没有达到预期效果，很有可能是因为存在路由规则冲突，而且没有设置优先级，可以先删除之前设置的路由规则或者把优先级设置高一点。 把所有流量导向v1版本1234567891011121314151617181920212223# 清理之前创建的gateway相关的路由规则istioctl delete -f istio/gateway-virtualservice.yml# 创建路由规则istioctl create -f istio/gateway-virtualservice-v1.ymlistioctl create -f istio/route-rule-all-v1.yml# 查看路由规则istioctl get virtualserviceistioctl get destinationrule# 访问浏览器测试http://istio-test.will/# 此时你会看到react app的界面# 点击发射按钮，会发送ajax请求到python服务# 由于把所有流量都导向了v1版本# 多次点击发射按钮会得到一样的内容# react-----&gt;Python2.7.15-----&gt;Gogo1.9.6# 清除路由规则istioctl delete -f istio/route-rule-all-v1.ymlistioctl delete -f istio/gateway-virtualservice-v1.yml 根据请求把流量导向不同版本（A/B测试）123456789101112131415161718192021# 创建路由规则# 根据浏览器的不同返回不同内容istioctl create -f istio/route-rule-js-by-agent.yml# 查看路由规则istioctl get virtualserviceistioctl get destinationrule# 使用访问浏览器# 如果你用chrome浏览器你会看到react app的界面# 如果你用firefox浏览器你会看到vue app的界面# 多次点击发射按钮，会获取到不同的内容# 清除路由规则istioctl delete -f istio/route-rule-js-by-agent.yml# 根据前端app不同使用不同版本的python服务istioctl create -f istio/route-rule-python-by-header.yml# 清除路由规则istioctl delete -f istio/route-rule-python-by-header.yml 根据源服务把流量导向不同版本12345678910# 先创建如下路由方便测试访问# 根据浏览器的不同返回不同内容istioctl create -f istio/route-rule-js-by-agent.yml# 创建路由规则istioctl create -f istio/route-rule-go-by-source.yml# 清除路由规则istioctl delete -f istio/route-rule-js-by-agent.ymlistioctl delete -f istio/route-rule-go-by-source.yml 指定权重进行流量分隔1234567891011121314# 指定权重把流量分隔# 25%流量路由到v1版本# 75%流量路由到v2版本# 先创建如下路由方便测试访问# 根据浏览器的不同返回不同内容istioctl create -f istio/route-rule-js-by-agent.yml# 创建路由规则istioctl create -f istio/route-rule-go-v1-v2.yaml# 清除路由规则istioctl delete -f istio/route-rule-js-by-agent.ymlistioctl delete -f istio/route-rule-go-v1-v2.yaml 集群内访问公开服务1234567891011121314151617181920212223242526272829303132# 默认情况下，启用了istio的服务是无法访问外部url的# 如果需要访问外部url，需要使用egress进行配置# egress同样支持设置路由规则# httpistioctl create -f istio/egress-rule-http-bin.yml# tcpistioctl create -f istio/egress-rule-tcp-wikipedia.yml# 查看istioctl get serviceentry# 测试# 使用exec进入作为测试源使用的podkubectl apply -f istio/sleep.yamlkubectl get podsexport SOURCE_POD=$(kubectl get pod -l app=sleep -o jsonpath=&#123;.items..metadata.name&#125;)kubectl exec -it $SOURCE_POD -c sleep bash# http测试curl http://httpbin.org/headerscurl http://httpbin.org/delay/5# tcp测试curl -o /dev/null -s -w "%&#123;http_code&#125;\n" https://www.wikipedia.orgcurl -s https://en.wikipedia.org/wiki/Main_Page | grep articlecount | grep 'Special:Statistics'# 清理istioctl delete -f istio/egress-rule-http-bin.ymlistioctl delete -f istio/egress-rule-tcp-wikipedia.ymlkubectl delete -f istio/sleep.yaml 故障管理 调用超时设置和重试设置 故障注入，模拟服务故障 设置超时时间与模拟服务超时故障123456789101112131415161718# 先创建如下路由方便测试访问# 根据浏览器的不同返回不同内容istioctl create -f istio/route-rule-js-by-agent.yml# 设置python服务超时时间istioctl create -f istio/route-rule-node-timeout.yml# 模拟go服务超时故障istioctl create -f istio/route-rule-go-delay.yml# 使用浏览器访问并打开调试面板查看网络标签（按F12键）# 多次点击发射按钮观察响应时间# 会看到部分50%的请求会返回500错误# 清除路由规则istioctl delete -f istio/route-rule-js-by-agent.ymlistioctl delete -f istio/route-rule-node-timeout.ymlistioctl delete -f istio/route-rule-go-delay.yml 设置重试与模拟服务500故障123456789101112131415161718# 先创建如下路由方便测试访问# 根据浏览器的不同返回不同内容istioctl create -f istio/route-rule-js-by-agent.yml# 设置python服务超时时间istioctl create -f istio/route-rule-node-retry.yml# 模拟go服务超时故障istioctl create -f istio/route-rule-go-abort.yml# 使用浏览器访问并打开调试面板查看网络标签（按F12键）# 多次点击发射按钮观察响应时间# 会看到部分请求会返回500错误# 清除路由规则istioctl delete -f istio/route-rule-js-by-agent.ymlistioctl delete -f istio/route-rule-node-retry.ymlistioctl delete -f istio/route-rule-go-abort.yml 超时和服务故障模拟配合使用1234567891011# 所有请求延迟5秒钟，然后失败其中的10％... route: - labels: version: v1 httpFault: delay: fixedDelay: 5s abort: percent: 10 httpStatus: 400 熔断器12345678910111213141516171819202122232425262728# 设置熔断规则istioctl create -f istio/route-rule-go-cb.yml# 查看规则istioctl get destinationrule# 创建测试用的fortiokubectl apply -f istio/fortio-deploy.yaml# 正常访问测试FORTIO_POD=$(kubectl get pod | grep fortio | awk '&#123; print $1 &#125;')kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -curl http://service-go/env# 测试熔断 2并发kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -c 2 -qps 0 -n 20 -loglevel Warning http://service-go/env# 测试熔断 3并发kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -c 3 -qps 0 -n 20 -loglevel Warning http://service-go/env# 增加并发会看到失败的请求占比增高# 查看状态# upstream_rq_pending_overflow 表示被熔断的请求数kubectl exec -it $FORTIO_POD -c istio-proxy -- sh -c 'curl localhost:15000/stats' | grep service-go | grep pending# 清理kubectl delete -f istio/fortio-deploy.yamlistioctl delete -f istio/route-rule-go-cb.yml 限流动态设置服务qps https://github.com/istio/istio/blob/master/samples/bookinfo/kube/mixer-rule-ratings-ratelimit.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 配置 memquota, quota, rule, QuotaSpec, QuotaSpecBinding 启用限速# 默认设置500qpsistioctl create -f istio/ratelimit-handler.yaml# 配置速率限制实例和规则istioctl create -f istio/ratelimit-rule-service-go.yaml# 查看kubectl get memquota -n istio-systemkubectl get quota -n istio-systemkubectl get rule -n istio-systemkubectl get quotaspec -n istio-systemkubectl get quotaspecbinding -n istio-system# 创建测试用的fortiokubectl apply -f istio/fortio-deploy.yaml# 正常访问测试FORTIO_POD=$(kubectl get pod | grep fortio | awk '&#123; print $1 &#125;')kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -curl http://service-node/env# 测试# 会出现部分请求不正常# node 返回 code 500# go 返回 code 429kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -qps 20 -n 100 -loglevel Warning http://service-node/envkubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -qps 50 -n 100 -loglevel Warning http://service-go/env# 清理istioctl delete -f istio/ratelimit-handler.yamlistioctl delete -f istio/ratelimit-rule-service-go.yamlkubectl delete -f istio/fortio-deploy.yaml# 带条件的速率限制apiVersion: config.istio.io/v1alpha2kind: rulemetadata: name: quota namespace: istio-systemspec: match: source.namespace != destination.namespace actions: - handler: handler.memquota instances: - requestcount.quota 流量镜像复制服务的流量到别一个镜像服务，一般用于线上新上服务的测试。 1234567891011121314151617181920212223# 创建测试用的fortiokubectl apply -f istio/fortio-deploy.yaml# 正常访问测试FORTIO_POD=$(kubectl get pod | grep fortio | awk '&#123; print $1 &#125;')kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -curl http://service-go/env# 查看v1的日志kubectl logs -f $(kubectl get pods | grep service-go-v1 | awk '&#123;print $1&#125;'| head -n 1) -c service-go# 查看v2的日志# 再开一个终端查看日志kubectl logs -f $(kubectl get pods | grep service-go-v2 | awk '&#123;print $1&#125;'| head -n 1) -c service-go# 创建镜像规则istioctl create -f istio/route-rule-go-mirror.yml# 测试多次访问kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -c 10 -qps 0 -t 10s -loglevel Warning http://service-go/env# 清理kubectl delete -f istio/fortio-deploy.yamlistioctl delete -f istio/route-rule-go-mirror.yml 清理1234567891011121314151617# 删除相关deploy和svckubectl delete -f service/go/v1/go-v1.ymlkubectl delete -f service/go/v2/go-v2.ymlkubectl delete -f service/python/v1/python-v1.ymlkubectl delete -f service/python/v2/python-v2.ymlkubectl delete -f service/js/v1/js-v1.ymlkubectl delete -f service/js/v2/js-v2.ymlkubectl delete -f service/node/v1/node-v1.ymlkubectl delete -f service/node/v2/node-v2.ymlkubectl delete -f service/lua/v1/lua-v1.ymlkubectl delete -f service/lua/v2/lua-v2.yml# 清除路由规则kubectl delete -f istio/gateway.ymlkubectl delete -f istio/gateway-virtualservice.ymlistioctl delete destinationrule $(istioctl get destinationrule | grep 'service-' | awk '&#123;print $1&#125;')istioctl delete virtualservice $(istioctl get virtualservice | grep 'service-' | awk '&#123;print $1&#125;') 参考文档 http://istio.doczh.cn https://istio.io/docs https://istio.io/docs/reference/config/istio.networking.v1alpha3.html https://istio.io/docs/reference/config/istio.routing.v1alpha1.html]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>service mesh</tag>
        <tag>microservice</tag>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[istio-0.8长期支持版安装测试]]></title>
    <url>%2Fposts%2F30%2F</url>
    <content type="text"><![CDATA[简介istio是一个service mesh开源实现，由Google/IBM/Lyft共同开发。与之类似的还有conduit，但是功能不如istio丰富稳定。架构图如下： istio-0.8版本是第一个长期支持版本，相对于之前的版本配置改动较大。 安装1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586# 去下面的地址下载压缩包# https://github.com/istio/istio/releaseswget https://github.com/istio/istio/releases/download/0.8.0/istio-0.8.0-linux.tar.gztar xf istio-0.8.0-linux.tar.gz# 使用官方的安装脚本安装curl -L https://git.io/getLatestIstio | sh -# 安装配置环境变量mv istio-0.8.0 /usr/local/ln -sv /usr/local/istio-0.8.0 /usr/local/istioecho 'export PATH=/usr/local/istio/bin:$PATH' &gt; /etc/profile.d/istio.shsource /etc/profile.d/istio.shistioctl version# 如果环境不是云环境，不支持LoadBalancer# 作如下修改，使得 ingressgateway 监听在80和443端口# 修改使用主机端口映射# 使用此修改版本之后，每台机器只能运行单个实例# 大概在2661行左右cd /usr/local/istiocp install/kubernetes/istio-demo.yaml install/kubernetes/istio-demo.yaml.orivim install/kubernetes/istio-demo.yaml...# Source: istio/charts/ingressgateway/templates/deployment.yamlapiVersion: extensions/v1beta1# kind: Deployment# 使用DaemonSet部署方式kind: DaemonSetmetadata: name: istio-ingressgateway namespace: istio-system labels: app: ingressgateway chart: ingressgateway-0.8.0 release: RELEASE-NAME heritage: Tiller istio: ingressgatewayspec: # DaemonSet不支持replicas # replicas: template: metadata: labels: istio: ingressgateway annotations: sidecar.istio.io/inject: "false" spec: serviceAccountName: istio-ingressgateway-service-account containers: - name: ingressgateway image: "docker.io/istio/proxyv2:0.8.0" imagePullPolicy: IfNotPresent ports: - containerPort: 80 #主机80端口映射 hostPort: 80 - containerPort: 443 #主机443端口映射 hostPort: 443 - containerPort: 31400 #主机443端口映射 hostPort: 31400...# 由于镜像问题，提前拉取镜像# 在所有节点上执行如下命令输出的命令# 可能会失败，需要多次执行image=$(grep 'quay.io/coreos/hyperkube' install/kubernetes/istio-demo.yaml | head -1 | awk '&#123;print $2&#125;' | tr -d '"')echo "docker pull $image"# 以下两种选择一种安装方式# 安装不使用认证（不使用tls）kubectl apply -f install/kubernetes/istio-demo.yaml# 安装使用认证（使用tls）kubectl apply -f install/kubernetes/istio-demo-auth.yaml# 查看状态kubectl get svc -n istio-systemkubectl get pods -n istio-system# 访问测试nodeName=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')nodeIP=$(ping -c 1 $nodeName | grep PING | awk '&#123;print $3&#125;' | tr -d '()')echo "curl -I http://$nodeIP" 注意 istio-0.8.0 默认已经开启了自动注入功能以及其他日志监控和追踪的相关组件如 istio-tracing istio-telemetry grafana prometheus servicegraph 启用自动注入 sidecar 不开启自动注入部署应用需要使用如下方式的命令 kubectl apply -f &lt;(istioctl kube-inject -f samples/bookinfo/kube/bookinfo.yaml) 开启自动注入后，使用正常命令即可部署应用 kubectl apply -f samples/bookinfo/kube/bookinfo.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041# istio-0.8.0默认已经开启了自动注入功能# k8s 1.9 及之后的版本才能使用自动注入功能# 查看是否支持kubectl api-versions | grep admissionregistration# 除了要满足以上条件外还需要检查kube-apiserver启动的参数# k8s 1.9 版本要确保 --admission-control 里有 MutatingAdmissionWebhook,ValidatingAdmissionWebhook# k8s 1.9 之后的版本要确保 --enable-admission-plugins 里有MutatingAdmissionWebhook,ValidatingAdmissionWebhook# 测试自动注入# 创建kubectl apply -f samples/sleep/sleep.yaml kubectl get deployment -o widekubectl get pod# 设置 default namespace 开启自动注入kubectl label namespace default istio-injection=enabledkubectl get namespace -L istio-injection# 删除创建的pod，等待重建kubectl delete pod $(kubectl get pod | grep sleep | cut -d ' ' -f 1)# 查看重建后的pod# 查看是否有istio-proxy容器kubectl get podkubectl describe pod $(kubectl get pod | grep sleep | cut -d ' ' -f 1)# 清理kubectl delete -f samples/sleep/sleep.yaml # 关闭自动注入kubectl label namespace default istio-injection-# 关闭部分pod的自动注入功能... template: metadata: annotations: sidecar.istio.io/inject: "false"... 部署官方测试用例12345678910111213# 启动（未开启自动注入）kubectl apply -f &lt;(istioctl kube-inject -f samples/bookinfo/kube/bookinfo.yaml)# 启动（已开启自动注入）kubectl apply -f samples/bookinfo/kube/bookinfo.yaml# 创建gatewayistioctl create -f samples/bookinfo/routing/bookinfo-gateway.yaml# 查看状态kubectl get serviceskubectl get podsistioctl get gateway 访问测试123456789101112131415# 命令行访问测试export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='&#123;.spec.ports[?(@.name=="http")].nodePort&#125;')NODE_NAME=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')NODE_IP=$(ping -c 1 $NODE_NAME | grep PING | awk '&#123;print $3&#125;' | tr -d '()')export GATEWAY_URL=$NODE_IP:$INGRESS_PORTecho $GATEWAY_URLcurl -o /dev/null -s -w "%&#123;http_code&#125;\n" http://$&#123;GATEWAY_URL&#125;/productpage# 浏览器访问测试echo "http://$&#123;GATEWAY_URL&#125;/productpage"# 使用daemonset方式部署可以使用如下方式访问# 11.11.11.112为其中一个node节点的ipcurl http://11.11.11.112/productpage 清理123456# 清理官方用例samples/bookinfo/kube/cleanup.sh# 清理istiokubectl delete -f install/kubernetes/istio-demo.yaml# kubectl delete -f install/kubernetes/istio-demo-auth.yaml 参考文档 https://istio.io/docs/setup/kubernetes/quick-start.html https://istio.io/docs/guides/bookinfo.html https://istio.io/docs/setup/kubernetes/sidecar-injection.html#automatic-sidecar-injection]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>service mesh</tag>
        <tag>microservice</tag>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kata-containers安装配置]]></title>
    <url>%2Fposts%2F29%2F</url>
    <content type="text"><![CDATA[简介kata-containers是新的虚拟机实现，可以实现和现在容器生态无缝连接，与时下最流行的容器编排工具k8s完美结合，提供容器的快速启动，和虚拟机的安全隔离，与docker技术相比，容器之间不共用内核，使得隔离性更好。 kata与docker对比图 与传统容器对比 架构图 安装本实验在centos7环境下 安装docker123yum -y install yum-utilsyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repoyum -y install docker-ce 安装kata-containers12345source /etc/os-releaseecho $VERSION_IDVERSION_ID=$VERSION_ID yum-config-manager --add-repo \"http://download.opensuse.org/repositories/home:/katacontainers:/release/CentOS_$&#123;VERSION_ID&#125;/home:katacontainers:release.repo"yum -y install kata-runtime kata-proxy kata-shim 配置docker123456mkdir -p /etc/systemd/system/docker.service.d/cat &lt;&lt;EOF | sudo tee /etc/systemd/system/docker.service.d/kata-containers.conf[Service]ExecStart=ExecStart=/usr/bin/dockerd -D --add-runtime kata-runtime=/usr/bin/kata-runtime --default-runtime=kata-runtimeEOF 启动docker12systemctl daemon-reloadsystemctl restart docker 查看机器是否支持kata-containers1234567kata-runtime kata-check# 经过测试使用virtualbox创建的虚拟机来进行实验都不能成功# 因为virtualbox创建的虚拟机，不允许在虚拟机里再使用虚拟化# 而kata-containers需要使用虚拟化# 虽然使用vmware创建的虚拟机使用些步骤检查时，仍然会报部分错误# 但是依然可以成功启动容器 使用测试12345678910# 启动docker run -d busybox sh -c 'sleep 9999999'# 查看docker ps# 查看qemups -ef | grep qemu# 可以看到启动了一个轻量的虚拟机]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7使用kubeadm配置高可用k8s集群的另一种方式]]></title>
    <url>%2Fposts%2F28%2F</url>
    <content type="text"><![CDATA[简介使用kubeadm配置多master节点，实现高可用。 安装实验环境说明实验架构12345678lab1: etcd master keepalived 11.11.11.111lab2: etcd master keepalived 11.11.11.112lab3: etcd master keepalived 11.11.11.113lab4: node 11.11.11.114lab5: node 11.11.11.115lab6: node 11.11.11.116vip: 11.11.11.110 实验使用的Vagrantfile123456789101112131415161718192021# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| (1..6).each do |i| config.vm.define "lab#&#123;i&#125;" do |node| node.vm.box = "centos-7.4-docker-17" node.ssh.insert_key = false node.vm.hostname = "lab#&#123;i&#125;" node.vm.network "private_network", ip: "11.11.11.11#&#123;i&#125;" node.vm.provision "shell", inline: "echo hello from node #&#123;i&#125;" node.vm.provider "virtualbox" do |v| v.cpus = 2 v.customize ["modifyvm", :id, "--name", "lab#&#123;i&#125;", "--memory", "2048"] end end endend 在所有机器上安装kubeadm参考之前的文章《centos7安装kubeadm》 配置所有节点的kubelet12345678910# 配置kubelet使用国内可用镜像# 修改/etc/systemd/system/kubelet.service.d/10-kubeadm.conf# 添加如下配置 Environment="KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.cn-shanghai.aliyuncs.com/gcr-k8s/pause-amd64:3.0"# 使用命令sed -i '/ExecStart=$/i Environment="KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.cn-shanghai.aliyuncs.com/gcr-k8s/pause-amd64:3.0"' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf# 重新载入配置systemctl daemon-reload 配置hosts12345678cat &gt;&gt;/etc/hosts&lt;&lt;EOF11.11.11.111 lab111.11.11.112 lab211.11.11.113 lab311.11.11.114 lab411.11.11.115 lab511.11.11.116 lab6EOF 启动etcd集群在lab1,lab2,lab3节点上启动etcd集群 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# lab1docker stop etcd &amp;&amp; docker rm etcdrm -rf /data/etcdmkdir -p /data/etcddocker run -d \--restart always \-v /etc/etcd/ssl/certs:/etc/ssl/certs \-v /data/etcd:/var/lib/etcd \-p 2380:2380 \-p 2379:2379 \--name etcd \registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:3.1.12 \etcd --name=etcd0 \--advertise-client-urls=http://11.11.11.111:2379 \--listen-client-urls=http://0.0.0.0:2379 \--initial-advertise-peer-urls=http://11.11.11.111:2380 \--listen-peer-urls=http://0.0.0.0:2380 \--initial-cluster-token=9477af68bbee1b9ae037d6fd9e7efefd \--initial-cluster=etcd0=http://11.11.11.111:2380,etcd1=http://11.11.11.112:2380,etcd2=http://11.11.11.113:2380 \--initial-cluster-state=new \--auto-tls \--peer-auto-tls \--data-dir=/var/lib/etcd# lab2docker stop etcd &amp;&amp; docker rm etcdrm -rf /data/etcdmkdir -p /data/etcddocker run -d \--restart always \-v /etc/etcd/ssl/certs:/etc/ssl/certs \-v /data/etcd:/var/lib/etcd \-p 2380:2380 \-p 2379:2379 \--name etcd \registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:3.1.12 \etcd --name=etcd1 \--advertise-client-urls=http://11.11.11.112:2379 \--listen-client-urls=http://0.0.0.0:2379 \--initial-advertise-peer-urls=http://11.11.11.112:2380 \--listen-peer-urls=http://0.0.0.0:2380 \--initial-cluster-token=9477af68bbee1b9ae037d6fd9e7efefd \--initial-cluster=etcd0=http://11.11.11.111:2380,etcd1=http://11.11.11.112:2380,etcd2=http://11.11.11.113:2380 \--initial-cluster-state=new \--auto-tls \--peer-auto-tls \--data-dir=/var/lib/etcd# lab3docker stop etcd &amp;&amp; docker rm etcdrm -rf /data/etcdmkdir -p /data/etcddocker run -d \--restart always \-v /etc/etcd/ssl/certs:/etc/ssl/certs \-v /data/etcd:/var/lib/etcd \-p 2380:2380 \-p 2379:2379 \--name etcd \registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:3.1.12 \etcd --name=etcd2 \--advertise-client-urls=http://11.11.11.113:2379 \--listen-client-urls=http://0.0.0.0:2379 \--initial-advertise-peer-urls=http://11.11.11.113:2380 \--listen-peer-urls=http://0.0.0.0:2380 \--initial-cluster-token=9477af68bbee1b9ae037d6fd9e7efefd \--initial-cluster=etcd0=http://11.11.11.111:2380,etcd1=http://11.11.11.112:2380,etcd2=http://11.11.11.113:2380 \--initial-cluster-state=new \--auto-tls \--peer-auto-tls \--data-dir=/var/lib/etcd# 验证查看集群docker exec -ti etcd ashetcdctl member listetcdctl cluster-healthexit 配置keepalived在3台master节点操作 1234567891011121314151617181920212223242526# 载入内核相关模块lsmod | grep ip_vsmodprobe ip_vs# 启动keepalived# eth1为本次实验11.11.11.0/24网段的所在网卡docker run --net=host --cap-add=NET_ADMIN \-e KEEPALIVED_INTERFACE=eth1 \-e KEEPALIVED_VIRTUAL_IPS="#PYTHON2BASH:['11.11.11.110']" \-e KEEPALIVED_UNICAST_PEERS="#PYTHON2BASH:['11.11.11.111','11.11.11.112','11.11.11.113']" \-e KEEPALIVED_PASSWORD=hello \--name k8s-keepalived \--restart always \-d osixia/keepalived:1.4.4# 查看日志# 会看到两个成为backup 一个成为masterdocker logs k8s-keepalived# 此时会配置 11.11.11.110 到其中一台机器# ping测试ping -c4 11.11.11.110# 如果失败后清理后，重新实验docker rm -f k8s-keepalivedip a del 11.11.11.110/32 dev eth1 在第一台master节点初始化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105# 生成token# 保留token后面还要使用token=$(kubeadm token generate)echo $token# 生成配置文件# advertiseAddress 配置为VIP地址cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha1kind: MasterConfigurationkubernetesVersion: v1.10.3imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containersapi: advertiseAddress: 11.11.11.110apiServerExtraArgs: endpoint-reconciler-type: leasecontrollerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 10.244.0.0/16etcd: endpoints: - "http://11.11.11.111:2379" - "http://11.11.11.112:2379" - "http://11.11.11.113:2379"apiServerCertSANs:- "lab1"- "lab2"- "lab3"- "11.11.11.111"- "11.11.11.112"- "11.11.11.113"- "11.11.11.110"- "127.0.0.1"token: $tokentokenTTL: "0"featureGates: CoreDNS: trueEOF# 初始化kubeadm init --config kubeadm-master.configsystemctl enable kubelet# 保存初始化完成之后的join命令# kubeadm join 11.11.11.110:6443 --token nevmjk.iuh214fc8i0k3iue --discovery-token-ca-cert-hash sha256:0e4f738348be836ff810bce754e059054845f44f01619a37b817eba83282d80f# 配置kubectl使用mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# 安装网络插件# 下载配置mkdir flannel &amp;&amp; cd flannelwget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml# 修改配置# 此处的ip配置要与上面kubeadm的pod-network一致 net-conf.json: | &#123; "Network": "10.244.0.0/16", "Backend": &#123; "Type": "vxlan" &#125; &#125;# 修改镜像image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64# 如果Node有多个网卡的话，参考flannel issues 39701，# https://github.com/kubernetes/kubernetes/issues/39701# 目前需要在kube-flannel.yml中使用--iface参数指定集群主机内网网卡的名称，# 否则可能会出现dns无法解析。容器无法通信的情况，需要将kube-flannel.yml下载到本地，# flanneld启动参数加上--iface=&lt;iface-name&gt; containers: - name: kube-flannel image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=eth1# 启动kubectl apply -f kube-flannel.yml# 查看kubectl get pods -n kube-systemkubectl get svc -n kube-system# 设置master允许部署应用pod，参与工作负载，现在可以部署其他系统组件# 如 dashboard, heapster, efk等kubectl taint nodes --all node-role.kubernetes.io/master- 启动其他master节点12345678910111213141516171819202122# 打包第一台master初始化之后的/etc/kubernetes/pki目录cd /etc/kubernetes &amp;&amp; tar czvf /root/pki.tgz pki/ &amp;&amp; cd ~# 上传到其他master的/etc/kubernetes目录下tar xf pki.tgz -C /etc/kubernetes/# 复制启动第一台master时的配置文件到其他master节点# 初始化kubeadm init --config kubeadm-master.configsystemctl enable kubelet# 配置kubectl使用mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# 在第一台配置master节点查看kubectl get pod --all-namespaces -o wide | grep lab1kubectl get pod --all-namespaces -o wide | grep lab2kubectl get pod --all-namespaces -o wide | grep lab3kubectl get nodes -o wide 启动node节点1234# 加入master节点# 这个命令是之前初始化master完成时，输出的命令kubeadm join 11.11.11.110:6443 --token nevmjk.iuh214fc8i0k3iue --discovery-token-ca-cert-hash sha256:0e4f738348be836ff810bce754e059054845f44f01619a37b817eba83282d80fsystemctl enable kubelet 测试重建多个coredns副本12345678910111213# 删除coredns的podskubectl get pods -n kube-system -o wide | grep corednsall_coredns_pods=$(kubectl get pods -n kube-system -o wide | grep coredns | awk '&#123;print $1&#125;' | xargs)echo $all_coredns_podskubectl delete pods $all_coredns_pods -n kube-system# 修改副本数# replicas: 3# 可以修改为node节点的个数kubectl edit deploy coredns -n kube-system# 查看状态kubectl get pods -n kube-system -o wide | grep coredns 基础测试1. 启动12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# 直接使用命令测试kubectl run nginx --replicas=2 --image=nginx:alpine --port=80kubectl expose deployment nginx --type=NodePort --name=example-service-nodeportkubectl expose deployment nginx --name=example-service# 使用配置文件测试cat &gt;example-nginx.yml&lt;&lt;EOFapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginxspec: replicas: 2 template: metadata: labels: app: nginx spec: restartPolicy: Always containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 livenessProbe: httpGet: path: / port: 80 initialDelaySeconds: 10 periodSeconds: 3 readinessProbe: httpGet: path: / port: 80 initialDelaySeconds: 10 periodSeconds: 3---kind: ServiceapiVersion: v1metadata: name: example-servicespec: selector: app: nginx ports: - name: http port: 80 targetPort: 80---kind: ServiceapiVersion: v1metadata: name: example-service-nodeportspec: selector: app: nginx type: NodePort ports: - name: http-nodeport port: 80 nodePort: 32223EOFkubectl apply -f example-nginx.yml 2. 查看状态1234kubectl get deploykubectl get podskubectl get svckubectl describe svc example-service 3. DNS解析12345678kubectl run curl --image=radial/busyboxplus:curl -i --ttynslookup kubernetesnslookup example-servicecurl example-service# 如果时间过长会返回错误，可以使用如下方式再进入测试curlPod=$(kubectl get pod | grep curl | awk '&#123;print $1&#125;')kubectl exec -ti $curlPod -- sh 4. 访问测试123456# 10.96.59.56 为查看svc时获取到的clusteripcurl "10.96.59.56:80"# 32223 为查看svc时获取到的 nodeporthttp://11.11.11.114:32223/http://11.11.11.115:32223/ 3. 清理删除12kubectl delete svc example-service example-service-nodeportkubectl delete deploy nginx curl 高可用测试任意关闭master节点测试集群是能否正常执行上一步的基础测试，查看相关信息，只关闭到只一台master，因为etcd部署在相应的master节点上，如果关闭了两台，会造成etcd不可用，进而让整个集群不可用。 12345kubectl get pod --all-namespaces -o widekubectl get pod --all-namespaces -o wide | grep lab1kubectl get pod --all-namespaces -o wide | grep lab2kubectl get pod --all-namespaces -o wide | grep lab3kubectl get nodes -o wide 注意事项 当直接把node节点关闭时，只有过了5分钟之后，上面的pod才会被检测到有问题，并迁移到其他节点 如果想快速迁移可以执行 kubectl delete node 也可以修改controller-manager的的pod-eviction-timeout参数，默认5m node-monitor-grace-period参数，默认40s 此方案和之前文章中写的高可用方案相比，缺点就是不能使用 kube-apiserver 多节点负载均衡的功能。所有对kube-apiserver的请求都只会发给一个master节点，只有当这个master节点挂掉之后，才会把所有有请求发给另外的master。 参考文档 https://www.kubernetes.org.cn/3808.html https://kubernetes.io/docs/admin/high-availability/ https://www.kubernetes.org.cn/3536.html https://github.com/indiketa/kubeadm-ha https://zhuanlan.zhihu.com/p/34740013 https://github.com/cookeem/kubeadm-ha/blob/master/README_CN.md https://blog.frognew.com/2017/04/install-etcd-cluster.html https://blog.frognew.com/2017/04/install-ha-kubernetes-1.6-cluster.html https://medium.com/@bambash/ha-kubernetes-cluster-via-kubeadm-b2133360b198 https://github.com/kubernetes/kubeadm/issues/546 https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s使用kube-router网络插件并监控流量状态]]></title>
    <url>%2Fposts%2F27%2F</url>
    <content type="text"><![CDATA[简介kube-router是一个新的k8s的网络插件，使用lvs做服务的代理及负载均衡，使用iptables来做网络的隔离策略。部署简单，只需要在每个节点部署一个daemonset即可，高性能，易维护。支持pod间通信，以及服务的代理。 安装123456789101112131415161718192021222324# 本次实验重新创建了集群，使用之前测试其他网络插件的集群环境没有成功# 可能是由于环境干扰，实验时需要注意# 创建kube-router目录下载相关文件mkdir kube-router &amp;&amp; cd kube-routerwget https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yamlwget https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter-all-features.yaml# 以下两种部署方式任选其一# 1. 只启用 pod网络通信，网络隔离策略 功能kubectl apply -f kubeadm-kuberouter.yaml# 2. 启用 pod网络通信，网络隔离策略，服务代理 所有功能# 删除kube-proxy和其之前配置的服务代理kubectl apply -f kubeadm-kuberouter-all-features.yamlkubectl -n kube-system delete ds kube-proxy# 在每个节点上执行docker run --privileged --net=host registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64:v1.10.2 kube-proxy --cleanup# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system 测试1234567891011121314# 启动用于测试的deploymentkubectl run nginx --replicas=2 --image=nginx:alpine --port=80kubectl expose deployment nginx --type=NodePort --name=example-service-nodeportkubectl expose deployment nginx --name=example-service# dns及访问测试kubectl run curl --image=radial/busyboxplus:curl -i --ttynslookup kubernetesnslookup example-servicecurl example-service# 清理kubectl delete svc example-service example-service-nodeportkubectl delete deploy nginx curl 监控相关数据并可视化重新部署kube-router123456789101112131415161718192021222324252627282930313233343536373839# 修改yml文件cp kubeadm-kuberouter-all-features.yaml kubeadm-kuberouter-all-features.yaml.orivim kubeadm-kuberouter-all-features.yaml...spec: template: metadata: labels: k8s-app: kube-router tier: node annotations: scheduler.alpha.kubernetes.io/critical-pod: '' # 添加如下参数，让prometheus收集数据 prometheus.io/scrape: "true" prometheus.io/path: "/metrics" prometheus.io/port: "8080" spec: serviceAccountName: kube-router serviceAccount: kube-router containers: - name: kube-router image: cloudnativelabs/kube-router imagePullPolicy: Always args: # 添加如下参数开启metrics - --metrics-path=/metrics - --metrics-port=8080 - --run-router=true - --run-firewall=true - --run-service-proxy=true - --kubeconfig=/var/lib/kube-router/kubeconfig...# 重新部署kubectl delete ds kube-router -n kube-systemkubectl apply -f kubeadm-kuberouter-all-features.yaml# 测试获取metricscurl http://127.0.0.1:8080/metrics 部署prometheus复制如下内容到prometheus.yml文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226---apiVersion: v1kind: ConfigMapmetadata: name: prometheus namespace: kube-systemdata: prometheus.yml: |- global: scrape_interval: 15s scrape_configs: # scrape config for API servers - job_name: 'kubernetes-apiservers' kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https # scrape config for nodes (kubelet) - job_name: 'kubernetes-nodes' scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics # Scrape config for Kubelet cAdvisor. # # This is required for Kubernetes 1.7.3 and later, where cAdvisor metrics # (those whose names begin with 'container_') have been removed from the # Kubelet metrics endpoint. This job scrapes the cAdvisor endpoint to # retrieve those metrics. # # In Kubernetes 1.7.0-1.7.2, these metrics are only exposed on the cAdvisor # HTTP endpoint; use "replacement: /api/v1/nodes/$&#123;1&#125;:4194/proxy/metrics" # in that case (and ensure cAdvisor's HTTP server hasn't been disabled with # the --cadvisor-port=0 Kubelet flag). # # This job is not necessary and should be removed in Kubernetes 1.6 and # earlier versions, or it will cause the metrics to be scraped twice. - job_name: 'kubernetes-cadvisor' scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token kubernetes_sd_configs: - role: node relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/$&#123;1&#125;/proxy/metrics/cadvisor # scrape config for service endpoints. - job_name: 'kubernetes-service-endpoints' kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\d+)?;(\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name # Example scrape config for pods - job_name: 'kubernetes-pods' kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\d+)?;(\d+) replacement: $1:$2 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: pod_name---apiVersion: v1kind: Servicemetadata: annotations: prometheus.io/scrape: 'true' labels: name: prometheus name: prometheus namespace: kube-systemspec: selector: app: prometheus type: NodePort ports: - name: prometheus protocol: TCP port: 9090---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: prometheus namespace: kube-systemspec: replicas: 1 selector: matchLabels: app: prometheus template: metadata: name: prometheus labels: app: prometheus annotations: sidecar.istio.io/inject: "false" spec: serviceAccountName: prometheus containers: - name: prometheus image: docker.io/prom/prometheus:v2.2.1 imagePullPolicy: IfNotPresent args: - '--storage.tsdb.retention=6h' - '--config.file=/etc/prometheus/prometheus.yml' ports: - name: web containerPort: 9090 volumeMounts: - name: config-volume mountPath: /etc/prometheus volumes: - name: config-volume configMap: name: prometheus---apiVersion: v1kind: ServiceAccountmetadata: name: prometheus namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRolemetadata: name: prometheusrules:- apiGroups: [""] resources: - nodes - services - endpoints - pods - nodes/proxy verbs: ["get", "list", "watch"]- apiGroups: [""] resources: - configmaps verbs: ["get"]- nonResourceURLs: ["/metrics"] verbs: ["get"]---apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: prometheusroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheussubjects:- kind: ServiceAccount name: prometheus namespace: kube-system--- 部署测试 12345678910111213# 部署kubectl apply -f prometheus.yml# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system# 访问prometheus# 输入 kube_router 关键字查找 看有无提示出现prometheusNodePort=$(kubectl get svc -n kube-system | grep prometheus | awk '&#123;print $5&#125;' | cut -d '/' -f 1 | cut -d ':' -f 2)nodeName=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')nodeIP=$(ping -c 1 $nodeName | grep PING | awk '&#123;print $3&#125;' | tr -d '()')echo "http://$nodeIP:"$prometheusNodePort 部署grafana复制如下内容到grafana.yml文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647---apiVersion: v1kind: Servicemetadata: name: grafana namespace: kube-systemspec: type: NodePort ports: - port: 3000 protocol: TCP name: http selector: app: grafana---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: grafana namespace: kube-systemspec: replicas: 1 template: metadata: labels: app: grafana spec: serviceAccountName: grafana containers: - name: grafana image: grafana/grafana imagePullPolicy: IfNotPresent ports: - containerPort: 3000 volumeMounts: - mountPath: /var/lib/grafana name: grafana-data volumes: - name: grafana-data emptyDir: &#123;&#125;---apiVersion: v1kind: ServiceAccountmetadata: name: grafana namespace: kube-system--- 部署测试 12345678910111213141516# 部署kubectl apply -f grafana.yml# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system# 访问grafanagrafanaNodePort=$(kubectl get svc -n kube-system | grep grafana | awk '&#123;print $5&#125;' | cut -d '/' -f 1 | cut -d ':' -f 2)nodeName=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')nodeIP=$(ping -c 1 $nodeName | grep PING | awk '&#123;print $3&#125;' | tr -d '()')echo "http://$nodeIP:"$grafanaNodePort# 默认用户密码admin/admin 导入并查看dashboard12# 下载官方dashboard的json文件wget https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/dashboard/kube-router.json 创建名为Prometheus类型也为Prometheus的数据源，连接地址为http://prometheus:9090/ 选择刚刚下载的json文件导入dashboard 查看dashboard]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[istio微服务实验之监控日志与可视化]]></title>
    <url>%2Fposts%2F26%2F</url>
    <content type="text"><![CDATA[简介本文是istio微服务实验的后续文章，实验前请先参考之前文章。 分布式调用链追踪安装12345678910111213141516171819202122232425262728# 下载yml文件mkdir jaeger &amp;&amp; cd jaegerwget https://raw.githubusercontent.com/jaegertracing/jaeger-kubernetes/master/all-in-one/jaeger-all-in-one-template.yml# 实验环境不支持 LoadBalancer# 可以修改jaeger-all-in-one-template.yml使用nodeport# 也可以不修改，这样的会使用随机的nodeport# 启动kubectl apply -n istio-system -f jaeger-all-in-one-template.yml# 查看kubectl get pods -n istio-systemkubectl get svc -n istio-system# 多次访问之前的vue react界面并点击发射按钮# 访问jaegerNodePort=$(kubectl get svc -n istio-system | grep jaeger-query | awk '&#123;print $5&#125;' | cut -d '/' -f 1 | cut -d ':' -f 2)nodeName=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')nodeIP=$(ping -c 1 $nodeName | grep PING | awk '&#123;print $3&#125;' | tr -d '()')echo "http://$nodeIP:"$jaegerNodePort# 选择 istio-ingress 可以方便查看整个调用链# 清理cd jaegerkubectl delete -n istio-system -f jaeger-all-in-one-template.yml jaeger的dashboard界面 调用链 服务树展示 日志与指标收集安装12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 安装prometheuscd /usr/local/istio# 修改支持nodeportcp install/kubernetes/addons/prometheus.yaml install/kubernetes/addons/prometheus.yaml.orivim install/kubernetes/addons/prometheus.yaml...apiVersion: v1kind: Servicemetadata: annotations: prometheus.io/scrape: 'true' labels: name: prometheus name: prometheus namespace: istio-systemspec: selector: app: prometheus ports: - name: prometheus protocol: TCP port: 9090 # 设置使用 nodeport type: NodePort...# 部署kubectl apply -f install/kubernetes/addons/prometheus.yaml# 配置收集istioctl create -f istio/new_telemetry.yml# 多次访问之前的vue react界面并点击发射按钮# 访问web测试prometheusNodePort=$(kubectl get svc -n istio-system | grep prometheus | awk '&#123;print $5&#125;' | cut -d '/' -f 1 | cut -d ':' -f 2)nodeName=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')nodeIP=$(ping -c 1 $nodeName | grep PING | awk '&#123;print $3&#125;' | tr -d '()')echo "http://$nodeIP:"$prometheusNodePort# 使用 istio_double_request_count 关键字查询# 查看日志kubectl -n istio-system logs $(kubectl -n istio-system get pods -l istio=mixer -o jsonpath='&#123;.items[0].metadata.name&#125;') mixer | grep \"instance\":\"newlog.logentry.istio-system\"# 清理kubectl delete -f install/kubernetes/addons/prometheus.yamlistioctl delete -f istio/new_telemetry.yml 收集TCP服务的指标安装12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 安装prometheuscd /usr/local/istio# 修改支持nodeportcp install/kubernetes/addons/prometheus.yaml install/kubernetes/addons/prometheus.yaml.orivim install/kubernetes/addons/prometheus.yaml...apiVersion: v1kind: Servicemetadata: annotations: prometheus.io/scrape: 'true' labels: name: prometheus name: prometheus namespace: istio-systemspec: selector: app: prometheus ports: - name: prometheus protocol: TCP port: 9090 # 设置使用 nodeport type: NodePort...# 部署kubectl apply -f install/kubernetes/addons/prometheus.yaml# 配置收集istioctl create -f istio/tcp_telemetry.yml# 部署使用mongodb应用测试# 访问web测试prometheusNodePort=$(kubectl get svc -n istio-system | grep prometheus | awk '&#123;print $5&#125;' | cut -d '/' -f 1 | cut -d ':' -f 2)nodeName=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')nodeIP=$(ping -c 1 $nodeName | grep PING | awk '&#123;print $3&#125;' | tr -d '()')echo "http://$nodeIP:"$prometheusNodePort# 使用 istio_mongo_received_bytes 关键字查询# 清理kubectl delete -f install/kubernetes/addons/prometheus.yamlistioctl delete -f istio/tcp_telemetry.yml TCP数据流图 使用grafana可视化指标安装1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 安装prometheuscd /usr/local/istio# 修改支持nodeportcp install/kubernetes/addons/prometheus.yaml install/kubernetes/addons/prometheus.yaml.orivim install/kubernetes/addons/prometheus.yaml...apiVersion: v1kind: Servicemetadata: annotations: prometheus.io/scrape: 'true' labels: name: prometheus name: prometheus namespace: istio-systemspec: selector: app: prometheus ports: - name: prometheus protocol: TCP port: 9090 # 设置使用 nodeport type: NodePort...cp install/kubernetes/addons/grafana.yaml install/kubernetes/addons/grafana.yaml.orivim install/kubernetes/addons/grafana.yamlkind: Servicemetadata: name: grafana namespace: istio-systemspec: # 设置使用 nodeport type: NodePort ports: - port: 3000 protocol: TCP name: http selector: app: grafana# 部署kubectl apply -f install/kubernetes/addons/prometheus.yamlkubectl apply -f install/kubernetes/addons/grafana.yaml# 访问web测试grafanaNodePort=$(kubectl get svc -n istio-system | grep grafana | awk '&#123;print $5&#125;' | cut -d '/' -f 1 | cut -d ':' -f 2)nodeName=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')nodeIP=$(ping -c 1 $nodeName | grep PING | awk '&#123;print $3&#125;' | tr -d '()')echo "http://$nodeIP:"$grafanaNodePort# 压力测试查看图表# 创建测试用的fortiokubectl apply -f &lt;(istioctl kube-inject -f istio/fortio-deploy.yaml)# 正常访问测试FORTIO_POD=$(kubectl get pod | grep fortio | awk '&#123; print $1 &#125;')kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -curl http://service-python/env# 加大压力测试kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -qps 20 -t 100s -loglevel Warning http://service-python/envkubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -qps 50 -t 100s -loglevel Warning http://service-go/env# 清理kubectl delete -f install/kubernetes/addons/prometheus.yamlkubectl delete -f install/kubernetes/addons/grafana.yamlkubectl delete -f istio/fortio-deploy.yaml service mesh 数据监控展示 pilot数据监控展示 生成服务树安装123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# 修改使用nodeportcd /usr/local/istiocp install/kubernetes/addons/servicegraph.yaml install/kubernetes/addons/servicegraph.yaml.orivim install/kubernetes/addons/servicegraph.yaml...apiVersion: v1kind: Servicemetadata: name: servicegraph namespace: istio-systemspec: # 设置使用 nodeport type: NodePort ports: - name: http port: 8088 selector: app: servicegraph...# 安装prometheuscd /usr/local/istio# 修改支持nodeportcp install/kubernetes/addons/prometheus.yaml install/kubernetes/addons/prometheus.yaml.orivim install/kubernetes/addons/prometheus.yaml...apiVersion: v1kind: Servicemetadata: annotations: prometheus.io/scrape: 'true' labels: name: prometheus name: prometheus namespace: istio-systemspec: selector: app: prometheus ports: - name: prometheus protocol: TCP port: 9090 # 设置使用 nodeport type: NodePort...# 部署kubectl apply -f install/kubernetes/addons/prometheus.yamlkubectl apply -f install/kubernetes/addons/servicegraph.yaml# 多次访问之前的vue react界面并点击发射按钮# 访问web测试servicegraphNodePort=$(kubectl get svc -n istio-system | grep servicegraph | awk '&#123;print $5&#125;' | cut -d '/' -f 1 | cut -d ':' -f 2)nodeName=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')nodeIP=$(ping -c 1 $nodeName | grep PING | awk '&#123;print $3&#125;' | tr -d '()')echo "http://$nodeIP:"$servicegraphNodePort/force/forcegraph.html# 可使用url# /force/forcegraph.html# /dotviz# /dotgraph# /d3graph# /graph# 清理kubectl delete -f install/kubernetes/addons/prometheus.yamlkubectl delete -f install/kubernetes/addons/servicegraph.yaml 服务树 使用Fluentd收集日志安装1234567891011121314151617# 安装efkkubectl apply -f istio/logging-stack.yml# 配置istio使用efkistioctl create -f istio/fluentd-istio.yml# 多次访问之前的vue react界面并点击发射按钮# 访问web测试kibanaNodePort=$(kubectl get svc -n istio-system | grep kibana | awk '&#123;print $5&#125;' | cut -d '/' -f 1 | cut -d ':' -f 2)nodeName=$(kubectl get no | grep '&lt;none&gt;' | head -1 | awk '&#123;print $1&#125;')nodeIP=$(ping -c 1 $nodeName | grep PING | awk '&#123;print $3&#125;' | tr -d '()')echo "http://$nodeIP:"$kibanaNodePort# 清理kubectl delete -f istio/logging-stack.ymlistio delete -f istio/fluentd-istio.yml]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>service mesh</tag>
        <tag>microservice</tag>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[istio微服务实验]]></title>
    <url>%2Fposts%2F25%2F</url>
    <content type="text"><![CDATA[简介本实验通过在k8s上部署istio，实现微服务的基础功能。其中会涉及到服务的限流，超时，熔断，降级，流量分隔，A/B测试等功能。实验之前需要安装k8s和istio，请参考之前文章。注意开启istio的自动注入功能。本实验的服务间调用关系如下： 本实验采用时下流行的前后端分离模式 前端项目基于vue/react实现 前端调用python实现的API接口 python服务调用后端node实现的服务和lua实现的服务 node服务调用go实现的服务 —-&gt;service-js —-&gt;service-python —-&gt;service-lua —-&gt;service-node —-&gt;service-go 本实验使用的语言技术栈： vue/react python2/3 node8/10 openresty1.11 /1.13 go1.10/1.9 架构图如下： 下载实验仓库1git clone https://github.com/mgxian/istio-test 部署服务1234567891011cd istio-testkubectl apply -f service/go/v1/go-v1.ymlkubectl apply -f service/go/v2/go-v2.ymlkubectl apply -f service/python/v1/python-v1.ymlkubectl apply -f service/python/v2/python-v2.ymlkubectl apply -f service/js/v1/js-v1.ymlkubectl apply -f service/js/v2/js-v2.ymlkubectl apply -f service/node/v1/node-v1.ymlkubectl apply -f service/node/v2/node-v2.ymlkubectl apply -f service/lua/v1/lua-v1.ymlkubectl apply -f service/lua/v2/lua-v2.yml 暴露服务1234567# 使用istio提供的ingress功能# 暴露js和python服务让k8s集群外部访问kubectl apply -f istio/ingress-python.ymlkubectl apply -f istio/ingress-js.yml# 查看kubectl get ingress 测试访问12345678910# 配置hosts解析# 11.11.11.112为其中一个node的ip11.11.11.112 istio-test.will# 使用curlcurl -I istio-test.willcurl -s istio-test.will | egrep "vue|React"# 此时如果作用浏览器，可能会出会页面显示不正常的情况。# 因为此时请求会轮流分发到后端js服务的v1/v2版本，因此css/js并不能正常加载 流量管理根据请求的信息，把流量路由到服务的不同版本。实验过程如果没有达到预期效果，很有可能是因为存在路由规则冲突，而且没有设置优先级，可以先删除之前设置的路由规则或者把优先级设置高一点。 把所有流量导向v1版本1234567891011121314151617# 创建路由规则istioctl create -f istio/route-rule-all-v1.yml# 查看路由规则istioctl get routerule# 访问浏览器测试http://istio-test.will/# 此时你会看到react app的界面# 点击发射按钮，会发送ajax请求到python服务# 由于把所有流量都导向了v1版本# 多次点击发射按钮会得到一样的内容# react-----&gt;Python2.7.15-----&gt;Gogo1.9.6# 清除路由规则istioctl delete -f istio/route-rule-all-v1.yml 根据请求把流量导向不同版本（A/B测试）123456789101112131415# 创建路由规则# 根据浏览器的不同返回不同内容istioctl create -f istio/route-rule-js-by-agent.yml# 使用访问浏览器# 如果你用chrome浏览器你会看到react app的界面# 如果你用firefox浏览器你会看到vue app的界面# 多次点击发射按钮，会获取到不同的内容# 根据前端app不同使用不同版本的python服务istioctl create -f istio/route-rule-python-by-header.yml# 此步骤创建的第一个路由规则保留不删除，为下面做实验提供方便istioctl delete -f istio/route-rule-python-by-header.yml 根据源服务把流量导向不同版本1234567891011# 创建路由规则istioctl create -f istio/route-rule-go-by-source.yml# 此时规则如下# 所有chrome浏览器都走v1版本服务# 所有firefox浏览器都走v2版本服务# react-----&gt;Python2.7.15-----&gt;Gogo1.9.6# vue-----&gt;Python3.6.5-----&gt;Gogo1.10.2# 清除路由规则istioctl delete -f istio/route-rule-go-by-source.yml 指定权重进行流量分隔123456789# 指定权重把流量分隔# 25%流量路由到v1版本# 75%流量路由到v2版本# 创建路由规则istioctl create -f istio/route-rule-go-v1-v2.yaml# 清除路由规则istioctl delete -f istio/route-rule-go-v1-v2.yaml 集群内访问公开服务1234567891011121314151617181920212223242526272829# 默认情况下，启用了istio的服务是无法访问外部url的# 如果需要访问外部url，需要使用egress进行配置# egress同样支持设置路由规则# httpistioctl create -f istio/egress-rule-http-bin.yml# tcpistioctl create -f istio/egress-rule-tcp-wikipedia.yml# 查看istioctl get egressrule# 测试# 使用exec进入作为测试源使用的podkubectl apply -f istio/sleep.yamlkubectl get podsexport SOURCE_POD=$(kubectl get pod -l app=sleep -o jsonpath=&#123;.items..metadata.name&#125;)kubectl exec -it $SOURCE_POD -c sleep bash# http测试curl http://httpbin.org/headers# tcp测试curl -o /dev/null -s -w "%&#123;http_code&#125;\n" https://www.wikipedia.orgcurl -s https://en.wikipedia.org/wiki/Main_Page | grep articlecount | grep 'Special:Statistics'# 清理istioctl delete -f istio/egress-rule-http-bin.ymlistioctl delete -f istio/egress-rule-tcp-wikipedia.ymlkubectl delete -f istio/sleep.yaml 故障管理 调用超时设置和重试设置 故障注入，模拟服务故障 设置超时时间与模拟服务超时故障12345678910111213# 设置python服务超时时间istioctl create -f istio/route-rule-python-timeout.yml# 模拟go服务超时故障istioctl create -f istio/route-rule-go-delay.yml# 使用浏览器访问并打开调试面板查看网络标签（按F12键）# 多次点击发射按钮观察响应时间# 会看到平均50%的请求会返回504超时# 清除路由规则istioctl delete -f istio/route-rule-python-timeout.ymlistioctl delete -f istio/route-rule-go-delay.yml 设置重试与模拟服务500故障12345678910111213# 设置python服务超时时间istioctl create -f istio/route-rule-python-retry.yml# 模拟go服务超时故障istioctl create -f istio/route-rule-go-abort.yml# 使用浏览器访问并打开调试面板查看网络标签（按F12键）# 多次点击发射按钮观察响应时间# 会看到部分请求会返回500错误# 清除路由规则istioctl delete -f istio/route-rule-python-retry.ymlistioctl delete -f istio/route-rule-go-abort.yml 超时和服务故障模拟配合使用1234567891011# 所有请求延迟5秒钟，然后失败其中的10％... route: - labels: version: v1 httpFault: delay: fixedDelay: 5s abort: percent: 10 httpStatus: 400 熔断器1234567891011121314151617181920212223242526272829303132333435# 熔断器规则需要应用到路由规则上# 需要先配置至少一个路由规则# 设置路由规则istioctl create -f istio/route-rule-go-default.yml# 设置熔断规则istioctl create -f istio/route-rule-go-cb.yml# 查看规则istioctl get destinationpolicy# 创建测试用的fortiokubectl apply -f &lt;(istioctl kube-inject --debug -f istio/fortio-deploy.yaml)# 正常访问测试FORTIO_POD=$(kubectl get pod | grep fortio | awk '&#123; print $1 &#125;')kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -curl http://service-go/env# 测试熔断 2并发kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -c 2 -qps 0 -n 20 -loglevel Warning http://service-go/env# 测试熔断 3并发kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -c 3 -qps 0 -n 20 -loglevel Warning http://service-go/env# 增加并发会看到失败的请求占比增高# 查看状态# upstream_rq_pending_overflow 表示被熔断的请求数kubectl exec -it $FORTIO_POD -c istio-proxy -- sh -c 'curl localhost:15000/stats' | grep service-go | grep pending# 清理kubectl delete -f istio/fortio-deploy.yamlistioctl delete -f istio/route-rule-go-default.ymlistioctl delete -f istio/route-rule-go-cb.yml 限流动态设置服务qps https://github.com/istio/istio/blob/master/samples/bookinfo/kube/mixer-rule-ratings-ratelimit.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# 创建service-python默认路由# 经测试，一定要配置路由规则，否则无法完成限流# 所以极有可能限流是配置在路由规则上的# 在路由时进行限流统计istioctl create -f istio/route-rule-python-default.yml# 配置一个速率限制的memquota适配器# 默认设置500qpsistioctl create -f istio/ratelimit-handler.yaml# 配置速率限制实例和规则istioctl create -f istio/ratelimit-rule-service-go.yaml# 查看kubectl get memquota -n istio-systemkubectl get quota -n istio-systemkubectl get rule -n istio-systemkubectl get quotaspec -n istio-systemkubectl get quotaspecbinding -n istio-system# 创建测试用的fortiokubectl apply -f &lt;(istioctl kube-inject -f istio/fortio-deploy.yaml)# 正常访问测试FORTIO_POD=$(kubectl get pod | grep fortio | awk '&#123; print $1 &#125;')kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -curl http://service-python/env# 测试# 会出现部分请求不正常# python 返回 code 500# go 返回 code 429kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -qps 20 -n 100 -loglevel Warning http://service-python/envkubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -qps 50 -n 100 -loglevel Warning http://service-go/env# 清理istioctl delete -f istio/route-rule-python-default.ymlistioctl delete -f istio/ratelimit-handler.yamlistioctl delete -f istio/ratelimit-rule-service-go.yamlkubectl delete -f istio/fortio-deploy.yaml# 带条件的速率限制apiVersion: config.istio.io/v1alpha2kind: rulemetadata: name: quota namespace: istio-systemspec: match: source.namespace != destination.namespace actions: - handler: handler.memquota instances: - requestcount.quota 流量镜像复制服务的流量到别一个镜像服务，一般用于线上新上服务的测试。 12345678910111213141516171819202122232425262728# 创建默认策略# 默认所有流量路由到v1istioctl create -f istio/route-rule-go-default.yml# 创建测试用的fortiokubectl apply -f &lt;(istioctl kube-inject -f istio/fortio-deploy.yaml)# 正常访问测试FORTIO_POD=$(kubectl get pod | grep fortio | awk '&#123; print $1 &#125;')kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -curl http://service-go/env# 查看v1的日志kubectl logs -f $(kubectl get pods | grep service-go-v1 | awk '&#123;print $1&#125;'| head -n 1) -c service-go# 查看v2的日志# 再开一个终端查看日志kubectl logs -f $(kubectl get pods | grep service-go-v2 | awk '&#123;print $1&#125;'| head -n 1) -c service-go# 创建镜像规则istioctl create -f istio/route-rule-go-mirror.yml# 测试多次访问kubectl exec -it $FORTIO_POD -c fortio /usr/local/bin/fortio -- load -c 10 -qps 0 -t 10s -loglevel Warning http://service-go/env# 清理kubectl delete -f istio/fortio-deploy.yamlistioctl delete -f istio/route-rule-go-default.ymlistioctl delete -f istio/route-rule-go-mirror.yml 清理12345678910111213141516# 删除相关deploy和svckubectl delete -f service/go/v1/go-v1.ymlkubectl delete -f service/go/v2/go-v2.ymlkubectl delete -f service/python/v1/python-v1.ymlkubectl delete -f service/python/v2/python-v2.ymlkubectl delete -f service/js/v1/js-v1.ymlkubectl delete -f service/js/v2/js-v2.ymlkubectl delete -f service/node/v1/node-v1.ymlkubectl delete -f service/node/v2/node-v2.ymlkubectl delete -f service/lua/v1/lua-v1.ymlkubectl delete -f service/lua/v2/lua-v2.yml# 清除路由规则kubectl delete -f istio/ingress-python.ymlkubectl delete -f istio/ingress-js.ymlistioctl delete routerule $(istioctl get routerule | grep RouteRule | awk '&#123;print $1&#125;') 参考文档 http://istio.doczh.cn https://istio.io/docs https://istio.io/docs/reference/config/istio.networking.v1alpha3.html https://istio.io/docs/reference/config/istio.routing.v1alpha1.html]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>service mesh</tag>
        <tag>microservice</tag>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[istio安装测试]]></title>
    <url>%2Fposts%2F24%2F</url>
    <content type="text"><![CDATA[简介istio是一个service mesh开源实现，由Google/IBM/Lyft共同开发。与之类似的还有conduit，但是功能不如istio丰富稳定。架构图如下： 安装1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980# 去下面的地址下载压缩包# https://github.com/istio/istio/releaseswget https://github.com/istio/istio/releases/download/0.7.1/istio-0.7.1-linux.tar.gztar xf istio-0.7.1-linux.tar.gz# 使用官方的安装脚本安装curl -L https://git.io/getLatestIstio | sh -# 安装配置环境变量mv istio-0.7.1 /usr/local/ln -sv /usr/local/istio-0.7.1 /usr/local/istioecho 'export PATH=/usr/local/istio/bin:$PATH' &gt; /etc/profile.d/istio.shsource /etc/profile.d/istio.shistioctl version# 如果环境不是云环境，不支持LoadBalancer# 作如下修改，使得 ingress 监听在80和443端口# 修改 Istio ingress 使用 NodePort# 修改使用主机端口映射# 使用此修改版本之后，每台机器只能运行单个实例# 大概在1548-1590行左右cd /usr/local/istiocp install/kubernetes/istio.yaml install/kubernetes/istio.yaml.orivim install/kubernetes/istio.yaml...################################# Istio ingress################################apiVersion: v1kind: Servicemetadata: name: istio-ingress namespace: istio-system labels: istio: ingressspec: #type: LoadBalancer # 使用NodePort方式 type: NodePort ports: - port: 80# nodePort: 32000 name: http - port: 443 name: https selector: istio: ingress---apiVersion: extensions/v1beta1#kind: Deployment# 使用DaemonSet部署方式kind: DaemonSetmetadata: name: istio-ingress namespace: istio-systemspec: #DaemonSet不支持replicas #replicas: 1 template:... imagePullPolicy: IfNotPresent ports: - containerPort: 80 #主机80端口映射 hostPort: 80 - containerPort: 443 #主机443端口映射 hostPort: 443...# 以下两种选择一种安装方式# 安装不使用认证（不使用tls）kubectl apply -f install/kubernetes/istio.yaml# 安装使用认证（使用tls）kubectl apply -f install/kubernetes/istio-auth.yaml# 查看状态kubectl get svc -n istio-systemkubectl get pods -n istio-system 启用自动注入 sidecar 不开启自动注入部署应用需要使用如下方式的命令 kubectl apply -f &lt;(istioctl kube-inject -f samples/bookinfo/kube/bookinfo.yaml) 开启自动注入后，使用正常命令即可部署应用 kubectl apply -f samples/bookinfo/kube/bookinfo.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960# k8s 1.9 版本以后才能使用# 查看是否支持kubectl api-versions | grep admissionregistration# 除了要满足以上条件外还需要检查kube-apiserver启动的参数# k8s 1.9 版本要确保 --admission-control 里有 MutatingAdmissionWebhook,ValidatingAdmissionWebhook# k8s 1.9 之后的版本要确保 --enable-admission-plugins 里有MutatingAdmissionWebhook,ValidatingAdmissionWebhook# 生成所需要的证书./install/kubernetes/webhook-create-signed-cert.sh \ --service istio-sidecar-injector \ --namespace istio-system \ --secret sidecar-injector-certs # 创建配置configmapkubectl apply -f install/kubernetes/istio-sidecar-injector-configmap-release.yaml# 生成相关yamlcat install/kubernetes/istio-sidecar-injector.yaml | \ ./install/kubernetes/webhook-patch-ca-bundle.sh &gt; \ install/kubernetes/istio-sidecar-injector-with-ca-bundle.yaml # 安装webhookkubectl apply -f install/kubernetes/istio-sidecar-injector-with-ca-bundle.yaml# 查看kubectl -n istio-system get deployment -listio=sidecar-injectorkubectl get namespace -L istio-injection# 测试自动注入# 创建kubectl apply -f samples/sleep/sleep.yaml kubectl get deployment -o widekubectl get pod# 设置 default namespace 开启自动注入kubectl label namespace default istio-injection=enabledkubectl get namespace -L istio-injection# 删除创建的pod，等待重建kubectl delete pod $(kubectl get pod | grep sleep | cut -d ' ' -f 1)# 查看重建后的pod# 查看是否有istio-proxy容器kubectl get podkubectl describe pod $(kubectl get pod | grep sleep | cut -d ' ' -f 1)# 清理kubectl delete -f samples/sleep/sleep.yaml # 关闭自动注入kubectl label namespace default istio-injection-# 关闭部分pod的自动注入功能... template: metadata: annotations: sidecar.istio.io/inject: "false"... 部署官方测试用例12345678910# 启动（未开启自动注入）kubectl apply -f &lt;(istioctl kube-inject -f samples/bookinfo/kube/bookinfo.yaml)# 启动（已开启自动注入）kubectl apply -f samples/bookinfo/kube/bookinfo.yaml# 查看状态kubectl get serviceskubectl get podskubectl get ingress -o wide 访问测试12345678910111213# 命令行访问测试GATEWAY_URL=$(kubectl get po -l istio=ingress -n istio-system -o 'jsonpath=&#123;.items[0].status.hostIP&#125;'):$(kubectl get svc istio-ingress -n istio-system -o 'jsonpath=&#123;.spec.ports[0].nodePort&#125;')curl -o /dev/null -s -w "%&#123;http_code&#125;\n" http://$&#123;GATEWAY_URL&#125;/productpage# 浏览器访问测试NODE_PORT=$(kubectl get svc istio-ingress -n istio-system -o jsonpath='&#123;.spec.ports[0].nodePort&#125;')NODE_IP='11.11.11.112'echo http://$&#123;NODE_IP&#125;:$&#123;NODE_PORT&#125;/productpage# 使用daemonset方式部署可以使用如下方式访问# 11.11.11.112为其中一个node节点的ipcurl http://11.11.11.112/productpage 清理123456# 清理官方用例samples/bookinfo/kube/cleanup.sh# 清理istiokubectl delete -f install/kubernetes/istio.yaml# kubectl delete -f install/kubernetes/istio-auth.yaml 参考文档 https://istio.io/docs/setup/kubernetes/quick-start.html https://istio.io/docs/guides/bookinfo.html https://istio.io/docs/setup/kubernetes/sidecar-injection.html#automatic-sidecar-injection]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>service mesh</tag>
        <tag>microservice</tag>
        <tag>istio</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s安装traefik作为ingress]]></title>
    <url>%2Fposts%2F23%2F</url>
    <content type="text"><![CDATA[简介traefik 是一个前端负载均衡器，对于微服务架构尤其是 kubernetes 等编排工具具有良好的支持；同 nginx 等相比，traefik 能够自动感知后端容器变化，从而实现自动服务发现。 traefik部署在k8s上分为daemonset和deployment两种方式各有优缺点： daemonset 能确定有哪些node在运行traefik，所以可以确定的知道后端ip，但是不能方便的伸缩 deployment 可以更方便的伸缩，但是不能确定有哪些node在运行traefik所以不能确定的知道后端ip 一般部署两种不同类型的traefik: 面向内部(internal)服务的traefik，建议可以使用deployment的方式 面向外部(external)服务的traefik，建议可以使用daemonset的方式 建议使用traffic-type标签 traffic-type: external traffic-type: internal traefik相应地使用labelSelector traffic-type=internal traffic-type=external 安装12345678910111213141516171819202122232425262728293031mkdir traefik &amp;&amp; cd traefikwget https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/traefik-rbac.yaml# 配置rbackubectl apply -f traefik-rbac.yaml# 以下两种方式选择一个# 80 提供正常服务，8080 是其自带的 UI 界面# 以daemonset方式启动traefik# 会在所有node节点启动一个traefik并监听在80端口# master节点不会启动traefikwget https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/traefik-ds.yamlkubectl apply -f traefik-ds.yaml# 以deployment方式启动traefikwget https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/traefik-deployment.yamlkubectl apply -f traefik-deployment.yaml# 查看状态kubectl get pods -n kube-system# 访问测试，如果有响应说明安装正确# 应该返回404# 如果以daemonset方式启动traefik使用如下方式验证# 11.11.11.112为任何一个node节点的ipcurl 11.11.11.112# 如果以deployment方式启动traefik# 访问node:nodeport或者集群ip验证 部署Træfik Web UI12345678910wget https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/ui.yamlkubectl apply -f ui.yaml# 访问webui# 需要先配置host# 11.11.11.112为任何一个node节点的ip11.11.11.112 traefik-ui.minikube# 浏览器访问如下地址http://traefik-ui.minikube/ 使用basic验证1234567891011121314151617181920212223242526272829303132# 生成加密密码，如果没有安装htpasswd可以在线生成# https://tool.lu/htpasswd/htpasswd -c ./auth myusernamecat authmyusername:$apr1$78Jyn/1K$ERHKVRPPlzAX8eBtLuvRZ0# 从密码文件创建secret# monitoring必须和ingress rule处于同一个namespace kubectl create secret generic mysecret --from-file auth --namespace=monitoring# 创建ingresscat &gt;prometheus-ingress.yaml&lt;&lt;EOFapiVersion: extensions/v1beta1kind: Ingressmetadata: name: prometheus-dashboard namespace: monitoring annotations: kubernetes.io/ingress.class: traefik ingress.kubernetes.io/auth-type: "basic" ingress.kubernetes.io/auth-secret: "mysecret"spec: rules: - host: dashboard.prometheus.example.com http: paths: - backend: serviceName: prometheus servicePort: 9090EOFkubectl create -f prometheus-ingress.yaml -n monitoring 官方实例1. 根据域名(host)路由12345678910111213141516171819202122232425# deploymentwget https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/cheese-deployments.yamlkubectl apply -f cheese-deployments.yaml# servicewget https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/cheese-services.yamlkubectl apply -f cheese-services.yaml# ingresswget https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/cheese-ingress.yamlkubectl apply -f cheese-ingress.yaml# 查看状态kubectl get podskubectl get svckubectl get ingress# 测试# 配置hosts11.11.11.112 stilton.minikube cheddar.minikube wensleydale.minikube# 浏览器访问测试http://stilton.minikube/http://cheddar.minikube/http://wensleydale.minikube/ 2. 根据路径(path)路由123456789101112# 使用新的ingresswget https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/cheeses-ingress.yamlkubectl apply -f cheeses-ingress.yaml# 测试# 配置hosts11.11.11.112 cheeses.minikube# 浏览器访问测试http://cheeses.minikube/stilton/http://cheeses.minikube/cheddar/http://cheeses.minikube/wensleydale/ 3. 指定路由优先级123456789101112131415161718192021222324252627282930apiVersion: extensions/v1beta1kind: Ingressmetadata: name: wildcard-cheeses annotations: traefik.frontend.priority: "1"spec: rules: - host: *.minikube http: paths: - path: / backend: serviceName: stilton servicePort: httpkind: Ingressmetadata: name: specific-cheeses annotations: traefik.frontend.priority: "2"spec: rules: - host: specific.minikube http: paths: - path: / backend: serviceName: stilton servicePort: http 参考文档 https://docs.traefik.io/user-guide/kubernetes/ https://mritd.me/2016/12/06/try-traefik-on-kubernetes/]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>ingress</tag>
        <tag>traefik</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10安装配置minikube]]></title>
    <url>%2Fposts%2F22%2F</url>
    <content type="text"><![CDATA[简介minukube是一个可以让开发人员在本地环境运行k8s的软件，便于开发人员在本地测试运行k8s 相关下载链接 链接：https://pan.baidu.com/s/10dJLJiUnXsZcA5c6HwWVqQ 密码：qh6k 安装1. 安装minikube 12345678# 到 minikube release 页面下载 minikube 安装文件https://github.com/kubernetes/minikube/releases/download/v0.26.1/minikube-installer.exe# 直接双击安装# 设置环境变量# 如果不设置，默认会在C盘中安装相关文件MINIKUBE_HOME=D:\minikube 2. 安装kubectl 12345678# 下载 如果不能正常下载 可能需要翻墙curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/windows/amd64/kubectl.exe# 把kubectl所在目录放在系统path中# 也可以放在minikube的安装目录中# 查看版本kubectl version 3.安装virtualbox 官方下载virtualbox对应平台的软件包安装 启动12345678910111213141516171819# 获取k8s可用版本minikube get-k8s-versions# 启动之前因为需要下载minikube-iso和localkube，所以需要使用代理# 有时不需要使用代理也能下载，但是启动之后 由于需要拉取gcr.io上面的镜像# 所以仍然需要配置docker的代理# 使用minikube ssh连接到minikube主机里# 下载好相关镜像，再重新打tag为gcr.io也是一种方法# 下载存储在 MINIKUBE_HOME/.minikube/cache 目录下# MINIKUBE_HOME 如果没有设置 默认为用户的家目录 https_proxy=http://127.0.0.1:1080 minikube start \--vm-driver virtualbox \--memory 2048 --disable-driver-mounts \--registry-mirror https://tfhzn46h.mirror.aliyuncs.com \--docker-env http_proxy=http://172.16.0.10:1080 \--docker-env https_proxy=http://172.16.0.10:1080 \--docker-env no_proxy='192.168.99.0/24,.docker.io,.aliyuncs.com'# 然后根据提示操作 基本测试使用12345678910111213141516171819202122232425262728293031323334353637# 获取minikube的ipminikube ip# ssh连接到minikube主机里# 使用git-bash可能会无法正常连接minikube ssh# 官方示例kubectl run hello-minikube --image=k8s.gcr.io/echoserver:1.8 --port=8080kubectl expose deployment hello-minikube --type=NodePort# 启动nginx并创建服务kubectl run nginx --image=nginx:alpine --port=80kubectl expose deployment nginx --type=NodePort# 查看访问kubectl get podskubectl get svccurl $(minikube service hello-minikube --url)curl $(minikube service nginx --url)# 清理kubectl delete deployment nginx hello-minikubekubectl delete svc nginx hello-minikube# 停止删除minikube集群minikube stopminikube delete# 查看其他组件minikube addons list# 启用组件minikube addons enable heapster# 在浏览器中打开组件minikube addons open heapster 参考文档 https://kubernetes.io/docs/getting-started-guides/minikube/ https://github.com/kubernetes/minikube/blob/v0.24.1/README.md]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>minikube</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用vagrant基于官方的box制作自己的基础box]]></title>
    <url>%2Fposts%2F21%2F</url>
    <content type="text"><![CDATA[使用vagrant启动虚拟机 123456789101112131415161718mkdir base &amp;&amp; cd basecat &gt;Vagrantfile&lt;&lt;EOF# -*- mode: ruby -*-# vi: set ft=ruby :Vagrant.configure("2") do |config| # centos-7.4-docker-17为你想使用的基础box config.vm.box = "centos-7.4-docker-17" config.ssh.insert_key = false config.vm.provider "virtualbox" do |v| v.customize ["modifyvm", :id, "--name", "will"] end config.vm.synced_folder ".", "/vagrant", disabled: trueendEOF# 启动vagrant up 登录配置安装相关软件123456789101112131415161718192021222324# 使用xshell等ssh工具登录# 配置源# 安装配置所需要的软件# 安装完成关机前做如下清理操作# 删除网卡mac信息rm -f /etc/udev/rules.d/70-persistent-net.rules# 删除临时文件yum clean allapt-get cleanrm -rf /tmp/*rm -f /var/log/wtmp /var/log/btmp# 清除命令历史记录history -c&gt; .bash_historysudo su - vagranthistory -c&gt; .bash_history# 关机vagrant halt 制作基础box123456789101112cat &gt;Vagrantfile.base&lt;&lt;EOF# -*- mode: ruby -*-# vi: set ft=ruby :Vagrant.configure(2) do |config| # Disable synced folders config.vm.synced_folder ".", "/vagrant", disabled: trueendEOF# will为之前启动时设置的vm名vagrant package --base will --vagrantfile Vagrantfile.base --output will-base.box 测试12345678910111213# 添加制作完成的boxvagrant box add base will-base.box# 初始化vagrant init base# 启动vagrant up# 登录# 如果能ssh连接成功，表示基础box配置正确vagrant ssh-configvagrant ssh 参考文档 https://thornelabs.net/2013/11/11/create-a-centos-6-vagrant-base-box-from-scratch-using-virtualbox.html https://www.dravetech.com/blog/2016/01/14/vagrant_box_ios_xr.html http://blog.pangyanhan.com/posts/2015-11-10-creating-a-vagrant-base-box.html]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>vagrant</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用vagrant快速搭建linux实验环境]]></title>
    <url>%2Fposts%2F20%2F</url>
    <content type="text"><![CDATA[简介本文主要介绍如何使用vagrant配合virtualbox快速搭建实验环境。virtualbox是一个开源跨平台虚拟机管理软件，功能类似收费的vmwarevagrant是一个开源的虚拟机配置编排软件，可以在命令行快速启动管理虚拟机。 相关资源的百度云下载链接链接：https://pan.baidu.com/s/1nt_b96SEOIIWl2gIrabPpg 密码：6c3d 安装1.安装virtualbox 官方下载virtualbox对应平台的软件包安装 2.安装vagrant 官方下载vagrant对应平台的软件包安装，由于官方网站在国外，可能下载比较慢。 3.设置virtualbox虚拟机存方目录 1234# 默认情况下 virtualbox 启动虚拟机会存放在用户的家目录里# windows 下C盘可能过小，需要设置特定目录存储虚拟机VBoxManage setproperty machinefolder D:\virtualboxVBoxManage list systemproperties | grep machine 搭建实验环境1.下载导入相关box 1234# 由于需要从国外拉取box，可能会很慢。推荐使用我存储在百度云的box# 导入boxvagrant box add centos-7.4-base centos-7.4-base.boxvagrant box list 2.启动单机 1234567891011121314151617181920212223242526272829303132mkdir single &amp;&amp; cd singlecat &gt;Vagrantfile&lt;&lt;EOF# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| config.vm.box = "centos-7.4-base" config.vm.hostname = "will" config.ssh.insert_key = false # 指定CPU和内存大小 config.vm.provider "virtualbox" do |v| v.memory = 1024 v.cpus = 2 v.customize ["modifyvm", :id, "--name", "will"] end # 配置网络 config.vm.network "private_network", ip: "11.11.11.111" # config.vm.network "private_network", ip: "192.168.22.10" # 配置启动后的操作 config.vm.provision "shell", inline: &lt;&lt;-SHELL hostname SHELLendEOF# 启动vagrant up 3.启动多主机 123456789101112131415161718192021222324252627282930mkdir double &amp;&amp; cd doublecat &gt;Vagrantfile&lt;&lt;EOF# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| config.vm.define "web" do |web| web.vm.provider "virtualbox" do |v| v.customize ["modifyvm", :id, "--name", "web", "--memory", "512"] end web.vm.box = "centos-6.9" web.vm.hostname = "web" web.vm.network "private_network", ip: "11.11.11.11" end config.vm.define "db" do |db| db.vm.provider "virtualbox" do |v| v.customize ["modifyvm", :id, "--name", "db", "--memory", "512"] end db.vm.box = "centos-6.9" db.vm.hostname = "db" db.vm.network "private_network", ip: "11.11.11.22" endendEOF# 启动vagrant up 3.启动集群 12345678910111213141516171819202122232425262728mkdir cluster &amp;&amp; cd clustercat &gt;Vagrantfile&lt;&lt;EOF# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| (1..6).each do |i| config.vm.define "lab#&#123;i&#125;" do |node| node.vm.box = "centos-7.4-docker-17" node.ssh.insert_key = false node.vm.hostname = "lab#&#123;i&#125;" node.vm.network "private_network", ip: "11.11.11.11#&#123;i&#125;" node.vm.network "private_network", ip: "11.11.12.11#&#123;i&#125;" node.vm.provision "shell", inline: "echo hello from node #&#123;i&#125;" node.vm.provider "virtualbox" do |v| v.cpus = 4 v.customize ["modifyvm", :id, "--name", "lab#&#123;i&#125;", "--memory", "2048"] end end endendEOF# 启动vagrant up 4.连接虚拟机 1234# 一般情况下虚拟机ssh连接用户名为 vagrant# 一般情况下都不支持 密码登录， 可以登录之后自行配置支持密码登录# 连接的sshkey存储在用户家目录下 .vagrant.d 目录下# C:\Users\will\.vagrant.d\insecure_private_key 常用命令 以下命令后面都可以接虚拟机名，只对指定虚拟机作操作 启动虚拟机 vagrant up 暂停虚拟机 vagrant suspend 关闭虚拟机 vagrant halt 删除虚拟机 vagrant destroy 存储快照 vagrant snapshot save lab1 init 恢复快照 vagrant snapshot restore lab1 init]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>vagrant</tag>
        <tag>virtualbox</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7配置k8s集群使用coredns]]></title>
    <url>%2Fposts%2F19%2F</url>
    <content type="text"><![CDATA[简介CoreDNS是一个Go语言实现的链式插件DNS服务端，是CNCF成员，是一个高性能、易扩展的DNS服务端。可以很方便的部署在k8s集群中，用来代替kube-dns。 使用kubeadm初始化时指定 安装方法与《centos7使用kubeadm安装k8s集群》基本一致只需要简单修改kubeadm-master.config配置文件 123456789101112apiVersion: kubeadm.k8s.io/v1alpha1kind: MasterConfigurationkubernetesVersion: v1.9.0imageRepository: registry.cn-shanghai.aliyuncs.com/gcr-k8setcd: image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/etcd-amd64:3.1.10api: advertiseAddress: 11.11.11.111networking: podSubnet: 10.244.0.0/16featureGates: CoreDNS: true 单独部署coredns 不依赖kubeadm的方式，适用于不是使用kubeadm创建的k8s集群，或者kubeadm初始化集群之后，删除了dns相关部署。 123456789101112# 在calico网络中也配置一个coredns# 10.96.0.10 为k8s官方指定的kube-dns地址mkdir coredns &amp;&amp; cd corednswget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sedwget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/deploy.shchmod +x deploy.sh./deploy.sh -i 10.96.0.10 &gt; coredns.ymlkubectl apply -f coredns.yml# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k8s与各网络插件集成]]></title>
    <url>%2Fposts%2F18%2F</url>
    <content type="text"><![CDATA[通用说明 如果多次换不同网络插件实验，每次实验前先把/etc/cni/net.d/目录下文件清空 1rm -rf /etc/cni/net.d/* flannel1234567891011121314151617181920212223242526272829303132333435363738# 创建flannel目录下载相关文件mkdir flannel &amp;&amp; cd flannelwget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml# 修改配置# 此处的ip配置要与kubeadm的pod-network参数配置的一致 net-conf.json: | &#123; "Network": "192.168.0.0/16", "Backend": &#123; "Type": "vxlan" &#125; &#125;# 修改镜像image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64# 如果Node有多个网卡的话，参考flannel issues 39701，# https://github.com/kubernetes/kubernetes/issues/39701# 目前需要在kube-flannel.yml中使用--iface参数指定集群主机内网网卡的名称，# 否则可能会出现dns无法解析。容器无法通信的情况，需要将kube-flannel.yml下载到本地，# flanneld启动参数加上--iface=&lt;iface-name&gt; containers: - name: kube-flannel image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=eth1# 启动kubectl apply -f kube-flannel.yml# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system calico1.配置启动etcd集群 12# 本次实验使用与k8s一个etcd集群# 生境环境建议使用单独的一套集群 2.配置启动calico 1234567891011121314151617181920# 创建calico目录下载相关文件mkdir calico &amp;&amp; cd calicowget https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/rbac.yamlwget https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/calico.yaml# 如果启用了RBAC（默认k8s集群启用），配置RBACkubectl apply -f rbac.yaml# 修改calico.yaml文件中名为calico-config的ConfigMap中的etcd_endpoints参数为自己的etcd集群etcd_endpoints: "http://11.11.11.111:2379,http://11.11.11.112:2379,http://11.11.11.113:2379"# 修改镜像为国内镜像sed -i 's@image: quay.io/calico/@image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/calico-@g' calico.yaml# 启动kubectl apply -f calico.yaml# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system 3.参考文档 https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/calico#installing-with-the-etcd-datastore canal123456789101112131415161718192021222324252627282930313233343536373839# 创建flannel目录下载相关文件mkdir canal &amp;&amp; cd canalwget https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/canal/rbac.yamlwget https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/canal/canal.yaml# 修改配置# 此处的ip配置要与kubeadm的pod-network参数配置的一致 net-conf.json: | &#123; "Network": "192.168.0.0/16", "Backend": &#123; "Type": "vxlan" &#125; &#125;# 修改calico镜像sed -i 's@image: quay.io/calico/@image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/calico-@g' canal.yaml# 修改flannel镜像image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64# 如果Node有多个网卡的话，参考flannel issues 39701，# https://github.com/kubernetes/kubernetes/issues/39701# 目前需要在kube-flannel.yml中使用--iface参数指定集群主机内网网卡的名称，# 否则可能会出现dns无法解析。容器无法通信的情况，需要将kube-flannel.yml下载到本地，# flanneld启动参数加上--iface=&lt;iface-name&gt; containers: - name: kube-flannel image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64 command: [ "/opt/bin/flanneld", "--ip-masq", "--kube-subnet-mgr", "--iface=eth1" ]# 启动kubectl apply -f rbac.yamlkubectl apply -f canal.yaml# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system kube-router123456789101112131415161718192021222324# 本次实验重新创建了集群，使用之前测试其他网络插件的集群环境没有成功# 可能是由于环境干扰，实验时需要注意# 创建kube-router目录下载相关文件mkdir kube-router &amp;&amp; cd kube-routerwget https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter.yamlwget https://raw.githubusercontent.com/cloudnativelabs/kube-router/master/daemonset/kubeadm-kuberouter-all-features.yaml# 以下两种部署方式任选其一# 1. 只启用 pod网络通信，网络隔离策略 功能kubectl apply -f kubeadm-kuberouter.yaml# 2. 启用 pod网络通信，网络隔离策略，服务代理 所有功能# 删除kube-proxy和其之前配置的服务代理kubectl apply -f kubeadm-kuberouter-all-features.yamlkubectl -n kube-system delete ds kube-proxy# 在每个节点上执行docker run --privileged --net=host registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64:v1.10.2 kube-proxy --cleanup# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system romana1234567891011121314# 创建flannel目录下载相关文件mkdir romana &amp;&amp; cd romanawget https://raw.githubusercontent.com/romana/romana/master/containerize/specs/romana-kubeadm.yml# 修改镜像sed -i 's@gcr.io/@registry.cn-hangzhou.aliyuncs.com/@g' romana-kubeadm.ymlsed -i 's@quay.io/romana/@registry.cn-shanghai.aliyuncs.com/gcr-k8s/romana-@g' romana-kubeadm.yml# 启动kubectl apply -f romana-kubeadm.yml# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system CNI-Genie12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# CNI-Genie是华为开源的网络组件，可以使k8s同时部署多个网络插件# 在k8s集群中安装calico组件# 在k8s集群中安装flannel组件# 在k8s集群中安装Genie组件mkdir CNI-Genie &amp;&amp; cd CNI-Geniewget https://raw.githubusercontent.com/Huawei-PaaS/CNI-Genie/master/conf/1.8/genie.yamlsed -i 's@image: quay.io/cnigenie/v1.5:latest@image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/cnigenie-v1.5:latest@g' genie.yamlkubectl apply -f genie.yaml# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system# 测试cat &gt;nginx-calico.yml&lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: nginx-calico labels: app: web annotations: cni: "calico"spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80EOFcat &gt;nginx-flannel.yml&lt;&lt;EOFapiVersion: v1kind: Podmetadata: name: nginx-flannel labels: app: web annotations: cni: "flannel"spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80EOFkubectl apply -f nginx-calico.ymlkubectl apply -f nginx-flannel.yml# 查看kubectl get pods -o wide# 测试网络通信kubectl exec nginx-calico -i -t -- ping -c4 1.1.1.1kubectl exec nginx-flannel -i -t -- ping -c4 1.1.1.1# 由于先启动的flannel，然后k8s创建了coredns，所以使用flannel cni的能正常使用dns# 使用calico cni无法使用正常dns# 测试dnskubectl exec nginx-calico -i -t -- ping -c4 www.baidu.comkubectl exec nginx-flannel -i -t -- ping -c4 www.baidu.com 总结 kube-router性能损失最小，时延最小，其他网络插件性能差距不大。除了flannel没有网络隔离策略，其他均支持网络隔离策略。CNI-Genie是一个可以让k8s使用多个cni网络插件的组件，暂时不支持隔离策略。 理论结果： kube-router &gt; calico &gt; canal = flannel = romana]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7使用kubeadm配置高可用k8s集群]]></title>
    <url>%2Fposts%2F17%2F</url>
    <content type="text"><![CDATA[简介使用kubeadm配置多master节点，实现高可用。 安装实验环境说明实验架构图12345678lab1: etcd master haproxy keepalived 11.11.11.111lab2: etcd master haproxy keepalived 11.11.11.112lab3: etcd master haproxy keepalived 11.11.11.113lab4: node 11.11.11.114lab5: node 11.11.11.115lab6: node 11.11.11.116vip(loadblancer ip): 11.11.11.110 实验使用的Vagrantfile123456789101112131415161718192021# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| (1..6).each do |i| config.vm.define "lab#&#123;i&#125;" do |node| node.vm.box = "centos-7.4-docker-17" node.ssh.insert_key = false node.vm.hostname = "lab#&#123;i&#125;" node.vm.network "private_network", ip: "11.11.11.11#&#123;i&#125;" node.vm.provision "shell", inline: "echo hello from node #&#123;i&#125;" node.vm.provider "virtualbox" do |v| v.cpus = 2 v.customize ["modifyvm", :id, "--name", "lab#&#123;i&#125;", "--memory", "2048"] end end endend 在所有机器上安装kubeadm参考之前的文章《centos7安装kubeadm》 配置所有节点的kubelet12345678910# 配置kubelet使用国内可用镜像# 修改/etc/systemd/system/kubelet.service.d/10-kubeadm.conf# 添加如下配置 Environment="KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.cn-shanghai.aliyuncs.com/gcr-k8s/pause-amd64:3.0"# 使用命令sed -i '/ExecStart=$/i Environment="KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.cn-shanghai.aliyuncs.com/gcr-k8s/pause-amd64:3.0"' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf# 重新载入配置systemctl daemon-reload 配置所有节点的hosts12345678cat &gt;&gt;/etc/hosts&lt;&lt;EOF11.11.11.111 lab111.11.11.112 lab211.11.11.113 lab311.11.11.114 lab411.11.11.115 lab511.11.11.116 lab6EOF 启动etcd集群在lab1,lab2,lab3节点上启动etcd集群 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# lab1docker stop etcd &amp;&amp; docker rm etcdrm -rf /data/etcdmkdir -p /data/etcddocker run -d \--restart always \-v /etc/etcd/ssl/certs:/etc/ssl/certs \-v /data/etcd:/var/lib/etcd \-p 2380:2380 \-p 2379:2379 \--name etcd \registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:3.1.12 \etcd --name=etcd0 \--advertise-client-urls=http://11.11.11.111:2379 \--listen-client-urls=http://0.0.0.0:2379 \--initial-advertise-peer-urls=http://11.11.11.111:2380 \--listen-peer-urls=http://0.0.0.0:2380 \--initial-cluster-token=9477af68bbee1b9ae037d6fd9e7efefd \--initial-cluster=etcd0=http://11.11.11.111:2380,etcd1=http://11.11.11.112:2380,etcd2=http://11.11.11.113:2380 \--initial-cluster-state=new \--auto-tls \--peer-auto-tls \--data-dir=/var/lib/etcd# lab2docker stop etcd &amp;&amp; docker rm etcdrm -rf /data/etcdmkdir -p /data/etcddocker run -d \--restart always \-v /etc/etcd/ssl/certs:/etc/ssl/certs \-v /data/etcd:/var/lib/etcd \-p 2380:2380 \-p 2379:2379 \--name etcd \registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:3.1.12 \etcd --name=etcd1 \--advertise-client-urls=http://11.11.11.112:2379 \--listen-client-urls=http://0.0.0.0:2379 \--initial-advertise-peer-urls=http://11.11.11.112:2380 \--listen-peer-urls=http://0.0.0.0:2380 \--initial-cluster-token=9477af68bbee1b9ae037d6fd9e7efefd \--initial-cluster=etcd0=http://11.11.11.111:2380,etcd1=http://11.11.11.112:2380,etcd2=http://11.11.11.113:2380 \--initial-cluster-state=new \--auto-tls \--peer-auto-tls \--data-dir=/var/lib/etcd# lab3docker stop etcd &amp;&amp; docker rm etcdrm -rf /data/etcdmkdir -p /data/etcddocker run -d \--restart always \-v /etc/etcd/ssl/certs:/etc/ssl/certs \-v /data/etcd:/var/lib/etcd \-p 2380:2380 \-p 2379:2379 \--name etcd \registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:3.1.12 \etcd --name=etcd2 \--advertise-client-urls=http://11.11.11.113:2379 \--listen-client-urls=http://0.0.0.0:2379 \--initial-advertise-peer-urls=http://11.11.11.113:2380 \--listen-peer-urls=http://0.0.0.0:2380 \--initial-cluster-token=9477af68bbee1b9ae037d6fd9e7efefd \--initial-cluster=etcd0=http://11.11.11.111:2380,etcd1=http://11.11.11.112:2380,etcd2=http://11.11.11.113:2380 \--initial-cluster-state=new \--auto-tls \--peer-auto-tls \--data-dir=/var/lib/etcd# 验证查看集群docker exec -ti etcd ashetcdctl member listetcdctl cluster-healthexit 在第一台master节点初始化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106# 生成token# 保留token后面还要使用token=$(kubeadm token generate)echo $token# 生成配置文件cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha1kind: MasterConfigurationkubernetesVersion: v1.10.1#imageRepository: registry.cn-shanghai.aliyuncs.com/gcr-k8simageRepository: registry.cn-hangzhou.aliyuncs.com/google_containersapi: advertiseAddress: 11.11.11.111apiServerExtraArgs: endpoint-reconciler-type: leasecontrollerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 192.168.0.0/16etcd: endpoints: - "http://11.11.11.111:2379" - "http://11.11.11.112:2379" - "http://11.11.11.113:2379"apiServerCertSANs:- "lab1"- "lab2"- "lab3"- "11.11.11.111"- "11.11.11.112"- "11.11.11.113"- "11.11.11.110"- "127.0.0.1"token: $tokentokenTTL: "0"featureGates: CoreDNS: trueEOF# 初始化kubeadm init --config kubeadm-master.configsystemctl enable kubelet# 保存初始化完成之后的join命令# 如果丢失可以使用命令"kubeadm token list"获取# kubeadm join 11.11.11.111:6443 --token nevmjk.iuh214fc8i0k3iue --discovery-token-ca-cert-hash sha256:0e4f738348be836ff810bce754e059054845f44f01619a37b817eba83282d80f# 配置kubectl使用mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# 安装网络插件# 下载配置mkdir flannel &amp;&amp; cd flannelwget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml# 修改配置# 此处的ip配置要与上面kubeadm的pod-network一致 net-conf.json: | &#123; "Network": "192.168.0.0/16", "Backend": &#123; "Type": "vxlan" &#125; &#125;# 修改镜像image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64# 启动kubectl apply -f kube-flannel.yml# 如果Node有多个网卡的话，参考flannel issues 39701，# https://github.com/kubernetes/kubernetes/issues/39701# 目前需要在kube-flannel.yml中使用--iface参数指定集群主机内网网卡的名称，# 否则可能会出现dns无法解析。容器无法通信的情况，需要将kube-flannel.yml下载到本地，# flanneld启动参数加上--iface=&lt;iface-name&gt; containers: - name: kube-flannel image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=eth1# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system# 设置master允许部署应用pod，参与工作负载，现在可以部署其他系统组件# 如 dashboard, heapster, efk等kubectl taint nodes --all node-role.kubernetes.io/master- 启动其他master节点1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# 打包第一台master初始化之后的/etc/kubernetes/pki目录cd /etc/kubernetes &amp;&amp; tar czvf /root/pki.tgz pki/ &amp;&amp; cd ~# 上传到其他master的/etc/kubernetes目录下tar xf pki.tgz -C /etc/kubernetes/# 删除pki目录下的apiserver.crt 和 apiserver.key文件rm -rf /etc/kubernetes/pki/&#123;apiserver.crt,apiserver.key&#125;# 生成配置文件# 使用和之前master一样的配置文件# token保持一致cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha1kind: MasterConfigurationkubernetesVersion: v1.10.1#imageRepository: registry.cn-shanghai.aliyuncs.com/gcr-k8simageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers# 注意修改IPapi: advertiseAddress: 11.11.11.112apiServerExtraArgs: endpoint-reconciler-type: leasecontrollerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 192.168.0.0/16etcd: endpoints: - "http://11.11.11.111:2379" - "http://11.11.11.112:2379" - "http://11.11.11.113:2379"apiServerCertSANs:- lab1- lab2- lab3- "11.11.11.111"- "11.11.11.112"- "11.11.11.113"- "11.11.11.110"- "127.0.0.1"token: nevmjk.iuh214fc8i0k3iuetokenTTL: "0"featureGates: CoreDNS: trueEOF# 初始化kubeadm init --config kubeadm-master.configsystemctl enable kubelet# 查看状态kubectl get pod --all-namespaces -o wide | grep lab1kubectl get pod --all-namespaces -o wide | grep lab2kubectl get pod --all-namespaces -o wide | grep lab3kubectl get nodes -o wide 配置haproxy代理和keepalived在lab1,lab2,lab3节点上启动haproxy和keepalived 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697# 拉取haproxy镜像docker pull haproxy:1.7.8-alpinemkdir /etc/haproxycat &gt;/etc/haproxy/haproxy.cfg&lt;&lt;EOFglobal log 127.0.0.1 local0 err maxconn 50000 uid 99 gid 99 #daemon nbproc 1 pidfile haproxy.piddefaults mode http log 127.0.0.1 local0 err maxconn 50000 retries 3 timeout connect 5s timeout client 30s timeout server 30s timeout check 2slisten admin_stats mode http bind 0.0.0.0:1080 log 127.0.0.1 local0 err stats refresh 30s stats uri /haproxy-status stats realm Haproxy\ Statistics stats auth will:will stats hide-version stats admin if TRUEfrontend k8s-https bind 0.0.0.0:8443 mode tcp #maxconn 50000 default_backend k8s-httpsbackend k8s-https mode tcp balance roundrobin server lab1 11.11.11.111:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server lab2 11.11.11.112:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server lab3 11.11.11.113:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3EOF# 启动haproxydocker run -d --name my-haproxy \-v /etc/haproxy:/usr/local/etc/haproxy:ro \-p 8443:8443 \-p 1080:1080 \--restart always \haproxy:1.7.8-alpine# 查看日志docker logs my-haproxy# 浏览器查看状态http://11.11.11.111:1080/haproxy-statushttp://11.11.11.112:1080/haproxy-status# 拉取keepalived镜像docker pull osixia/keepalived:1.4.4# 启动# 载入内核相关模块lsmod | grep ip_vsmodprobe ip_vs# 启动keepalived# eth1为本次实验11.11.11.0/24网段的所在网卡docker run --net=host --cap-add=NET_ADMIN \-e KEEPALIVED_INTERFACE=eth1 \-e KEEPALIVED_VIRTUAL_IPS="#PYTHON2BASH:['11.11.11.110']" \-e KEEPALIVED_UNICAST_PEERS="#PYTHON2BASH:['11.11.11.111','11.11.11.112','11.11.11.113']" \-e KEEPALIVED_PASSWORD=hello \--name k8s-keepalived \--restart always \-d osixia/keepalived:1.4.4# 查看日志# 会看到两个成为backup 一个成为masterdocker logs k8s-keepalived# 此时会配置 11.11.11.110 到其中一台机器# ping测试ping -c4 11.11.11.110# 如果失败后清理后，重新实验docker rm -f k8s-keepalivedip a del 11.11.11.110/32 dev eth1# 修改~/.kube/config文件里ip和端口，然后使用kubectl测试rm -rf .kube/cache .kube/http-cachekubectl get pods -n kube-system -o wide 修改master节点相关组件配置指向vip123456789# lab1 lab2 lab3sed -i 's@server: https://11.11.11.*:6443@server: https://11.11.11.110:8443@g' /etc/kubernetes/&#123;admin.conf,kubelet.conf,scheduler.conf,controller-manager.conf&#125;# 重启kubeletsystemctl daemon-reloadsystemctl restart kubelet docker# 查看所有节点状态kubectl get nodes -o wide 修改kube-proxy的配置12345678910111213# 修改kube-proxy的配置指定vip# 执行命令之后修改为 server: https://11.11.11.110:8443kubectl edit -n kube-system configmap/kube-proxy# 查看设置kubectl get -n kube-system configmap/kube-proxy -o yaml# 删除重建kube-proxykubectl get pods --all-namespaces -o wide | grep proxyall_proxy_pods=$(kubectl get pods --all-namespaces -o wide | grep proxy | awk '&#123;print $2&#125;' | xargs)echo $all_proxy_podskubectl delete pods $all_proxy_pods -n kube-systemkubectl get pods --all-namespaces -o wide | grep proxy 启动node节点1234# 加入master节点# 这个命令是之前初始化master完成时，输出的命令kubeadm join 11.11.11.110:8443 --token nevmjk.iuh214fc8i0k3iue --discovery-token-ca-cert-hash sha256:0e4f738348be836ff810bce754e059054845f44f01619a37b817eba83282d80fsystemctl enable kubelet 修改node节点kubelet配置并重启123456789# 修改配置sed -i 's@server: https://11.11.11.*:6443@server: https://11.11.11.110:8443@g' /etc/kubernetes/kubelet.conf# 重启kubeletsystemctl daemon-reloadsystemctl restart kubelet docker# 查看所有节点状态kubectl get nodes -o wide 禁止master节点发布应用 设置master不接受负载123456789# 查看状态kubectl get nodes# 设置# kubectl patch node lab1 -p '&#123;"spec":&#123;"unschedulable":true&#125;&#125;'kubectl taint nodes lab1 lab2 lab3 node-role.kubernetes.io/master=true:NoSchedule# 查看状态kubectl get nodes 测试重建多个coredns副本12345678910111213# 删除coredns的podskubectl get pods -n kube-system -o wide | grep corednsall_coredns_pods=$(kubectl get pods -n kube-system -o wide | grep coredns | awk '&#123;print $1&#125;' | xargs)echo $all_coredns_podskubectl delete pods $all_coredns_pods -n kube-system# 修改副本数# replicas: 3# 可以修改为node节点的个数kubectl edit deploy coredns -n kube-system# 查看状态kubectl get pods -n kube-system -o wide | grep coredns 基础测试1. 启动12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364# 直接使用命令测试kubectl run nginx --replicas=2 --image=nginx:alpine --port=80kubectl expose deployment nginx --type=NodePort --name=example-service-nodeportkubectl expose deployment nginx --name=example-service# 使用配置文件测试cat &gt;example-nginx.yml&lt;&lt;EOFapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginxspec: replicas: 2 template: metadata: labels: app: nginx spec: restartPolicy: Always containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 livenessProbe: httpGet: path: / port: 80 initialDelaySeconds: 10 periodSeconds: 3 readinessProbe: httpGet: path: / port: 80 initialDelaySeconds: 10 periodSeconds: 3---kind: ServiceapiVersion: v1metadata: name: example-servicespec: selector: app: nginx ports: - name: http port: 80 targetPort: 80---kind: ServiceapiVersion: v1metadata: name: example-service-nodeportspec: selector: app: nginx type: NodePort ports: - name: http-nodeport port: 80 nodePort: 32223EOFkubectl apply -f example-nginx.yml 2. 查看状态1234kubectl get deploykubectl get podskubectl get svckubectl describe svc example-service 3. DNS解析12345678kubectl run curl --image=radial/busyboxplus:curl -i --ttynslookup kubernetesnslookup example-servicecurl example-service# 如果时间过长会返回错误，可以使用如下方式再进入测试curlPod=$(kubectl get pod | grep curl | awk '&#123;print $1&#125;')kubectl exec -ti $curlPod -- sh 4. 访问测试123456# 10.96.59.56 为查看svc时获取到的clusteripcurl "10.96.59.56:80"# 32223 为查看svc时获取到的 nodeporthttp://11.11.11.114:32223/http://11.11.11.115:32223/ 3. 清理删除12kubectl delete svc example-service example-service-nodeportkubectl delete deploy nginx curl 高可用测试关闭master节点测试集群是能否正常执行上一步的基础测试，查看相关信息，不能同时关闭lab1和lab2，因为上面有haproxy和keepalived服务 123456789kubectl get pod --all-namespaces -o widekubectl get pod --all-namespaces -o wide | grep lab1kubectl get pod --all-namespaces -o wide | grep lab2kubectl get pod --all-namespaces -o wide | grep lab3kubectl get nodes -o widekubectl get deploykubectl get podskubectl get svckubectl describe svc example-service 注意事项 当直接把node节点关闭时，只有过了5分钟之后，上面的pod才会被检测到有问题，并迁移到其他节点 如果想快速迁移可以执行 kubectl delete node 也可以修改controller-manager的的pod-eviction-timeout参数，默认5m node-monitor-grace-period参数，默认40s 参考文档 https://kubernetes.io/docs/admin/high-availability/ https://www.kubernetes.org.cn/3536.html https://github.com/indiketa/kubeadm-ha https://zhuanlan.zhihu.com/p/34740013 https://github.com/cookeem/kubeadm-ha/blob/master/README_CN.md https://blog.frognew.com/2017/04/install-etcd-cluster.html https://blog.frognew.com/2017/04/install-ha-kubernetes-1.6-cluster.html https://medium.com/@bambash/ha-kubernetes-cluster-via-kubeadm-b2133360b198 https://github.com/kubernetes/kubeadm/issues/546 https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7使用kubeadm安装k8s集群]]></title>
    <url>%2Fposts%2F16%2F</url>
    <content type="text"><![CDATA[实验环境说明实验架构123lab1: master 11.11.11.111lab2: node 11.11.11.112lab3: node 11.11.11.113 实验使用的Vagrantfile123456789101112131415161718192021# -*- mode: ruby -*-# vi: set ft=ruby :ENV["LC_ALL"] = "en_US.UTF-8"Vagrant.configure("2") do |config| (1..3).each do |i| config.vm.define "lab#&#123;i&#125;" do |node| node.vm.box = "centos-7.4-docker-17" node.ssh.insert_key = false node.vm.hostname = "lab#&#123;i&#125;" node.vm.network "private_network", ip: "11.11.11.11#&#123;i&#125;" node.vm.provision "shell", inline: "echo hello from node #&#123;i&#125;" node.vm.provider "virtualbox" do |v| v.cpus = 2 v.customize ["modifyvm", :id, "--name", "lab#&#123;i&#125;", "--memory", "2048"] end end endend 安装要求 需要在每个节点上先安装好kubeadm 有每个节点配置好hosts解析 12345cat &gt;&gt;/etc/hosts&lt;&lt;EOF11.11.11.111 lab111.11.11.112 lab211.11.11.113 lab3EOF 安装配置master节点说明 由于kubeadm默认会去拉取gcr.io上的镜像来启动master相关的组件，由于在国内无法访问gcr.io所以会导致无法成功启动。有如下几种解决办法： 在能翻墙的机器上拉取镜像，再打包导入到master机器上（docker save/load） 直接拉国内别人的镜像，然后打tag为gcr.io的镜像 启动配置集群的时候指定镜像相关配置，使用阿里镜像（本次实验采用） 初始化12345678910111213141516171819202122232425262728293031cat &gt;kubeadm-master.config&lt;&lt;EOFapiVersion: kubeadm.k8s.io/v1alpha1kind: MasterConfigurationkubernetesVersion: v1.10.7imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containersetcd: image: registry.cn-hangzhou.aliyuncs.com/google_containers/etcd-amd64:3.1.12api: advertiseAddress: 11.11.11.111controllerManagerExtraArgs: node-monitor-grace-period: 10s pod-eviction-timeout: 10snetworking: podSubnet: 10.244.0.0/16EOF# 配置kubelet使用国内可用pause镜像# 修改/etc/systemd/system/kubelet.service.d/10-kubeadm.conf# 添加如下配置sed -i '/ExecStart=$/i Environment="KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.cn-shanghai.aliyuncs.com/gcr-k8s/pause-amd64:3.0"' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf# 重新载入配置systemctl daemon-reload# 设置kubelet开机启动systemctl enable kubelet# 使用配置文件方式kubeadm init --config kubeadm-master.config 没有经过测试的另一种初始化方法12345# 使用命令行方式KUBE_REPO_PREFIX='registry.cn-hangzhou.aliyuncs.com/google_containers' kubeadm init \--kubernetes-version=v1.10.3 \--pod-network-cidr=10.244.0.0/16 \--apiserver-advertise-address=11.11.11.111 配置kubectl使用123456789101112rm -rf $HOME/.kubemkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# 查看node节点kubectl get nodes# 只有网络插件也安装配置完成之后，才能会显示为ready状态# 设置master允许部署应用pod，参与工作负载，现在可以部署其他系统组件# 如 dashboard, heapster, efk等kubectl taint nodes --all node-role.kubernetes.io/master- 配置使用网络插件配置使用flannel 1234567891011121314151617181920212223242526272829303132333435363738# 下载配置mkdir flannel &amp;&amp; cd flannelwget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml# 修改配置# 此处的ip配置要与上面kubeadm的pod-network一致 net-conf.json: | &#123; "Network": "10.244.0.0/16", "Backend": &#123; "Type": "vxlan" &#125; &#125;# 修改镜像image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64# 如果Node有多个网卡的话，参考flannel issues 39701，# https://github.com/kubernetes/kubernetes/issues/39701# 目前需要在kube-flannel.yml中使用--iface参数指定集群主机内网网卡的名称，# 否则可能会出现dns无法解析。容器无法通信的情况，需要将kube-flannel.yml下载到本地，# flanneld启动参数加上--iface=&lt;iface-name&gt; containers: - name: kube-flannel image: registry.cn-shanghai.aliyuncs.com/gcr-k8s/flannel:v0.10.0-amd64 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=eth1# 启动kubectl apply -f kube-flannel.yml# 查看kubectl get pods --namespace kube-systemkubectl get svc --namespace kube-system 安装配置node节点配置12345678910# 配置kubelet使用国内可用pause镜像# 修改/etc/systemd/system/kubelet.service.d/10-kubeadm.conf# 添加如下配置sed -i '/ExecStart=$/i Environment="KUBELET_EXTRA_ARGS=--pod-infra-container-image=registry.cn-shanghai.aliyuncs.com/gcr-k8s/pause-amd64:3.0"' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf# 重新载入配置systemctl daemon-reload# 设置kubelet开机启动systemctl enable kubelet 加入集群12# 此命令为启动master成功后返回的结果kubeadm join --token 55a6f8.1091208463fe1252 11.11.11.111:6443 --discovery-token-ca-cert-hash sha256:790c6b38b087b167c1f52c04526d8729115192a305eb91c01c0fd8dc7facbbcd 测试容器间的通信和DNS 配置好calico网络之后，kubeadm会自动部署kube-dns 启动123kubectl run nginx --replicas=2 --image=nginx:alpine --port=80kubectl expose deployment nginx --type=NodePort --name=example-service-nodeportkubectl expose deployment nginx --name=example-service 查看状态1234kubectl get deploykubectl get podskubectl get svckubectl describe svc example-service DNS解析1234kubectl run curl --image=radial/busyboxplus:curl -i --ttynslookup kubernetesnslookup example-servicecurl example-service 访问测试123456# 10.96.59.56 为查看svc时获取到的clusteripcurl "10.96.59.56:80"# 32223 为查看svc时获取到的 nodeporthttp://11.11.11.112:32223/http://11.11.11.113:32223/ 清理删除12kubectl delete svc example-service example-service-nodeportkubectl delete deploy nginx curl 挖坑记1. 如果删除kube-dns后怎么修复123456# 第一种方法kubeadm upgrade apply v1.10.3 --config kubeadm-master.config# 第二种方法kubeadm config upload from-file --config kubeadm-master.configkubeadm upgrade apply v1.10.3 2. 忘记初始master节点时的node节点加入集群命令怎么办 123456# 简单方法kubeadm token create --print-join-command# 第二种方法token=$(kubeadm token generate)kubeadm token create $token --print-join-command --ttl=0 参考文档 https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/ https://blog.frognew.com/2017/12/kubeadm-install-kubernetes-1.9.html]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7安装kubeadm]]></title>
    <url>%2Fposts%2F15%2F</url>
    <content type="text"><![CDATA[关闭防火墙12systemctl stop firewalldsystemctl disable firewalld 安装配置docker v1.9.0版本推荐使用docker v1.12,v1.11, v1.13, 17.03也可以使用，再高版本的docker可能无法正常使用。测试发现17.09无法正常使用，不能使用资源限制(内存CPU) 安装docker12345# 卸载安装指定版本docker-ceyum remove -y docker-ce docker-ce-selinux container-selinuxyum install -y --setopt=obsoletes=0 \docker-ce-17.03.1.ce-1.el7.centos \docker-ce-selinux-17.03.1.ce-1.el7.centos 修改docker配置使用systemd 在/etc/docker/daemon.json文件添加如下配置centos7安装的docker12不能添加此参数，否则会无法启动123456789&#123; "exec-opts": ["native.cgroupdriver=systemd"]&#125;# 如果使用了加速器配置格式如下&#123; "registry-mirrors": ["https://tfhzn46h.mirror.aliyuncs.com"], "exec-opts": ["native.cgroupdriver=systemd"]&#125; 启动docker1systemctl enable docker &amp;&amp; systemctl restart docker 安装 kubeadm, kubelet 和 kubectl如下的安装方法，选择其中一个即可。 翻墙安装 需要翻墙才能正常安装，如果不能翻墙，可以使用阿里云或其他云提供的容器海外构建功能，下载好包。启动容器，之后从容器中把文件获取出来安装即可。 12345678910111213141516171819202122232425262728# 安装依赖yum install -y ebtables socat# 在能翻墙的机器上下载rpm包cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgEOFyum install -y yum-utilsk8s_version=$(yum info kubelet | grep Version | awk -F ':' '&#123;print $2&#125;' | tr -d ' ')mkdir k8s-$k8s_version &amp;&amp; cd k8s-$k8s_version# 下载最新版本yumdownloader kubelet kubeadm kubectl kubernetes-cni# 下载指定版本yumdownloader kubelet-1.10.7 kubeadm-1.10.7 kubectl-1.10.7 kubernetes-cni# 打包下载到要安装的机器上cd .. &amp;&amp; tar cvzf k8s-$&#123;k8s_version&#125;.tgz k8s-$k8s_version/# 安装tar xf k8s-*.tgz &amp;&amp; cd k8s-* &amp;&amp; yum localinstall -y *.rpm 使用阿里镜像安装12345678910111213141516# 配置源cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF# 安装最新版本yum install -y kubelet kubeadm kubectl# 安装指定版本yum install -y kubelet-1.10.7 kubeadm-1.10.7 kubectl-1.10.7 二进制安装方法（不推荐）此需要自己安装kubernetes-cni 1234567# 下载安装version=$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)wget https://storage.googleapis.com/kubernetes-release/release/$version/bin/linux/amd64/kubectlwget https://storage.googleapis.com/kubernetes-release/release/$version/bin/linux/amd64/kubeadmwget https://storage.googleapis.com/kubernetes-release/release/$version/bin/linux/amd64/kubeletchmod +x kubectl kubeadm kubeletmv kubectl kubeadm kubelet /usr/local/bin 实验所用kubeadm相关文件已经上传到了百度网盘链接：https://pan.baidu.com/s/1pl7YYUYZsPd98J0DhNLWEQ 密码：40na 配置系统相关参数12345678910111213141516171819202122232425# 临时禁用selinux# 永久关闭 修改/etc/sysconfig/selinux文件设置sed -i 's/SELINUX=permissive/SELINUX=disabled/' /etc/sysconfig/selinuxsetenforce 0# 临时关闭swap# 永久关闭 注释/etc/fstab文件里swap相关的行swapoff -a# 开启forward# Docker从1.13版本开始调整了默认的防火墙规则# 禁用了iptables filter表中FOWARD链# 这样会引起Kubernetes集群中跨Node的Pod无法通信iptables -P FORWARD ACCEPT# 配置转发相关参数，否则可能会出错cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1vm.swappiness=0EOFsysctl --system 参考文档 https://kubernetes.io/docs/setup/independent/install-kubeadm/ https://blog.frognew.com/2017/12/kubeadm-install-kubernetes-1.9.html]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7安装终端记录回放器asciinema]]></title>
    <url>%2Fposts%2F14%2F</url>
    <content type="text"><![CDATA[简介终端记录器，可以记录你在终端的任务操作，然后回放。官方示例网站https://asciinema.org/ 安装1yum install -y asciinema 简单使用1.简单记录并传到asciinema.org1234567891011121314151617# 简单asciinema rec# 指定标题asciinema rec -t "My git tutorial"# 记录保存到本地 asciinema rec demo.json# 指定最多等待间隔2.5sasciinema rec -w 2.5 demo.json# 重放操作asciinema play demo.json# 重放asciinema.org上的操作asciinema play https://asciinema.org/a/difqlgx86ym6emrmd8u62yqu8 简单配置123vim ~/.config/asciinema/config[api]url = http://asciinema.example.com 安装配置自己的asciinema-server 安装之前先安装docker-ce， 测试发现docker-12版本不能正常使用 1. 安装配置123456789101112131415git clone --recursive https://github.com/asciinema/asciinema-server.gitcd asciinema-servergit checkout mastercp .env.production.sample .env.productionvim .env.production# 设置url相关URL_SCHEME=httpURL_HOST=asciinema.example.comURL_PORT=80# 设置密钥secret=$(docker-compose run --rm web mix phx.gen.secret)SECRET_KEY_BASE=$secret 2. 初始化12docker-compose up -d postgresdocker-compose run --rm web setup 3. 启动12docker-compose up -ddocker ps -f 'name=asciinema_' 4. 浏览器访问测试12345# 配置hosts11.11.11.111 asciinema.example.com# 浏览器访问http://asciinema.example.com/ 5. 配置客户端12345678910111213# 配置hostecho '11.11.11.111 asciinema.example.com' &gt;&gt; /etc/hosts# ~/.config/asciinema/config[api]url = http://asciinema.example.com# 也可以使用环境变量ASCIINEMA_API_URL=http://asciinema.example.com asciinema rec# 注意# 如果没有token配置项会无法上传，可以先直接执行asciinema rec生成配置文件后# 再添加url的配置项 6. 测试使用1asciinema rec 参考文档 https://github.com/asciinema/asciinema/blob/master/README.md https://github.com/asciinema/asciinema-server/blob/master/docs/INSTALL.md]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>terminal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7配置使用代理]]></title>
    <url>%2Fposts%2F13%2F</url>
    <content type="text"><![CDATA[简介 本文档主要介绍如何在centos7上使用代理，访问墙外的网站。 shadowsocks实现sock5代理，privoxy把sock5代理转换为http代理。 安装shadowsocks123456789101112131415161718192021222324252627282930313233343536373839# 安装sudo su - rootyum -y install python-pippip install shadowsocks# 配置mkdir /etc/shadowsockscat &gt;/etc/shadowsocks/shadowsocks.json&lt;&lt;EOF&#123; "server":"47.52.xx.xx", "server_port":52113, "local_address": "127.0.0.1", "local_port":1080, "password":"xxxxxx", "timeout":300, "method":"aes-256-cfb", "fast_open": false, "workers": 1&#125;EOF# 启动脚本cat &gt;/etc/systemd/system/shadowsocks.service&lt;&lt;EOF[Unit]Description=Shadowsocks[Service]TimeoutStartSec=0ExecStart=/usr/bin/sslocal -c /etc/shadowsocks/shadowsocks.json[Install]WantedBy=multi-user.targetEOF# 启动并加入开机启动systemctl enable shadowsocks.servicesystemctl start shadowsocks.servicesystemctl status shadowsocks.service# 测试curl --socks5 127.0.0.1:1080 ip.cn 安装privoxy 另一种选择polipo 安装1yum -y install privoxy 以下两种模式选择一种 简单全局模式配置1echo 'forward-socks5t / 127.0.0.1:1080 . #转发到本地端口' &gt;&gt; /etc/privoxy/config PAC模式配置（推荐）12345678# 下载生成privoxy-action配置的脚本curl -skL https://raw.github.com/zfl9/gfwlist2privoxy/master/gfwlist2privoxy -o gfwlist2privoxy# 生成配置启动# '127.0.0.1:1080' 为你的sock5代理地址bash gfwlist2privoxy '127.0.0.1:1080'cp -af gfwlist.action /etc/privoxy/echo 'actionsfile gfwlist.action' &gt;&gt; /etc/privoxy/config 启动测试12345678910111213141516171819202122232425# 启动并加入开机启动systemctl enable privoxy.servicesystemctl start privoxy.servicesystemctl status privoxy.service# export http_proxy=http://127.0.0.1:8118# export https_proxy=http://127.0.0.1:8118# export all_proxy=http://127.0.0.1:8118# 加入环境变量中cat &gt;/etc/profile.d/proxy.sh&lt;&lt;EOFalias proxy='export all_proxy=http://127.0.0.1:8118'alias unproxy='unset all_proxy'EOF# 测试. /etc/profile.d/proxy.sh# 使用代理proxycurl www.google.comcurl ip.cn# 不使用代理unproxycurl ip.cn]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>proxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码整洁之道]]></title>
    <url>%2Fposts%2F12%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>开发</tag>
        <tag>代码规范</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7安装kubernetes-v1.7 Volumes和Persistent Volumes]]></title>
    <url>%2Fposts%2F11%2F</url>
    <content type="text"><![CDATA[简介Volumes用来存储要持久保留的数据，不想重启容器后就消失。POD中容器之间共享数据。PersistentVolume（PV）和 PersistentVolumeClaim（PVC）是kubernetes提供的 两种API资源，用于抽象存储细节。管理员关注于如何通过pv提供存储功能而无需关注用户如何使用，同样的用户只需要挂载PVC到容器中而不需要关注存储卷采用何种技术实现。PVC和PV的关系跟pod和node关系类似，前者消耗后者的资源。PVC可以向PV申请指定大小的存储资源并设置访问模式。 基本使用1234567891011121314151617181920212223242526272829303132333435363738apiVersion: v1kind: Podmetadata: name: counterspec: containers: - name: count image: busybox args: - /bin/sh - -c - &gt; i=0; while true; do echo "$i: $(date)" &gt;&gt; /var/log/1.log; echo "$(date) INFO $i" &gt;&gt; /var/log/2.log; i=$((i+1)); sleep 1; done volumeMounts: - name: varlog mountPath: /var/log - name: count-log-1 image: busybox args: [/bin/sh, -c, 'tail -n+1 -f /var/log/1.log'] volumeMounts: - name: varlog mountPath: /var/log - name: count-log-2 image: busybox args: [/bin/sh, -c, 'tail -n+1 -f /var/log/2.log'] volumeMounts: - name: varlog mountPath: /var/log volumes: - name: varlog emptyDir: &#123;&#125; 安装配置glusterfs1. 安装glusterfs1234567891011121314# 先安装 gluster 源yum install centos-release-gluster -y# 安装 glusterfs 组件yum install -y glusterfs glusterfs-server glusterfs-fuse \glusterfs-rdma glusterfs-geo-replication glusterfs-devel# 启动 glusterfssystemctl start glusterd.servicesystemctl enable glusterd.service# 查看状态systemctl status glusterd.service 2. 配置集群123456789101112131415# 配置 hosts192.168.12.211 lab1192.168.12.212 lab2192.168.12.213 lab3# 创建存储目录mkdir -pv /data/gfs# 添加节点到 集群# 执行操作的本机不需要probe 本机gluster peer probe lab2gluster peer probe lab3# 查看集群状态gluster peer status 3. 配置 volume1234567891011# 创建分布卷gluster volume create k8s-volume transport tcp \lab1:/data/gfs \lab2:/data/gfs \lab3:/data/gfs force# 查看volume状态gluster volume info# 启动 分布卷gluster volume start k8s-volume Kubernetes中配置glusterfs1. kubernetes安装客户端123# 在所有 node 节点安装yum install centos-release-gluster -yyum install -y glusterfs glusterfs-fuse 2. 配置 hosts123192.168.12.211 lab1192.168.12.212 lab2192.168.12.213 lab3 3. 配置 endpoints12345678910# 下载curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/examples/volumes/glusterfs/glusterfs-endpoints.json# 配置 glusters 集群节点ip每一个 addresses 为一个 ip 组# 创建kubectl apply -f glusterfs-endpoints.json# 查看kubectl get ep 4. 配置 service1234567curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/examples/volumes/glusterfs/glusterfs-service.json# 修改 port 的刚刚上一步中修改的 port# 创建kubectl apply -f glusterfs-service.jsonkubectl get svc 测试1. 创建测试 pod1234567891011121314curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/examples/volumes/glusterfs/glusterfs-pod.json# 修改 volumes 下的 path 为上面创建的 k8s-volume"path": "k8s-volume"# 创建kubectl apply -f glusterfs-pod.json# 查看kubectl get podskubectl describe pods/glusterfs#登陆 node 物理机，使用 df 可查看挂载目录df -h PV和PVC1. 配置PV12345678910111213141516171819202122# 创建pvcat &gt;glusterfs-pv.yaml&lt;&lt;EOFapiVersion: v1kind: PersistentVolumemetadata: name: gluster-dev-volumespec: capacity: storage: 8Gi accessModes: - ReadWriteMany glusterfs: endpoints: "glusterfs-cluster" path: "k8s-volume" readOnly: falseEOF# 创建kubectl apply -f glusterfs-pv.yaml# 查看 pvkubectl get pv 2. 配置PVC12345678910111213141516171819cat &gt;glusterfs-pvc.yaml&lt;&lt;EOFkind: PersistentVolumeClaimapiVersion: v1metadata: name: glusterfs-nginxspec: accessModes: - ReadWriteMany resources: requests: storage: 8GiEOF# 创建 pvckubectl apply -f glusterfs-pvc.yaml# 查看 pvckubectl get pvkubectl get pvc 3. 创建 nginx deployment 挂载 volume123456789101112131415161718192021222324252627282930313233cat &gt;nginx-deployment.yaml&lt;&lt;EOFapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: nginx-dmspec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: gluster-dev-volume mountPath: "/usr/share/nginx/html" volumes: - name: gluster-dev-volume persistentVolumeClaim: claimName: glusterfs-nginxEOF# 创建kubectl apply -f nginx-deployment.yamlkubectl expose deployment nginx-dm --name=nginx-dm-svc# 查看 deploymentkubectl get pods |grep nginx-dm 4. 测试12345678# 查看挂载kubectl exec -it nginx-dm-2194008866-szfl2 -- df -h | grep k8s-volume# 创建文件 测试kubectl exec -it nginx-dm-2194008866-szfl2 -- touch /usr/share/nginx/html/index.html# 查看文件kubectl exec -it nginx-dm-2194008866-szfl2 -- ls -lt /usr/share/nginx/html/index.html StorageClasshttps://kubernetes.io/docs/concepts/storage/persistent-volumes/#storageclasses 参考文档 https://kubernetes.io/docs/concepts/storage/volumes/]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7安装kubernetes-v1.7插件]]></title>
    <url>%2Fposts%2F10%2F</url>
    <content type="text"><![CDATA[本教程紧接 centos7安装kubernetes-v1.7 请参考。 安装和配置 kubedns 插件1. 拉取镜像1234567891011# 在所有node节点上操作# pulldocker pull jicki/k8s-dns-sidecar-amd64:1.14.4docker pull jicki/k8s-dns-kube-dns-amd64:1.14.4docker pull jicki/k8s-dns-dnsmasq-nanny-amd64:1.14.4# tagdocker tag jicki/k8s-dns-sidecar-amd64:1.14.4 gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.4docker tag jicki/k8s-dns-kube-dns-amd64:1.14.4 gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.4docker tag jicki/k8s-dns-dnsmasq-nanny-amd64:1.14.4 gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.4 2. 修改配置ymal123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212# 创建目录mkdir /server/software/k8s/dnscd /server/software/k8s/dns# config mapcat &gt;kubedns-cm.yaml&lt;&lt;EOFapiVersion: v1kind: ConfigMapmetadata: name: kube-dns namespace: kube-system labels: addonmanager.kubernetes.io/mode: EnsureExistsdata: upstreamNameservers: | ["114.114.114.114", "8.8.8.8"]EOF# service accountcat &gt;kubedns-sa.yaml&lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: kube-dns namespace: kube-system labels: kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: ReconcileEOF# controllercat &gt;kubedns-controller.yaml&lt;&lt;EOFapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilespec: # replicas: not specified here: # 1. In order to make Addon Manager do not reconcile this replicas parameter. # 2. Default is 1. # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on. strategy: rollingUpdate: maxSurge: 10% maxUnavailable: 0 selector: matchLabels: k8s-app: kube-dns template: metadata: labels: k8s-app: kube-dns annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: tolerations: - key: "CriticalAddonsOnly" operator: "Exists" volumes: - name: kube-dns-config configMap: name: kube-dns optional: true containers: - name: kubedns image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.4 resources: # TODO: Set memory limits when we've profiled the container for large # clusters, then set request = limit to keep this container in # guaranteed class. Currently, this container falls into the # "burstable" category so the kubelet doesn't backoff from restarting it. limits: memory: 170Mi requests: cpu: 100m memory: 70Mi livenessProbe: httpGet: path: /healthcheck/kubedns port: 10054 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: httpGet: path: /readiness port: 8081 scheme: HTTP # we poll on pod startup for the Kubernetes master service and # only setup the /readiness HTTP server once that's available. initialDelaySeconds: 3 timeoutSeconds: 5 args: - --domain=cluster.local. - --dns-port=10053 - --config-dir=/kube-dns-config - --v=2 #__PILLAR__FEDERATIONS__DOMAIN__MAP__ env: - name: PROMETHEUS_PORT value: "10055" ports: - containerPort: 10053 name: dns-local protocol: UDP - containerPort: 10053 name: dns-tcp-local protocol: TCP - containerPort: 10055 name: metrics protocol: TCP volumeMounts: - name: kube-dns-config mountPath: /kube-dns-config - name: dnsmasq image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.4 livenessProbe: httpGet: path: /healthcheck/dnsmasq port: 10054 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 args: - -v=2 - -logtostderr - -configDir=/etc/k8s/dns/dnsmasq-nanny - -restartDnsmasq=true - -- - -k - --cache-size=1000 - --log-facility=- - --server=/cluster.local./127.0.0.1#10053 - --server=/in-addr.arpa/127.0.0.1#10053 - --server=/ip6.arpa/127.0.0.1#10053 ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP # see: https://github.com/kubernetes/kubernetes/issues/29055 for details resources: requests: cpu: 150m memory: 20Mi volumeMounts: - name: kube-dns-config mountPath: /etc/k8s/dns/dnsmasq-nanny - name: sidecar image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.4 livenessProbe: httpGet: path: /metrics port: 10054 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 args: - --v=2 - --logtostderr - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local.,5,A - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local.,5,A ports: - containerPort: 10054 name: metrics protocol: TCP resources: requests: memory: 20Mi cpu: 10m dnsPolicy: Default # Don't use cluster DNS. serviceAccountName: kube-dnsEOF# servicecat &gt;kubedns-svc.yaml&lt;&lt;EOFapiVersion: v1kind: Servicemetadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: "KubeDNS"spec: selector: k8s-app: kube-dns clusterIP: 10.254.0.2 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCPEOF 3. 系统预定义的 RoleBinding预定义的 RoleBinding system:kube-dns 将 kube-system 命名空间的 kubednsServiceAccount 与 system:kube-dns Role 绑定， 该 Role 具有访问 kubeapiserverDNS 相关 API 的权限； 1kubectl get clusterrolebindings system:kube-dns -o yaml 4. 创建 kube-dns12345cd /server/software/k8s/dnskubectl create -f .kubectl get pods -n kube-systemkubectl get deploy -n kube-systemkubectl get svc -n kube-system 5. 测试DNS1234567891011121314151617181920212223242526272829303132333435# 创建deploycat &gt;my-nginx.yaml&lt;&lt;EOFapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: my-nginxspec: replicas: 2 template: metadata: labels: run: my-nginx spec: containers: - name: my-nginx image: nginx:1.9 ports: - containerPort: 80EOFkubectl create -f my-nginx.yamlkubectl get podskubectl get deploykubectl expose deploy my-nginxkubectl get services --all-namespaces |grep my-nginx# 测试kubectl run dns-test --rm -ti --image busybox /bin/shnslookup kubernetescat /etc/resolv.confping my-nginxping kuberneteswget -q my-nginx -O -# 删除测试相关的podkubectl delete -f my-nginx.yaml 可以自行指定特殊的域名使用指定的dns服务器 ，也可以自行指定上游服务器。参考https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/ 安装dashboard插件1. 拉取镜像12docker pull jicki/kubernetes-dashboard-amd64:v1.6.1docker tag jicki/kubernetes-dashboard-amd64:v1.6.1 gcr.io/google_containers/kubernetes-dashboard-amd64:v1.6.1 2. 配置yaml文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192mkdir -pv /server/software/k8s/dashboardcd /server/software/k8s/dashboard# Controllercat &gt;dashboard-controller.yaml&lt;&lt;EOFapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: kubernetes-dashboard namespace: kube-system labels: k8s-app: kubernetes-dashboard kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilespec: selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: serviceAccountName: dashboard containers: - name: kubernetes-dashboard image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.6.1 resources: limits: cpu: 100m memory: 50Mi requests: cpu: 100m memory: 50Mi ports: - containerPort: 9090 livenessProbe: httpGet: path: / port: 9090 initialDelaySeconds: 30 timeoutSeconds: 30 tolerations: - key: "CriticalAddonsOnly" operator: "Exists"EOF# servicecat &gt;dashboard-service.yaml&lt;&lt;EOFapiVersion: v1kind: Servicemetadata: name: kubernetes-dashboard namespace: kube-system labels: k8s-app: kubernetes-dashboard kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilespec: type: NodePort selector: k8s-app: kubernetes-dashboard ports: - port: 80 targetPort: 9090EOF# rbaccat &gt;dashboard-rbac.yaml&lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: dashboard namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: dashboardsubjects: - kind: ServiceAccount name: dashboard namespace: kube-systemroleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.ioEOF 增加了一个dashboard-rbac.yaml文件，定义一个名为 dashboard 的 ServiceAccount，然后将它和 Cluster Role view 绑定。 指定端口类型为 NodePort，这样外界可以通过地址 nodeIP:nodePort 访问 dashboard； 3. 创建dashboard123456kubectl create -f .kubectl get pods -n kube-systemkubectl get deploy -n kube-system# 查看nodePortkubectl get services kubernetes-dashboard -n kube-system 4. 测试访问 暴露了 NodePort，可以使用 http://NodeIP:nodePort 地址访问 dashboard；http://192.168.12.212:31680 通过 kube-apiserver 访问 dashboard； 123456789101112131415# 获取地址信息kubectl cluster-info# 访问https://192.168.12.211:6443/api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy# 可能会遇到的问题https://github.com/opsnull/follow-me-install-kubernetes-cluster/issues/5# 导出证书openssl pkcs12 -export -in admin.pem -out admin.p12 -inkey admin-key.pem# 导入证书# 将生成的admin.p12证书导入的你的电脑，导出的时候记住你设置的密码，导入的时候还要用到。# 密码可以设置为空，直接回车即可。 通过 kubectl proxy 访问 dashboard 12345# 创建kubectl proxy --address='192.168.12.211' --port=8086 --accept-hosts='^*$'# 访问http://192.168.12.211:8086/ui 配置和安装 Heapster1. 拉取镜像12345678docker pull daocloud.io/will835559313/k8s:master-f7ff86edocker tag daocloud.io/will835559313/k8s:master-f7ff86e gcr.io/google_containers/heapster-amd64:v1.3.0docker pull daocloud.io/will835559313/k8s:master-545d95ddocker tag daocloud.io/will835559313/k8s:master-545d95d gcr.io/google_containers/heapster-grafana-amd64:v4.0.2docker pull daocloud.io/will835559313/k8s:master-f694b85docker tag daocloud.io/will835559313/k8s:master-f694b85 gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1 2. 配置yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315mkdir -pv /server/software/k8s/heapstercd /server/software/k8s/heapster# influxdb# ConfigMap的配置主要为了开启 admin UI 插件# 可以根据需求来删除此配置。cat &gt;influxdb.yaml&lt;&lt;EOF---apiVersion: v1kind: ConfigMapmetadata: name: influxdb-config namespace: kube-systemdata: config.toml: | reporting-disabled = true bind-address = ":8088" [meta] dir = "/data/meta" retention-autocreate = true logging-enabled = true [data] dir = "/data/data" wal-dir = "/data/wal" query-log-enabled = true cache-max-memory-size = 1073741824 cache-snapshot-memory-size = 26214400 cache-snapshot-write-cold-duration = "10m0s" compact-full-write-cold-duration = "4h0m0s" max-series-per-database = 1000000 max-values-per-tag = 100000 trace-logging-enabled = false [coordinator] write-timeout = "10s" max-concurrent-queries = 0 query-timeout = "0s" log-queries-after = "0s" max-select-point = 0 max-select-series = 0 max-select-buckets = 0 [retention] enabled = true check-interval = "30m0s" [admin] enabled = true bind-address = ":8083" https-enabled = false https-certificate = "/etc/ssl/influxdb.pem" [shard-precreation] enabled = true check-interval = "10m0s" advance-period = "30m0s" [monitor] store-enabled = true store-database = "_internal" store-interval = "10s" [subscriber] enabled = true http-timeout = "30s" insecure-skip-verify = false ca-certs = "" write-concurrency = 40 write-buffer-size = 1000 [http] enabled = true bind-address = ":8086" auth-enabled = false log-enabled = true write-tracing = false pprof-enabled = false https-enabled = false https-certificate = "/etc/ssl/influxdb.pem" https-private-key = "" max-row-limit = 10000 max-connection-limit = 0 shared-secret = "" realm = "InfluxDB" unix-socket-enabled = false bind-socket = "/var/run/influxdb.sock" [[graphite]] enabled = false bind-address = ":2003" database = "graphite" retention-policy = "" protocol = "tcp" batch-size = 5000 batch-pending = 10 batch-timeout = "1s" consistency-level = "one" separator = "." udp-read-buffer = 0 [[collectd]] enabled = false bind-address = ":25826" database = "collectd" retention-policy = "" batch-size = 5000 batch-pending = 10 batch-timeout = "10s" read-buffer = 0 typesdb = "/usr/share/collectd/types.db" [[opentsdb]] enabled = false bind-address = ":4242" database = "opentsdb" retention-policy = "" consistency-level = "one" tls-enabled = false certificate = "/etc/ssl/influxdb.pem" batch-size = 1000 batch-pending = 5 batch-timeout = "1s" log-point-errors = true [[udp]] enabled = false bind-address = ":8089" database = "udp" retention-policy = "" batch-size = 5000 batch-pending = 10 read-buffer = 0 batch-timeout = "1s" precision = "" [continuous_queries] log-enabled = true enabled = true run-interval = "1s"---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: monitoring-influxdb namespace: kube-systemspec: replicas: 1 template: metadata: labels: task: monitoring k8s-app: influxdb spec: containers: - name: influxdb image: gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1 volumeMounts: - mountPath: /data name: influxdb-storage - mountPath: /etc/ name: influxdb-config volumes: - name: influxdb-storage emptyDir: &#123;&#125; - name: influxdb-config configMap: name: influxdb-config---apiVersion: v1kind: Servicemetadata: labels: task: monitoring # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons) # If you are NOT using this as an addon, you should comment out this line. kubernetes.io/cluster-service: 'true' kubernetes.io/name: monitoring-influxdb name: monitoring-influxdb namespace: kube-systemspec: type: NodePort ports: - port: 8086 targetPort: 8086 name: http - port: 8083 targetPort: 8083 name: admin selector: k8s-app: influxdbEOF# grafanacat &gt;grafana.yaml&lt;&lt;EOF---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: monitoring-grafana namespace: kube-systemspec: replicas: 1 template: metadata: labels: task: monitoring k8s-app: grafana spec: containers: - name: grafana image: gcr.io/google_containers/heapster-grafana-amd64:v4.0.2 ports: - containerPort: 3000 protocol: TCP volumeMounts: - mountPath: /var name: grafana-storage env: - name: INFLUXDB_HOST value: monitoring-influxdb - name: GRAFANA_PORT value: "3000" # The following env variables are required to make Grafana accessible via # the kubernetes api-server proxy. On production clusters, we recommend # removing these env variables, setup auth for grafana, and expose the grafana # service using a LoadBalancer or a public IP. - name: GF_AUTH_BASIC_ENABLED value: "false" - name: GF_AUTH_ANONYMOUS_ENABLED value: "true" - name: GF_AUTH_ANONYMOUS_ORG_ROLE value: Admin - name: GF_SERVER_ROOT_URL # If you're only using the API Server proxy, set this value instead: value: /api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/ #value: / volumes: - name: grafana-storage emptyDir: &#123;&#125;---apiVersion: v1kind: Servicemetadata: labels: # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons) # If you are NOT using this as an addon, you should comment out this line. kubernetes.io/cluster-service: 'true' kubernetes.io/name: monitoring-grafana name: monitoring-grafana namespace: kube-systemspec: # In a production setup, we recommend accessing Grafana through an external Loadbalancer # or through a public IP. # type: LoadBalancer # You could also use NodePort to expose the service at a randomly-generated port ports: - port : 80 targetPort: 3000 selector: k8s-app: grafanaEOF# heapstercat &gt;heapster.yaml&lt;&lt;EOF---apiVersion: v1kind: ServiceAccountmetadata: name: heapster namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: heapstersubjects: - kind: ServiceAccount name: heapster namespace: kube-systemroleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io---apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: heapster namespace: kube-systemspec: replicas: 1 template: metadata: labels: task: monitoring k8s-app: heapster spec: serviceAccountName: heapster containers: - name: heapster image: gcr.io/google_containers/heapster-amd64:v1.3.0 imagePullPolicy: IfNotPresent command: - /heapster - --source=kubernetes:https://kubernetes.default - --sink=influxdb:http://monitoring-influxdb.kube-system.svc:8086---apiVersion: v1kind: Servicemetadata: labels: task: monitoring # For use as a Cluster add-on (https://github.com/kubernetes/kubernetes/tree/master/cluster/addons) # If you are NOT using this as an addon, you should comment out this line. kubernetes.io/cluster-service: 'true' kubernetes.io/name: Heapster name: heapster namespace: kube-systemspec: ports: - port: 80 targetPort: 8082 selector: k8s-app: heapsterEOF 3. 创建1234567kubectl create -f .kubectl get deploy -n kube-systemkubectl get pods -n kube-systemkubectl get svc -n kube-system# 访问dashboard查有无监控图表 4. 访问 grafana 通过 kube-apiserver 访问 12kubectl cluster-info# https://192.168.12.211:6443/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy 通过 kubectl proxy 访问 12kubectl proxy --address='192.168.12.211' --port=8086 --accept-hosts='^*$'# http://192.168.12.211:8086/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana 5. 访问 influxdb admin UI 通过NodePort 12kubectl get svc -n kube-system|grep influxdb# http://192.168.12.212:31083/ 通过api-server 1# http://192.168.12.211:8080/api/v1/proxy/namespaces/kube-system/services/monitoring-influxdb:8083/ 如果后续使用 kube-apiserver 或者 kubectl proxy 访问 grafana dashboard，则必须将 GF_SERVER_ROOT_URL 设置为/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/，否则后续访问grafana时访问时提示找不到http://192.168.12.211:8086/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana/api/dashboards/home 页面；heapster1.4版本会有问题，造成grafana无法成功启动，建议使用1.3版本 配置和安装 EFK1. 拉取镜像12345678docker pull daocloud.io/will835559313/k8s:master-6ccfe3ddocker tag daocloud.io/will835559313/k8s:master-6ccfe3d gcr.io/google_containers/kibana:v4.6.1-1docker pull daocloud.io/will835559313/k8s:master-425d70bdocker tag daocloud.io/will835559313/k8s:master-425d70b gcr.io/google_containers/fluentd-elasticsearch:1.22docker pull daocloud.io/will835559313/k8s:master-1e3ccdfdocker tag daocloud.io/will835559313/k8s:master-1e3ccdf gcr.io/google_containers/elasticsearch:v2.4.1-2 2. 修改yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219mkdir -pv /server/software/k8s/efkcd /server/software/k8s/efk# rbaccat &gt;efk-rbac.yaml&lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: efk namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: efksubjects: - kind: ServiceAccount name: efk namespace: kube-systemroleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.ioEOF# escat &gt;es.yaml&lt;&lt;EOFapiVersion: v1kind: ReplicationControllermetadata: name: elasticsearch-logging-v1 namespace: kube-system labels: k8s-app: elasticsearch-logging version: v1 kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilespec: replicas: 2 selector: k8s-app: elasticsearch-logging version: v1 template: metadata: labels: k8s-app: elasticsearch-logging version: v1 kubernetes.io/cluster-service: "true" spec: serviceAccountName: efk containers: - image: gcr.io/google_containers/elasticsearch:v2.4.1-2 name: elasticsearch-logging resources: # need more cpu upon initialization, therefore burstable class limits: cpu: 1000m requests: cpu: 100m ports: - containerPort: 9200 name: db protocol: TCP - containerPort: 9300 name: transport protocol: TCP volumeMounts: - name: es-persistent-storage mountPath: /data env: - name: "NAMESPACE" valueFrom: fieldRef: fieldPath: metadata.namespace volumes: - name: es-persistent-storage emptyDir: &#123;&#125;---apiVersion: v1kind: Servicemetadata: name: elasticsearch-logging namespace: kube-system labels: k8s-app: elasticsearch-logging kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: "Elasticsearch"spec: ports: - port: 9200 protocol: TCP targetPort: db selector: k8s-app: elasticsearch-loggingEOF# kibanacat &gt;kibana.yaml&lt;&lt;EOFapiVersion: extensions/v1beta1kind: Deploymentmetadata: name: kibana-logging namespace: kube-system labels: k8s-app: kibana-logging kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcilespec: replicas: 1 selector: matchLabels: k8s-app: kibana-logging template: metadata: labels: k8s-app: kibana-logging spec: serviceAccountName: efk containers: - name: kibana-logging image: gcr.io/google_containers/kibana:v4.6.1-1 resources: # keep request = limit to keep this container in guaranteed class limits: cpu: 100m requests: cpu: 100m env: - name: "ELASTICSEARCH_URL" value: "http://elasticsearch-logging:9200" - name: "KIBANA_BASE_URL" value: "/api/v1/proxy/namespaces/kube-system/services/kibana-logging" ports: - containerPort: 5601 name: ui protocol: TCP---apiVersion: v1kind: Servicemetadata: name: kibana-logging namespace: kube-system labels: k8s-app: kibana-logging kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: "Kibana"spec: ports: - port: 5601 protocol: TCP targetPort: ui selector: k8s-app: kibana-loggingEOF# flentdcat &gt;fluentd-es-ds.yaml&lt;&lt;EOFapiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: fluentd-es-v1.22 namespace: kube-system labels: k8s-app: fluentd-es kubernetes.io/cluster-service: "true" addonmanager.kubernetes.io/mode: Reconcile version: v1.22spec: template: metadata: labels: k8s-app: fluentd-es kubernetes.io/cluster-service: "true" version: v1.22 # This annotation ensures that fluentd does not get evicted if the node # supports critical pod annotation based priority scheme. # Note that this does not guarantee admission on the nodes (#40573). annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: serviceAccountName: efk containers: - name: fluentd-es image: gcr.io/google_containers/fluentd-elasticsearch:1.22 command: - '/bin/sh' - '-c' - '/usr/sbin/td-agent 2&gt;&amp;1 &gt;&gt; /var/log/fluentd.log' resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true nodeSelector: beta.kubernetes.io/fluentd-ds-ready: "true" tolerations: - key : "node.alpha.kubernetes.io/ismaster" effect: "NoSchedule" terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containersEOF 3. 给 Node 设置标签 定义 DaemonSet fluentd-es-v1.22 时设置了 nodeSelector beta.kubernetes.io/fluentd-ds-ready=true ，所以需要在期望运行 fluentd 的 Node 上设置该标签； 12kubectl get nodeskubectl label nodes 192.168.12.212 beta.kubernetes.io/fluentd-ds-ready=true 4. 创建12345678kubectl create -f .kubectl get deploy -n kube-systemkubectl get pods -n kube-systemkubectl get svc -n kube-system# kibana Pod 第一次启动时会用**较长时间(10-20分钟)**来优化和 Cache 状态页面，# 可以 tailf 该 Pod 的日志观察进度：kubectl logs -f kibana-logging-269483651-c2tl0 -n kube-system 5. 访问 通过 kube-apiserver 访问 12kubectl cluster-info# https://192.168.12.211:6443/api/v1/namespaces/kube-system/services/kibana-logging/proxy 通过 kubectl proxy 访问 123kubectl proxy --address='192.168.12.211' --port=8086 --accept-hosts='^*$'# http://192.168.12.211:8086/api/v1/proxy/namespaces/kube-system/services/kibana-logging 可能遇到的问题如果你在这里发现Create按钮是灰色的无法点击，且Time-filed name中没有选项，fluentd要读取/var/log/containers/目录下的log日志，这些日志是从/var/lib/docker/containers/${CONTAINER_ID}/${CONTAINER_ID}-json.log链接过来的，查看你的docker配置，—log-dirver需要设置为json-file格式，默认的可能是journald，参考docker logging。 安装配置 traefik ingress1. 简介 理解Ingress 简单的说，ingress就是从kubernetes集群外访问集群的入口，将用户的URL请求转发到不同的service上。Ingress相当于nginx、apache等负载均衡方向代理服务器，其中还包括规则定义，即URL的路由信息，路由信息得的刷新由Ingress controller来提供。 理解Ingress Controller Ingress Controller 实质上可以理解为是个监视器，Ingress Controller 通过不断地跟kubernetes API 打交道，实时的感知后端 service、pod 等变化，比如新增和减少pod，service 增加与减少等；当得到这些变化信息后，Ingress Controller 再结合下文的 Ingress 生成配置，然后更新反向代理负载均衡器，并刷新其配置，达到服务发现的作用。 2. 配置yaml文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140mkdir /server/software/k8s/traefikcd /server/software/k8s/traefik# rbaccat &gt;ingress-rbac.yaml&lt;&lt;EOFapiVersion: v1kind: ServiceAccountmetadata: name: ingress namespace: kube-system---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: ingresssubjects: - kind: ServiceAccount name: ingress namespace: kube-systemroleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.ioEOF# ingresscat &gt;ingress.yaml&lt;&lt;EOFapiVersion: extensions/v1beta1kind: Ingressmetadata: name: traefik-ingressspec: rules: - host: traefik.nginx.io http: paths: - path: / backend: serviceName: my-nginx servicePort: 80 - host: traefik.frontend.io http: paths: - path: / backend: serviceName: frontend servicePort: 80 - host: rolling-update-test.traefik.io http: paths: - path: / backend: serviceName: rolling-update-test servicePort: 9090 - host: k8s-app-monitor-agent.jimmysong.io http: paths: - path: / backend: serviceName: k8s-app-monitor-agent servicePort: 8080EOF# traefikcat &gt;traefik.yaml&lt;&lt;EOFapiVersion: extensions/v1beta1kind: DaemonSetmetadata: name: traefik-ingress-lb namespace: kube-system labels: k8s-app: traefik-ingress-lbspec: template: metadata: labels: k8s-app: traefik-ingress-lb name: traefik-ingress-lb spec: terminationGracePeriodSeconds: 60 hostNetwork: true restartPolicy: Always serviceAccountName: ingress containers: - image: traefik name: traefik-ingress-lb resources: limits: cpu: 200m memory: 30Mi requests: cpu: 100m memory: 20Mi ports: - name: http containerPort: 80 hostPort: 80 - name: admin containerPort: 8580 hostPort: 8580 args: - --web - --web.address=:8580 - --kubernetes nodeSelector: edgenode: "true"EOF# uicat &gt;ui.yaml&lt;&lt;EOFapiVersion: v1kind: Servicemetadata: name: traefik-web-ui namespace: kube-systemspec: selector: k8s-app: traefik-ingress-lb ports: - name: web port: 80 targetPort: 8580---apiVersion: extensions/v1beta1kind: Ingressmetadata: name: traefik-web-ui namespace: kube-systemspec: rules: - host: traefik-ui.local http: paths: - path: / backend: serviceName: traefik-web-ui servicePort: webEOF 3. 创建12345678# 设置 node 的 labelkubectl get nodeskubectl label nodes 192.168.12.212 edgenode=truekubectl create -f .kubectl get pods -n kube-systemkubectl get ds -n kube-systemkubectl get svc -n kube-system 4. 访问1234567# 管理页面http://192.168.12.212:8580/# 测试 nginx# 配置hosts# 192.168.12.212 traefik.nginx.iocurl traefik.nginx.io 参考文档 https://www.kubernetes.org.cn/1870.html https://github.com/rootsongjc/kubernetes-handbook https://github.com/opsnull/follow-me-install-kubernetes-cluster http://www.cnblogs.com/ericnie/p/6965091.html https://docs.traefik.io/user-guide/kubernetes/]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7安装kubernetes-v1.7安装配置calico网络组件]]></title>
    <url>%2Fposts%2F9%2F</url>
    <content type="text"><![CDATA[安装条件 kube-apiserver 必须开启参数 --runtime-config=extensions/v1beta1/networkpolicies=true v1.6以及以前的版本需要在apiserver开启 extensions/v1beta1/networkpolicies v1.7+版本Network Policy已经GA，API版本为 networking.k8s.io/v1 kubelet 配置使用cni网络插件 --network-plugin=cni kube-proxy 必须使用iptables模式 --proxy-mode=iptables，默认就是这个参数 kube-proxy 不能设置 --masquerade-all kubernetes &gt; v1.3.0 安装1. 配置rbac1234mkdir /server/software/k8s/calicocd /server/software/k8s/calicowget http://docs.projectcalico.org/v2.4/getting-started/kubernetes/installation/rbac.yamlkubectl apply -f rbac.yaml 2. 拉取镜像1234567docker pull calico/node:v2.4.0docker pull calico/cni:v1.10.0docker pull calico/kube-policy-controller:v0.7.0docker tag calico/node:v2.4.0 quay.io/calico/node:v2.4.0docker tag calico/cni:v1.10.0 quay.io/calico/cni:v1.10.0docker tag calico/kube-policy-controller:v0.7.0 quay.io/calico/kube-policy-controller:v0.7.0 3. 启动1234567891011121314151617181920212223242526272829303132333435363738# 下载配置文件wget http://docs.projectcalico.org/v2.4/getting-started/kubernetes/installation/hosted/calico.yaml# 修改etcd相关的配置etcd_endpoints: "https://192.168.12.211:2379,https://192.168.12.212:2379,https://192.168.12.213:2379"# 由于使用了tls需要配置如下位置etcd_ca: "/calico-secrets/etcd-ca"etcd_cert: "/calico-secrets/etcd-cert"etcd_key: "/calico-secrets/etcd-key"...etcd-key: xxxxxxxxxxetcd-cert: xxxxxxxxxxxetcd-ca: xxxxxxxxxxxx...# 上面的内容使用如下方式获取base64 /etc/kubernetes/ssl/kubernetes-key.pem | tr -d '\n'base64 /etc/kubernetes/ssl/kubernetes.pem | tr -d '\n'base64 /etc/kubernetes/ssl/ca.pem | tr -d '\n'# 如果pod不能正常上网，还可以指定网卡。# 在配置node的containers段的env配置如下环境变量- name: IP_AUTODETECTION_METHOD value: "IP_AUTODETECTION_METHOD=can-reach=www.baidu.com"# 或者配置如下形式- name: IP_AUTODETECTION_METHOD value: "IP_AUTODETECTION_METHOD=interface=eth0"# 启动kubectl apply -f calico.yaml# 查看kubectl get pods -n kube-system 4. 测试1234# 启动多个容器查看，看是否能ping通kubectl run calico-test-1 --rm -ti --image busybox /bin/shkubectl run calico-test-2 --rm -ti --image busybox /bin/shkubectl run calico-test-3 --rm -ti --image busybox /bin/sh Network policy 测试默认网络是全部连通的，POD之间可以随意访问。 1. 创建 nginx deploy123kubectl run nginx --image=nginx --replicas=2kubectl expose deployment nginx --port=80kubectl get svc,pod 2. 测试测试应该通过123# 10.254.132.113 为上面获取到的 nginx 的集群 IPkubectl run busybox --rm -ti --image=busybox /bin/shwget --spider --timeout=1 10.254.132.113 3. 限制连接限制连接，只有设置了label access: true才能访问到 nginx 服务1234567891011121314151617cat &gt;nginx-policy.yaml&lt;&lt;EOFkind: NetworkPolicyapiVersion: networking.k8s.io/v1metadata: name: access-nginxspec: podSelector: matchLabels: run: nginx ingress: - from: - podSelector: matchLabels: access: "true"EOFkubectl create -f nginx-policy.yaml 4. 测试测试应该不能通过12kubectl run busybox --rm -ti --image=busybox /bin/shwget --spider --timeout=1 10.254.132.113 5. 设置label测试测试应该通过12kubectl run busybox --rm -ti --labels="access=true" --image=busybox /bin/shwget --spider --timeout=1 10.254.132.113 6. 清理12kubectl delete deploy nginxkubectl delete svc nginx 配置参考文档http://docs.projectcalico.org/v2.4/getting-started/kubernetes/installation/hosted/index#configuration-options 参考文档 http://docs.projectcalico.org/v2.4/getting-started/kubernetes/installation/ http://docs.projectcalico.org/v2.4/getting-started/kubernetes/installation/hosted/hosted http://docs.projectcalico.org/v2.4/getting-started/kubernetes/installation/hosted/index#configuration-options http://docs.projectcalico.org/v2.4/reference/node/configuration https://zhuanlan.zhihu.com/p/27699958 https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
        <tag>calico</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7安装kubernetes-v1.7 master 高可用配置]]></title>
    <url>%2Fposts%2F8%2F</url>
    <content type="text"><![CDATA[本教程紧接 centos7安装kubernetes-v1.7 请参考。所有master节点均安装 kube-apiserver,kube-scheduler,kube-controller-manager组件。为了方便，本教程使用docker相关组件。 配置 haproxy1. 拉取镜像1docker pull haproxy:1.7.8-alpine 2. 配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758mkdir /etc/haproxycat &gt;/etc/haproxy/haproxy.cfg&lt;&lt;EOFglobal log 127.0.0.1 local0 info maxconn 50000 uid 99 gid 99 #daemon nbproc 1 pidfile haproxy.piddefaults mode http log 127.0.0.1 local0 info maxconn 50000 retries 3 timeout connect 5s timeout client 30s timeout server 30s timeout check 2slisten admin_stats mode http bind 0.0.0.0:1080 log 127.0.0.1 local0 err stats refresh 30s stats uri /haproxy-status stats realm Haproxy\ Statistics stats auth will:will stats hide-version stats admin if TRUEfrontend k8s-https bind 0.0.0.0:6443 mode tcp #maxconn 50000 default_backend k8s-httpsfrontend k8s-http bind 0.0.0.0:8080 mode tcp #maxconn 50000 default_backend k8s-httpbackend k8s-https mode tcp balance roundrobin server lab1 192.168.12.211:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server lab2 192.168.12.212:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server lab3 192.168.12.213:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3backend k8s-http mode tcp balance roundrobin server lab1 192.168.12.211:8080 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server lab2 192.168.12.212:8080 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3 server lab3 192.168.12.213:8080 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3EOF 3. 启动1234567docker run -d --name my-haproxy \-v /etc/haproxy:/usr/local/etc/haproxy:ro \-p 6445:6443 \-p 8090:8080 \-p 1080:1080 \--restart always \haproxy:1.7.8-alpine 4. 查看日志1docker logs my-haproxy 配置 keepalived1. 拉取镜像12#docker pull osixia/keepalived:1.3.5-1docker pull oberthur/docker-keepalived 2. 配置123# 本次使用的镜像不需要配置cat &gt;/etc/keepalived/keepalived.conf&lt;&lt;EOFEOF 3. 启动123456789101112131415161718lsmod | grep ip_vsmodprobe ip_vs# masterdocker run --net=host --cap-add=NET_ADMIN -e VIP=192.168.12.215 \-e VROUTERID=112 -e STATE=BACKUP -e INTERFACE=eth1 -e PRIORITY=100 \-e AUTHPASS=blah \--name keepalived0 \--restart always \-d oberthur/docker-keepalived# backupdocker run --net=host --cap-add=NET_ADMIN -e VIP=192.168.12.215 \-e VROUTERID=112 -e STATE=BACKUP -e INTERFACE=eth1 -e PRIORITY=100 \-e AUTHPASS=blah \--name keepalived0 \--restart always \-d oberthur/docker-keepalived 此时会配置 192.168.12.215 到其中一台机器 配置k8s的注意事项 把api的地址指向负载均衡的地址 192.168.12.215 在创建kubernetes.pem的时候要把这个 VIP 加入其中 测试关闭相应的组件查看日志。12journalctl -f -u kube-controller-managerjournalctl -f -u kube-scheduler]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7安装配置kubernetes-v1.7]]></title>
    <url>%2Fposts%2F7%2F</url>
    <content type="text"><![CDATA[环境介绍安装kubernetes-v1.7并使用TLS认证，请提前安装好docker环境。本教程使用docker-12.3安装时需要下载的文件都放在/server/software/k8s目录下 安装创建 kubernetes 各组件 TLS 加密通信的证书和秘钥1. 安装 CFSSL12345678910111213mkdir -pv /server/software/k8scd /server/software/k8swget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfomv cfssl_linux-amd64 /usr/local/bin/cfsslmv cfssljson_linux-amd64 /usr/local/bin/cfssljsonchmod +x /usr/local/bin/cfssl* 2. 创建 CA 生成配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243mkdir /root/sslcd /root/ssl#cfssl print-defaults config &gt; config.json#cfssl print-defaults csr &gt; csr.jsoncat &gt;ca-config.json&lt;&lt;EOF&#123; "signing": &#123; "default": &#123; "expiry": "8760h" &#125;, "profiles": &#123; "kubernetes": &#123; "usages": [ "signing", "key encipherment", "server auth", "client auth" ], "expiry": "8760h" &#125; &#125; &#125;&#125;EOFcat &gt;ca-csr.json&lt;&lt;EOF&#123; "CN": "kubernetes", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "System" &#125; ]&#125;EOF “CN”：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法； “O”：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)； 生成 CA 证书和私钥12cfssl gencert -initca ca-csr.json | cfssljson -bare cals ca* 3. 创建 kubernetes 证书 123456789101112131415161718192021222324252627282930313233343536# 配置cat &gt;kubernetes-csr.json&lt;&lt;EOF&#123; "CN": "kubernetes", "hosts": [ "127.0.0.1", "192.168.12.211", "192.168.12.212", "192.168.12.213", "10.254.0.1", "kubernetes", "kubernetes.default", "kubernetes.default.svc", "kubernetes.default.svc.cluster", "kubernetes.default.svc.cluster.local" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "System" &#125; ]&#125;EOF# 生成cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json \-profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetesls kubernetes* 如果 hosts 字段不为空则需要指定授权使用该证书的 IP 或域名列表，由于该证书后续被 etcd集群和 kubernetes master集群使用，所以上面分别指定了 etcd 集群、kubernetes master集群的主机 IP 和 kubernetes 服务的服务 IP（一般是kue-apiserver 指定的service-cluster-ip-range 网段的第一个IP，如 10.254.0.1。如果配置高可用，注意把负载均衡的VIP加入其中。 4. 创建 admin 证书12345678910111213141516171819202122232425# 配置cat &gt;admin-csr.json&lt;&lt;EOF&#123; "CN": "admin", "hosts": [], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "system:masters", "OU": "System" &#125; ]&#125;EOF# 生成cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json \-profile=kubernetes admin-csr.json | cfssljson -bare adminls admin* 后续 kube-apiserver 使用 RBAC 对客户端(如 kubelet、kube-proxy、Pod)请求进行授权； kube-apiserver 预定义了一些 RBAC 使用的 RoleBindings，如 cluster-admin 将Group system:masters 与 Rolecluster-admin 绑定，该 Role 授予了调用kube-apiserver的所有 API的权限； OU 指定该证书的 Group 为 system:masters，kubelet 使用该证书访问 kube-apiserver时 ，由于证书被 CA 签名，所以认证通过，同时由于证书用户组为经过预授权的 system:masters，所以被授予访问所有 API 的权限； 5. 创建 kube-proxy 证书12345678910111213141516171819202122232425# 配置cat &gt;kube-proxy-csr.json&lt;&lt;EOF&#123; "CN": "system:kube-proxy", "hosts": [], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "System" &#125; ]&#125;EOF# 生成cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json \-profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxyls kube-proxy* CN 指定该证书的 User 为 system:kube-proxy； kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-proxy与 Role system:node-proxier 绑定，该 Role 授予了调用 kube-apiserver Proxy相关 API 的权限； 6. 校验证书1cfssl-certinfo -cert kubernetes.pem 7. 分发证书1234567# 复制证书mkdir -p /etc/kubernetes/sslcp *.pem /etc/kubernetes/ssl# copy到其他机器ssh lab2 'mkdir -pv /etc/kubernetes'scp -r /etc/kubernetes/ssl lab2:/etc/kubernetes/ssl 创建 kubeconfig 文件 这一步骤的操作只需要在master节点上操作，然后把配置文件分发到其他node节点即可。 1. 安装 kubectl12345cd /server/software/k8swget https://storage.googleapis.com/kubernetes-release/release/v1.7.2/bin/linux/amd64/kubectlchmod +x kubectlmv kubectl /usr/local/binkubectl version 2. 创建 TLS Bootstrapping Token kubelet、kube-proxy 等 Node 机器上的进程与 Master 机器的 kube-apiserver 进程通信时需要认证和授权；kubernetes 1.4 开始支持由 kube-apiserver 为客户端生成 TLS 证书的TLS Bootstrapping 功能，这样就不需要为每个客户端生成证书了；该功能当前仅支持为 kubelet生成证书。 12345678export BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d ' ')cat &gt; token.csv &lt;&lt;EOF$&#123;BOOTSTRAP_TOKEN&#125;,kubelet-bootstrap,10001,"system:kubelet-bootstrap"EOF#将token.csv发到所有机器Master的 /etc/kubernetes/ 目录cp token.csv /etc/kubernetesscp token.csv lab2:/etc/kubernetes 3. 创建 kubelet bootstrapping kubeconfig 文件1234567891011121314151617181920212223cd /etc/kubernetesexport KUBE_APISERVER="https://192.168.12.211:6443"# 设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=bootstrap.kubeconfig# 设置客户端认证参数kubectl config set-credentials kubelet-bootstrap \ --token=$&#123;BOOTSTRAP_TOKEN&#125; \ --kubeconfig=bootstrap.kubeconfig# 设置上下文参数kubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=bootstrap.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=bootstrap.kubeconfig 4. 创建 kube-proxy kubeconfig 文件123456789101112131415161718192021222324export KUBE_APISERVER="https://192.168.12.211:6443"# 设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125; \ --kubeconfig=kube-proxy.kubeconfig# 设置客户端认证参数kubectl config set-credentials kube-proxy \ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \ --embed-certs=true \ --kubeconfig=kube-proxy.kubeconfig# 设置上下文参数kubectl config set-context default \ --cluster=kubernetes \ --user=kube-proxy \ --kubeconfig=kube-proxy.kubeconfig# 设置默认上下文kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 5. 分发 kubeconfig 文件12cp bootstrap.kubeconfig kube-proxy.kubeconfig /etc/kubernetes/scp bootstrap.kubeconfig kube-proxy.kubeconfig lab2:/etc/kubernetes/ 创建高可用 etcd 集群 本次实验使用lab1,lab2,lab3组成etcd集成，lab1为k8s-master节点，lab2,lab3为k8s-node节点。 在集群启动的时候尽量一起启动。因为ca里配置是使用的ip，所有在这里也全要使用ip地址。否则会无法识别ca，报错，无法成功创建集群。 1. 安装12345cd /server/software/k8swget https://github.com/coreos/etcd/releases/download/v3.2.4/etcd-v3.2.4-linux-amd64.tar.gztar xf etcd-v3.2.4-linux-amd64.tar.gzcd etcd-v3.2.4-linux-amd64 &amp;&amp; cp etcd etcdctl /usr/local/binetcdctl -v 2. 创建 etcd 的 systemd unit 文件1234567891011121314151617181920212223242526272829303132333435363738export ETCD_NAME=lab1export INTERNAL_IP=$(hostname -i)mkdir -pv /data/etcdcat &gt; etcd.service &lt;&lt;EOF[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.targetDocumentation=https://github.com/coreos[Service]Type=notifyWorkingDirectory=/data/etcdEnvironmentFile=-/etc/etcd/etcd.confExecStart=/usr/local/bin/etcd \\ --name $&#123;ETCD_NAME&#125; \\ --cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --peer-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls https://$&#123;INTERNAL_IP&#125;:2380 \\ --listen-peer-urls https://$&#123;INTERNAL_IP&#125;:2380 \\ --listen-client-urls https://$&#123;INTERNAL_IP&#125;:2379,https://127.0.0.1:2379 \\ --advertise-client-urls https://$&#123;INTERNAL_IP&#125;:2379 \\ --initial-cluster-token my-etcd-token \\ --initial-cluster lab1=https://192.168.12.211:2380,lab2=https://192.168.12.212:2380,lab3=https://192.168.12.213:2380 \\ --initial-cluster-state new \\ --data-dir=/data/etcdRestart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF 3. 启动12345mv etcd.service /etc/systemd/system/systemctl daemon-reloadsystemctl enable etcdsystemctl start etcdsystemctl status etcd 4. 测试12345etcdctl --endpoints "https://127.0.0.1:2379" \ --ca-file=/etc/kubernetes/ssl/ca.pem \ --cert-file=/etc/kubernetes/ssl/kubernetes.pem \ --key-file=/etc/kubernetes/ssl/kubernetes-key.pem \ cluster-health 配置 kubectl 命令行工具 只需要在需要连接api-server的client上配置。主要用来操作集群。 1234567891011121314151617export KUBE_APISERVER="https://192.168.12.211:6443"# 设置集群参数kubectl config set-cluster kubernetes \ --certificate-authority=/etc/kubernetes/ssl/ca.pem \ --embed-certs=true \ --server=$&#123;KUBE_APISERVER&#125;# 设置客户端认证参数kubectl config set-credentials admin \ --client-certificate=/etc/kubernetes/ssl/admin.pem \ --embed-certs=true \ --client-key=/etc/kubernetes/ssl/admin-key.pem# 设置上下文参数kubectl config set-context kubernetes \ --cluster=kubernetes \ --user=admin# 设置默认上下文kubectl config use-context kubernetes 生成的 kubeconfig 被保存到 ~/.kube/config 文件 部署高可用 kubernetes master 集群 kube-scheduler 、 kube-controller-manager 和 kube-apiserver 三者的功能紧密相关；要部署在同台服务器。 同时只能有一个 kube-scheduler 、 kube-controller-manager 进程处于工作状态，如果运行多个，则需要通过选举产生一个 leader； 多个master节点可以直接部署，多个master使用同一个etcd集成即可。然后把kube-apiserver通过负载均衡与其他组件通信。这样就组成了高可用 master 集群。 具体可以参考centos7安装kubernetes-v1.7 master 高可用配置教程 1. 下载安装1234567891011cd /server/software/k8swget https://storage.googleapis.com/kubernetes-release/release/v1.7.2/bin/linux/amd64/kube-apiserverwget https://storage.googleapis.com/kubernetes-release/release/v1.7.2/bin/linux/amd64/kube-controller-managerwget https://storage.googleapis.com/kubernetes-release/release/v1.7.2/bin/linux/amd64/kube-schedulerwget https://storage.googleapis.com/kubernetes-release/release/v1.7.2/bin/linux/amd64/kubectlwget https://storage.googleapis.com/kubernetes-release/release/v1.7.2/bin/linux/amd64/kubeletwget https://storage.googleapis.com/kubernetes-release/release/v1.7.2/bin/linux/amd64/kube-proxychmod +x kube-apiserver kube-controller-manager kubectl kubelet kube-proxy kube-scheduler\cp kube-apiserver kube-controller-manager kubectl kubelet kube-proxy kube-scheduler /usr/local/binscp kube-apiserver kube-controller-manager kubectl kubelet kube-proxy kube-scheduler lab2:/usr/local/bin 2. 配置和启动 kube-apiserver1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192cat &gt;/usr/lib/systemd/system/kube-apiserver.service&lt;&lt;EOF[Unit]Description=Kubernetes API ServiceDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.targetAfter=etcd.service[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/apiserverExecStart=/usr/local/bin/kube-apiserver \\ \$KUBE_LOGTOSTDERR \\ \$KUBE_LOG_LEVEL \\ \$KUBE_ETCD_SERVERS \\ \$KUBE_API_ADDRESS \\ \$KUBE_API_PORT \\ \$KUBELET_PORT \\ \$KUBE_ALLOW_PRIV \\ \$KUBE_SERVICE_ADDRESSES \\ \$KUBE_ADMISSION_CONTROL \\ \$KUBE_API_ARGSRestart=on-failureType=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF# 该配置文件同时被kube-apiserver、kube-controller-manager、# kube-scheduler、kubelet、kube-proxy使用。cat &gt;/etc/kubernetes/config&lt;&lt;EOF#### kubernetes system config## The following values are used to configure various aspects of all# kubernetes services, including## kube-apiserver.service# kube-controller-manager.service# kube-scheduler.service# kubelet.service# kube-proxy.service# logging to stderr means we get it in the systemd journalKUBE_LOGTOSTDERR="--logtostderr=true"# journal message level, 0 is debugKUBE_LOG_LEVEL="--v=0"# Should this cluster be allowed to run privileged docker containersKUBE_ALLOW_PRIV="--allow-privileged=true"# How the controller-manager, scheduler, and proxy find the apiserver#KUBE_MASTER="--master=http://192.168.12.211:8080"KUBE_MASTER="--master=http://192.168.12.211:8080"EOFcat &gt;/etc/kubernetes/apiserver&lt;&lt;EOF##### kubernetes system config#### The following values are used to configure the kube-apiserver##### The address on the local server to listen to.#KUBE_API_ADDRESS="--insecure-bind-address=192.168.12.211"KUBE_API_ADDRESS="--advertise-address=192.168.12.211 --bind-address=192.168.12.211 --insecure-bind-address=192.168.12.211"### The port on the local server to listen on.#KUBE_API_PORT="--port=8080"### Port minions listen on#KUBELET_PORT="--kubelet-port=10250"### Comma separated list of nodes in the etcd clusterKUBE_ETCD_SERVERS="--etcd-servers=https://192.168.12.211:2379,192.168.12.212:2379,192.168.12.213:2379"### Address range to use for servicesKUBE_SERVICE_ADDRESSES="--service-cluster-ip-range=10.254.0.0/16"### default admission control policiesKUBE_ADMISSION_CONTROL="--admission-control=ServiceAccount,NamespaceLifecycle,NamespaceExists,LimitRanger,ResourceQuota"### Add your own!KUBE_API_ARGS="--authorization-mode=RBAC --runtime-config=rbac.authorization.k8s.io/v1beta1 --kubelet-https=true --experimental-bootstrap-token-auth --token-auth-file=/etc/kubernetes/token.csv --service-node-port-range=30000-32767 --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem --client-ca-file=/etc/kubernetes/ssl/ca.pem --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem --etcd-cafile=/etc/kubernetes/ssl/ca.pem --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem --enable-swagger-ui=true --apiserver-count=3 --audit-log-maxage=30 --audit-log-maxbackup=3 --audit-log-maxsize=100 --audit-log-path=/var/lib/audit.log --event-ttl=1h"EOF# 启动systemctl daemon-reloadsystemctl enable kube-apiserversystemctl start kube-apiserversystemctl status kube-apiserver 2. 配置和启动 kube-controller-manager12345678910111213141516171819202122232425262728293031323334cat &gt;/usr/lib/systemd/system/kube-controller-manager.service&lt;&lt;EOFDescription=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/controller-managerExecStart=/usr/local/bin/kube-controller-manager \\ \$KUBE_LOGTOSTDERR \\ \$KUBE_LOG_LEVEL \\ \$KUBE_MASTER \\ \$KUBE_CONTROLLER_MANAGER_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOFcat &gt;/etc/kubernetes/controller-manager&lt;&lt;EOF#### The following values are used to configure the kubernetes controller-manager# defaults from config and apiserver should be adequate# Add your own!KUBE_CONTROLLER_MANAGER_ARGS="--address=127.0.0.1 --service-cluster-ip-range=10.254.0.0/16 --cluster-name=kubernetes --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem --root-ca-file=/etc/kubernetes/ssl/ca.pem --leader-elect=true"EOF# 启动systemctl daemon-reloadsystemctl enable kube-controller-managersystemctl start kube-controller-managersystemctl status kube-controller-manager 3. 配置和启动 kube-scheduler1234567891011121314151617181920212223242526272829303132333435cat &gt;/usr/lib/systemd/system/kube-scheduler.service&lt;&lt;EOF[Unit]Description=Kubernetes Scheduler PluginDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/schedulerExecStart=/usr/local/bin/kube-scheduler \\ \$KUBE_LOGTOSTDERR \\ \$KUBE_LOG_LEVEL \\ \$KUBE_MASTER \\ \$KUBE_SCHEDULER_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOFcat &gt;/etc/kubernetes/scheduler&lt;&lt;EOF#### kubernetes scheduler config# default config should be adequate# Add your own!KUBE_SCHEDULER_ARGS="--leader-elect=true --address=127.0.0.1"EOF# 启动systemctl daemon-reloadsystemctl enable kube-schedulersystemctl start kube-schedulersystemctl status kube-scheduler 4. 验证节点1kubectl get componentstatuses 以上3个组件在3个master节点安装运行就可以组成高可用。 部署kubernetes node节点1. 安装配置Flanneld 本教程使用手动配置的方法，也可以使用官方的yml文件直接使用k8s部署。https://github.com/coreos/flannel/blob/master/Documentation/kube-flannel.yml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546cd /server/software/k8swget https://github.com/coreos/flannel/releases/download/v0.8.0/flannel-v0.8.0-linux-amd64.tar.gztar xf flannel-v0.8.0-linux-amd64.tar.gzcp flanneld mk-docker-opts.sh /usr/local/bin/scp flanneld mk-docker-opts.sh lab2:/usr/local/bin/# 配置cat &gt;/usr/lib/systemd/system/flanneld.service&lt;&lt;EOF[Unit]Description=Flanneld overlay address etcd agentAfter=network.targetAfter=network-online.targetWants=network-online.targetAfter=etcd.serviceBefore=docker.service[Service]Type=notifyEnvironmentFile=/etc/sysconfig/flanneldEnvironmentFile=-/etc/sysconfig/docker-networkExecStart=/usr/local/bin/flanneld \$FLANNELD_ARGSExecStartPost=/usr/local/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/dockerRestart=on-failure[Install]WantedBy=multi-user.targetRequiredBy=docker.serviceEOFcat &gt;/etc/sysconfig/flanneld&lt;&lt;EOFFLANNELD_ARGS='--etcd-endpoints=https://192.168.12.211:2379,https://192.168.12.212:2379,https://192.168.12.213:2379 --etcd-cafile=/etc/kubernetes/ssl/ca.pem --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem --etcd-prefix=/k8s/network --iface=eth1'EOF# 启动# 如果你要使用 host-gw 模式，可以直接将vxlan改成 host-gw 即可。etcdctl --endpoints "https://127.0.0.1:2379" \--ca-file=/etc/kubernetes/ssl/ca.pem \--cert-file=/etc/kubernetes/ssl/kubernetes.pem \--key-file=/etc/kubernetes/ssl/kubernetes-key.pem \set /k8s/network/config '&#123;"Network":"10.1.0.0/16", "Backend": &#123;"Type": "vxlan"&#125;&#125;'#set /k8s/network/config '&#123;"Network":"10.1.0.0/16", "Backend": &#123;"Type": "host-gw"&#125;&#125;'systemctl daemon-reloadsystemctl enable flanneldsystemctl start flanneldsystemctl status flanneld 3. 重启docker1234567891011121314151617181920212223242526272829303132333435363738394041#修改docker配置mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker# 修改docker启动文件cat &gt;/usr/lib/systemd/system/docker.service&lt;&lt;EOF[Unit]Description=Docker Application Container EngineDocumentation=https://docs.docker.comAfter=network.target[Service]Type=notify# the default is not to use systemd for cgroups because the delegate issues still# exists and systemd currently does not support the cgroup feature set required# for containers run by dockerEnvironmentFile=-/run/flannel/dockerExecStart=/usr/bin/dockerd \$DOCKER_NETWORK_OPTIONS --registry-mirror=https://tfhzn46h.mirror.aliyuncs.comExecReload=/bin/kill -s HUP $MAINPID# Having non-zero Limit*s causes performance problems due to accounting overhead# in the kernel. We recommend using cgroups to do container-local accounting.LimitNOFILE=infinityLimitNPROC=infinityLimitCORE=infinity# Uncomment TasksMax if your systemd version supports it.# Only systemd 226 and above support this version.#TasksMax=infinityTimeoutStartSec=0# set delegate yes so that systemd does not reset the cgroups of docker containersDelegate=yes# kill only the docker process, not all processes in the cgroupKillMode=process[Install]WantedBy=multi-user.targetEOF# 重启systemctl daemon-reloadsystemctl restart dockersystemctl status dockerps -ef | grep docker 4. 安装配置kubelet kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper cluster 角色(role)， 然后 kubelet 才能有权限创建认证请求(certificate signing requests)： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990# --user=kubelet-bootstrap 是在 /etc/kubernetes/token.csv 文件中指定的用户名，同时也写入了/etc/kubernetes/bootstrap.kubeconfig 文件# 此步骤在master上操作cd /etc/kuberneteskubectl create clusterrolebinding kubelet-bootstrap \ --clusterrole=system:node-bootstrapper \ --user=kubelet-bootstrapcat &gt;/usr/lib/systemd/system/kubelet.service&lt;&lt;EOF[Unit]Description=Kubernetes Kubelet ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/data/kubeletEnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/kubeletExecStart=/usr/local/bin/kubelet \\ \$KUBE_LOGTOSTDERR \\ \$KUBE_LOG_LEVEL \\ \$KUBELET_API_SERVER \\ \$KUBELET_ADDRESS \\ \$KUBELET_PORT \\ \$KUBELET_HOSTNAME \\ \$KUBE_ALLOW_PRIV \\ \$KUBELET_POD_INFRA_CONTAINER \\ \$KUBELET_ARGSRestart=on-failure[Install]WantedBy=multi-user.targetEOFcat &gt;/etc/kubernetes/config&lt;&lt;EOF#### kubernetes system config## The following values are used to configure various aspects of all# kubernetes services, including## kube-apiserver.service# kube-controller-manager.service# kube-scheduler.service# kubelet.service# kube-proxy.service# logging to stderr means we get it in the systemd journalKUBE_LOGTOSTDERR="--logtostderr=true"# journal message level, 0 is debugKUBE_LOG_LEVEL="--v=0"# Should this cluster be allowed to run privileged docker containersKUBE_ALLOW_PRIV="--allow-privileged=true"# How the controller-manager, scheduler, and proxy find the apiserver#KUBE_MASTER="--master=http://192.168.12.211:8080"KUBE_MASTER="--master=http://192.168.12.211:8080"EOF# 注意修改相关ipcat &gt;/etc/kubernetes/kubelet&lt;&lt;EOF##### kubernetes kubelet (minion) config### The address for the info server to serve on (set to 0.0.0.0 or "" for all interfaces)KUBELET_ADDRESS="--address=192.168.12.212"### The port for the info server to serve on#KUBELET_PORT="--port=10250"### You may leave this blank to use the actual hostnameKUBELET_HOSTNAME="--hostname-override=192.168.12.212"### location of the api-serverKUBELET_API_SERVER="--api-servers=http://192.168.12.211:8080"### pod infrastructure container#KUBELET_POD_INFRA_CONTAINER="--pod-infra-container-image=sz-pg-oam-docker-hub-001.tendcloud.com/library/pod-infrastructure:rhel7"### Add your own!KUBELET_ARGS="--cgroup-driver=cgroupfs --cluster-dns=10.254.0.2 --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig --kubeconfig=/etc/kubernetes/kubelet.kubeconfig --require-kubeconfig --cert-dir=/etc/kubernetes/ssl --cluster-domain=cluster.local. --hairpin-mode promiscuous-bridge --serialize-image-pulls=false"EOF# 启动mkdir -pv /data/kubeletsystemctl daemon-reloadsystemctl enable kubeletsystemctl start kubeletsystemctl status kubelet 5. 通过 kublet 的 TLS 证书请求kubelet 首次启动时向 kube-apiserver 发送证书签名请求，必须通过后 kubernetes 系统才会将该 Node 加入到集群。123456789# 查看kubectl get csr# 通过kubectl certificate approve csr-2b308# 在node节点查看生成的文件ls -l /etc/kubernetes/kubelet.kubeconfigls -l /etc/kubernetes/ssl/kubelet* 6. 配置 kube-proxy123456789101112131415161718192021222324252627282930313233343536cat &gt;/usr/lib/systemd/system/kube-proxy.service&lt;&lt;EOF[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]EnvironmentFile=-/etc/kubernetes/configEnvironmentFile=-/etc/kubernetes/proxyExecStart=/usr/local/bin/kube-proxy \\ \$KUBE_LOGTOSTDERR \\ \$KUBE_LOG_LEVEL \\ \$KUBE_MASTER \\ \$KUBE_PROXY_ARGSRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOFcat &gt;/etc/kubernetes/proxy&lt;&lt;EOF#### kubernetes proxy config# default config should be adequate# Add your own!KUBE_PROXY_ARGS="--bind-address=192.168.12.212 --hostname-override=192.168.12.212 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig --cluster-cidr=10.254.0.0/16"EOF# 启动systemctl daemon-reloadsystemctl enable kube-proxysystemctl start kube-proxysystemctl status kube-proxy 7. 验证测试12345678910111213141516171819202122# 获取nodekubectl get nodes# 启动deploymentkubectl run nginx --replicas=2 --labels="run=load-balancer-example" --image=nginx:1.9 --port=80# 暴露服务kubectl expose deployment nginx --type=NodePort --name=example-service# 查看kubectl get podskubectl describe svc example-service# 访问curl "10.254.142.220:80"# 访问NodePorthttp://192.168.12.212:31075/# 删除kubectl delete svc example-servicekubectl delete deploy nginx 如果无法拉取镜像手动拉取docker pull pigletfly/pause-amd64:3.0docker tag pigletfly/pause-amd64:3.0 gcr.io/google_containers/pause-amd64:3.0 参考文档 https://www.kubernetes.org.cn/1870.html https://github.com/rootsongjc/kubernetes-handbook https://github.com/opsnull/follow-me-install-kubernetes-cluster]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang Web 框架 Echo 简单使用教程]]></title>
    <url>%2Fposts%2F6%2F</url>
    <content type="text"><![CDATA[简介Echo是一个高性能，灵活可扩展，极简的 go web 框架。支持多种格式的响应如：json、xml、html等。中间件众多，常用组件都能找到如：HTTPS、HTTP/2、WebSocket、JWT、Gzip、CORS、CSRF。天然支持RESTful API 开发，使用其做API开发异常的方便，同时支持使用模板，方便MVC模式的开发。 安装 Echo 如果没有翻墙，先配置如下的hosts 216.58.200.33 go.googlesource.com golang.org www.golang.org 12go get github.com/labstack/echogo get github.com/labstack/echo/middleware Hello World 测试写入如下内容到hello.go123456789101112131415package mainimport ( "net/http" "github.com/labstack/echo")func main() &#123; e := echo.New() e.GET("/", func(c echo.Context) error &#123; return c.String(http.StatusOK, "Hello, World!") &#125;) e.Logger.Fatal(e.Start(":1323"))&#125; 运行1go run hello.go 访问1curl -s http://127.0.0.1:1323/ JSON 格式输出响应内容写入如下内容到json.go123456789101112131415161718192021222324package mainimport ( "net/http" "github.com/labstack/echo")type User struct &#123; Name string `json:"name" xml:"name" form:"name" query:"name"` Email string `json:"email" xml:"email" form:"email" query:"email"`&#125;func main() &#123; e := echo.New() e.GET("/", func(c echo.Context) error &#123; u := new(User) u.Name = "will" u.Email = "will@will.com" return c.JSON(http.StatusOK, u) &#125;) e.Logger.Fatal(e.Start(":1323"))&#125; 运行1go run json.go 访问1curl -s http://127.0.0.1:1323/ 更多输出内容格式（xml、json、file、html、string、attachment、redirect等）参考如下官方文档https://echo.labstack.com/guide/response 路由12345678910111213// 路由e.POST("/users", saveUser)e.GET("/users/:id", getUser)e.PUT("/users/:id", updateUser)e.DELETE("/users/:id", deleteUser)// URI参数// e.GET("/users/:id", getUser)func getUser(c echo.Context) error &#123; // User ID from path `users/:id` id := c.Param("id") return c.String(http.StatusOK, id)&#125; URI查询参数/show?team=x-men&amp;member=wolverine1234567//e.GET("/show", show)func show(c echo.Context) error &#123; // Get team and member from the query string team := c.QueryParam("team") member := c.QueryParam("member") return c.String(http.StatusOK, "team:" + team + ", member:" + member)&#125; 从FORM接受参数POST /save12345678// e.POST("/save", save)func save(c echo.Context) error &#123; // Get name and email name := c.FormValue("name") email := c.FormValue("email") avatar, err := c.FormFile("avatar") return c.String(http.StatusOK, "name:" + name + ", email:" + email)&#125; REST API 示例写入如下内容到rest.go12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package mainimport ( "net/http" "github.com/labstack/echo")type User struct &#123; Id string `json:"id" form:"id"` Name string `json:"name" form:"name"` Email string `json:"email" form:"email"`&#125;func saveUser(c echo.Context) error &#123; u := new(User) u.Name = c.FormValue("name") u.Email = c.FormValue("email") u.Id = "1" return c.JSON(http.StatusCreated, u)&#125;func getUser(c echo.Context) error &#123; u := new(User) id := c.Param("id") u.Name = "will" u.Email = "will@will.com" u.Id = id return c.JSON(http.StatusOK, u)&#125;func updateUser(c echo.Context) error &#123; u := new(User) id := c.Param("id") u.Name = "will" u.Email = "will@will.com" u.Id = id return c.JSON(http.StatusOK, u)&#125;func deleteUser(c echo.Context) error &#123; u := new(User) id := c.Param("id") println(u) println(id) return c.NoContent(http.StatusNoContent)&#125;func main() &#123; e := echo.New() e.POST("/users", saveUser) e.GET("/users/:id", getUser) e.PUT("/users/:id", updateUser) e.DELETE("/users/:id", deleteUser) e.Logger.Fatal(e.Start(":1323"))&#125; 运行1go run rest.go 测试访问1234567891011# GETcurl -s -i http://127.0.0.1:1323/users/1# POSTcurl -s -i -F "name=Joe" -F "email=joe@labstack.com" http://127.0.0.1:1323/users# PUT curl -s -i -X PUT -F "name=Joe" -F "email=joe@labstack.com" http://127.0.0.1:1323/users/1# DELETE curl -s -i -X DELETE http://127.0.0.1:1323/users/1 官方参考文档https://echo.labstack.com/cookbook/crud 使用中间件1234567891011121314151617181920212223242526// 导入中间件模块import "github.com/labstack/echo/middleware"// 全局中间件e.Use(middleware.Logger())e.Use(middleware.Recover())// 组级中间件（只对以/admin开头的URI使用设置的中间件）g := e.Group("/admin")g.Use(middleware.BasicAuth(func(username, password string, c echo.Context) (error, bool) &#123; if username == "joe" &amp;&amp; password == "secret" &#123; return nil, true &#125; return nil, false&#125;))// 路由级中间件（只对单个URI使用设置的中间件）track := func(next echo.HandlerFunc) echo.HandlerFunc &#123; return func(c echo.Context) error &#123; println("request to /users") return next(c) &#125;&#125;e.GET("/users", func(c echo.Context) error &#123; return c.String(http.StatusOK, "/users")&#125;, track) 静态文件12345678910111213// 访问URI：/static/js/main.js 会寻找文件 assets/js/main.jse := echo.New()e.Static("/static", "assets")// 访问URI：/js/main.js 会寻找文件 assets/js/main.jse := echo.New()e.Static("/", "assets")// / ---&gt; public/index.htmle.File("/", "public/index.html")// /favicon.ico ---&gt; images/favicon.icoe.File("/favicon.ico", "images/favicon.ico") 使用模板创建模板文件夹1mkdir -pv public/views 添加模板文件public/views/hello.html1&#123;&#123;define &quot;hello&quot;&#125;&#125;Hello, &#123;&#123;.&#125;&#125;!&#123;&#123;end&#125;&#125; 写入如下内容到template.go123456789101112131415161718192021222324252627282930313233package mainimport ( "html/template" "io" "net/http" "github.com/labstack/echo")type Template struct &#123; templates *template.Template&#125;// 实现 echo.Renderer 接口func (t *Template) Render(w io.Writer, name string, data interface&#123;&#125;, c echo.Context) error &#123; return t.templates.ExecuteTemplate(w, name, data)&#125;func Hello(c echo.Context) error &#123; return c.Render(http.StatusOK, "hello", "Will")&#125;func main() &#123; t := &amp;Template&#123; templates: template.Must(template.ParseGlob("public/views/*.html")), &#125; e := echo.New() e.Renderer = t e.GET("/hello", Hello) e.Logger.Fatal(e.Start(":1323"))&#125; 使用测试组件写入如下内容到handler.go12345678910111213141516171819202122232425262728293031323334package handlerimport ( "net/http" "github.com/labstack/echo")type ( User struct &#123; Name string `json:"name" form:"name"` Email string `json:"email" form:"email"` &#125; handler struct &#123; db map[string]*User &#125;)func (h *handler) createUser(c echo.Context) error &#123; u := new(User) if err := c.Bind(u); err != nil &#123; return err &#125; return c.JSON(http.StatusCreated, u)&#125;func (h *handler) getUser(c echo.Context) error &#123; email := c.Param("email") user := h.db[email] if user == nil &#123; return echo.NewHTTPError(http.StatusNotFound, "user not found") &#125; return c.JSON(http.StatusOK, user)&#125; 写入如下内容到handler_test.go12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package handlerimport ( "net/http" "net/http/httptest" "strings" "testing" "github.com/labstack/echo" "github.com/stretchr/testify/assert")var ( mockDB = map[string]*User&#123; "jon@labstack.com": &amp;User&#123;"Jon Snow", "jon@labstack.com"&#125;, &#125; userJSON = `&#123;"name":"Jon Snow","email":"jon@labstack.com"&#125;`)func TestCreateUser(t *testing.T) &#123; // Setup e := echo.New() req := httptest.NewRequest(echo.POST, "/", strings.NewReader(userJSON)) req.Header.Set(echo.HeaderContentType, echo.MIMEApplicationJSON) rec := httptest.NewRecorder() c := e.NewContext(req, rec) h := &amp;handler&#123;mockDB&#125; // Assertions if assert.NoError(t, h.createUser(c)) &#123; assert.Equal(t, http.StatusCreated, rec.Code) assert.Equal(t, userJSON, rec.Body.String()) &#125;&#125;func TestGetUser(t *testing.T) &#123; // Setup e := echo.New() req := httptest.NewRequest(echo.GET, "/", nil) rec := httptest.NewRecorder() c := e.NewContext(req, rec) c.SetPath("/users/:email") c.SetParamNames("email") c.SetParamValues("jon@labstack.com") h := &amp;handler&#123;mockDB&#125; // Assertions if assert.NoError(t, h.getUser(c)) &#123; assert.Equal(t, http.StatusOK, rec.Code) assert.Equal(t, userJSON, rec.Body.String()) &#125;&#125; 运行测试12go get github.com/stretchr/testify/assertgo test 更多用法123456789101112131415// 使用 Formf := make(url.Values)f.Set("name", "Jon Snow")f.Set("email", "jon@labstack.com")req := httptest.NewRequest(echo.POST, "/", strings.NewReader(f.Encode()))req.Header.Set(echo.HandlerContentType, echo.MIMEApplicationForm)// 设置URI参数c.SetParamNames("id", "email")c.SetParamValues("1", "jon@labstack.com")// 设置查询参数q := make(url.Values)q.Set("email", "jon@labstack.com")req := http.NewRequest(echo.POST, "/?"+q.Encode(), nil) 使用cookie1234567891011121314151617181920212223242526272829// 创建cookiefunc writeCookie(c echo.Context) error &#123; cookie := new(http.Cookie) cookie.Name = "username" cookie.Value = "jon" cookie.Expires = time.Now().Add(24 * time.Hour) c.SetCookie(cookie) return c.String(http.StatusOK, "write a cookie")&#125;// 读取单个cookiefunc readCookie(c echo.Context) error &#123; cookie, err := c.Cookie("username") if err != nil &#123; return err &#125; fmt.Println(cookie.Name) fmt.Println(cookie.Value) return c.String(http.StatusOK, "read a cookie")&#125;// 读取所有cookiefunc readAllCookies(c echo.Context) error &#123; for _, cookie := range c.Cookies() &#123; fmt.Println(cookie.Name) fmt.Println(cookie.Value) &#125; return c.String(http.StatusOK, "read all cookie")&#125; 参考文档 https://echo.labstack.com/guide https://echo.labstack.com/cookbook https://echo.labstack.com/middleware]]></content>
      <categories>
        <category>Web开发</category>
        <category>golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
        <tag>Web开发</tag>
        <tag>Echo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go常用Web框架简介]]></title>
    <url>%2Fposts%2F5%2F</url>
    <content type="text"><![CDATA[常用web框架 echo 高性能，可扩展，极简 go Web 框架。 中间件多，性能高，REST支持，HTTPS支持，HTTP/2支持，WebSocket支持。 123456789101112131415package mainimport ( "net/http" "github.com/labstack/echo")func main() &#123; e := echo.New() e.GET("/", func(c echo.Context) error &#123; return c.String(http.StatusOK, "Hello, World!") &#125;) e.Logger.Fatal(e.Start(":1323"))&#125; gin 类martini，高性能 go Web 框架。 性能高，REST支持，HTTPS支持。 12345678910111213package mainimport "gopkg.in/gin-gonic/gin.v1"func main() &#123; r := gin.Default() r.GET("/ping", func(c *gin.Context) &#123; c.JSON(200, gin.H&#123; "message": "pong", &#125;) &#125;) r.Run() // listen and serve on 0.0.0.0:8080&#125; iris 最高性能 go Web 框架。 中间件多，性能高，REST支持。 1234567891011121314package mainimport ( "github.com/kataras/iris" "github.com/kataras/iris/context")func main() &#123; app := iris.New() app.Handle("GET", "/", func(ctx context.Context) &#123; ctx.HTML("&lt;b&gt; Hello world! &lt;/b&gt;") &#125;) app.Run(iris.Addr(":8080"))&#125; revel 高生产力，全功能 go Web 框架。 功能完整，MVC构架。 12revel new myapprevel run myapp martini Martini是一个强大为了编写模块化Web应用而生的GO语言框架。 1234567891011package mainimport "github.com/go-martini/martini"func main() &#123; m := martini.Classic() m.Get("/", func() string &#123; return "Hello world!" &#125;) m.Run()&#125; go-json-rest 易于构建RESTful JSON APIs的go Web框架。 中间件多，REST支持，HTTPS支持，WebSocket支持。 12345678910111213141516package mainimport ( "github.com/ant0ine/go-json-rest/rest" "log" "net/http")func main() &#123; api := rest.NewApi() api.Use(rest.DefaultDevStack...) api.SetApp(rest.AppSimple(func(w rest.ResponseWriter, r *rest.Request) &#123; w.WriteJson(map[string]string&#123;"Body": "Hello World!"&#125;) &#125;)) log.Fatal(http.ListenAndServe(":8080", api.MakeHandler()))&#125; utron 轻量级MVC的 go Web 框架。 macaron 高生产力 go Web 框架。 1234567891011package mainimport "gopkg.in/macaron.v1"func main() &#123; m := macaron.Classic() m.Get("/", func() string &#123; return "Hello world!" &#125;) m.Run()&#125; buffalo 快速的 go Web 开发框架。 12buffalo new cokebuffalo dev go-tigertonic json web 服务开发框架。 1234567891011121314151617type MyRequest struct &#123; ID string `json:"id"` Stuff interface&#123;&#125; `json:"stuff"`&#125;type MyResponse struct &#123; ID string `json:"id"` Stuff interface&#123;&#125; `json:"stuff"`&#125;func myHandler(u *url.URL, h http.Header, *MyRequest) (int, http.Header, *MyResponse, error) &#123; return http.StatusOK, nil, &amp;MyResponse&#123;"ID", "STUFF"&#125;, nil&#125;mux := tigertonic.NewTrieServeMux()mux.Handle("POST", "/stuff", tigertonic.Timed(tigertonic.Marshaled(myHandler), "myHandler", nil))tigertonic.NewServer(":8000", tigertonic.Logged(mux, nil)).ListenAndServe() faygo 快速方便的高性能 go Web 开发框架，特别是开发API。 12345678910111213141516171819202122232425262728293031323334353637package mainimport ( // "mime/multipart" "time" "github.com/henrylee2cn/faygo")type Index struct &#123; Id int `param:"&lt;in:path&gt; &lt;required&gt; &lt;desc:ID&gt; &lt;range: 0:10&gt;"` Title string `param:"&lt;in:query&gt; &lt;nonzero&gt;"` Paragraph []string `param:"&lt;in:query&gt; &lt;name:p&gt; &lt;len: 1:10&gt; &lt;regexp: ^[\\w]*$&gt;"` Cookie string `param:"&lt;in:cookie&gt; &lt;name:faygoID&gt;"` // Picture *multipart.FileHeader `param:"&lt;in:formData&gt; &lt;name:pic&gt; &lt;maxmb:30&gt;"`&#125;func (i *Index) Serve(ctx *faygo.Context) error &#123; if ctx.CookieParam("faygoID") == "" &#123; ctx.SetCookie("faygoID", time.Now().String()) &#125; return ctx.JSON(200, i)&#125;func main() &#123; app := faygo.New("myapp", "0.1") // Register the route in a chain style app.GET("/index/:id", new(Index)) // Register the route in a tree style // app.Route( // app.NewGET("/index/:id", new(Index)), // ) // Start the service faygo.Run()&#125; tango 微内核，可插拔，高性能 go Web 开发框架。 12345678910111213141516171819202122232425package mainimport ( "errors" "github.com/lunny/tango")type Action struct &#123; tango.JSON&#125;func (Action) Get() interface&#123;&#125; &#123; if true &#123; return map[string]string&#123; "say": "Hello tango!", &#125; &#125; return errors.New("something error")&#125;func main() &#123; t := tango.Classic() t.Get("/", new(Action)) t.Run()&#125; traffic 受 Sinatra 启发的多正则匹配的 go Web 框架。 123456789101112131415161718192021222324252627package mainimport ( "net/http" "github.com/pilu/traffic" "fmt")func rootHandler(w traffic.ResponseWriter, r *traffic.Request) &#123; fmt.Fprint(w, "Hello World\n")&#125;func pageHandler(w traffic.ResponseWriter, r *traffic.Request) &#123; params := r.URL.Query() fmt.Fprintf(w, "Category ID: %s\n", params.Get("category_id")) fmt.Fprintf(w, "Page ID: %s\n", params.Get("id"))&#125;func main() &#123; router := traffic.New() // Routes router.Get("/", rootHandler) router.Get("/categories/:category_id/pages/:id", pageHandler) router.Run()&#125; rest-layer 让创建 REST API 更简单的 go Web 框架。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142package mainimport ( "log" "net/http" "github.com/rs/rest-layer-mem" "github.com/rs/rest-layer/resource" "github.com/rs/rest-layer/rest" "github.com/rs/rest-layer/schema")var ( // Define a user resource schema user = schema.Schema&#123; Description: `The user object`, Fields: schema.Fields&#123; "id": &#123; Required: true, // When a field is read-only, on default values or hooks can // set their value. The client can't change it. ReadOnly: true, // This is a field hook called when a new user is created. // The schema.NewID hook is a provided hook to generate a // unique id when no value is provided. OnInit: schema.NewID, // The Filterable and Sortable allows usage of filter and sort // on this field in requests. Filterable: true, Sortable: true, Validator: &amp;schema.String&#123; Regexp: "^[0-9a-v]&#123;20&#125;$", &#125;, &#125;, "created": &#123; Required: true, ReadOnly: true, Filterable: true, Sortable: true, OnInit: schema.Now, Validator: &amp;schema.Time&#123;&#125;, &#125;, "updated": &#123; Required: true, ReadOnly: true, Filterable: true, Sortable: true, OnInit: schema.Now, // The OnUpdate hook is called when the item is edited. Here we use // provided Now hook which just return the current time. OnUpdate: schema.Now, Validator: &amp;schema.Time&#123;&#125;, &#125;, // Define a name field as required with a string validator "name": &#123; Required: true, Filterable: true, Validator: &amp;schema.String&#123; MaxLen: 150, &#125;, &#125;, &#125;, &#125; // Define a post resource schema post = schema.Schema&#123; Description: `Represents a blog post`, Fields: schema.Fields&#123; // schema.*Field are shortcuts for common fields // (identical to users' same fields) "id": schema.IDField, "created": schema.CreatedField, "updated": schema.UpdatedField, // Define a user field which references the user owning the post. // See bellow, the content of this field is enforced by the fact // that posts is a sub-resource of users. "user": &#123; Required: true, Filterable: true, Validator: &amp;schema.Reference&#123; Path: "users", &#125;, &#125;, "published": &#123; Required: true, Filterable: true, Default: false, Validator: &amp;schema.Bool&#123;&#125;, &#125;, "title": &#123; Required: true, Validator: &amp;schema.String&#123; MaxLen: 150, &#125;, &#125;, "body": &#123; // Dependency defines that body field can't be changed if // the published field is not "false". Dependency: schema.Q(`&#123;"published": false&#125;`), Validator: &amp;schema.String&#123; MaxLen: 100000, &#125;, &#125;, &#125;, &#125;)func main() &#123; // Create a REST API resource index index := resource.NewIndex() // Add a resource on /users[/:user_id] users := index.Bind("users", user, mem.NewHandler(), resource.Conf&#123; // We allow all REST methods // (rest.ReadWrite is a shortcut for []resource.Mode&#123;resource.Create, // resource.Read, resource.Update, resource.Delete, resource,List&#125;) AllowedModes: resource.ReadWrite, &#125;) // Bind a sub resource on /users/:user_id/posts[/:post_id] // and reference the user on each post using the "user" field of the posts resource. users.Bind("posts", "user", post, mem.NewHandler(), resource.Conf&#123; // Posts can only be read, created and deleted, not updated AllowedModes: []resource.Mode&#123;resource.Read, resource.List, resource.Create, resource.Delete&#125;, &#125;) // Create API HTTP handler for the resource graph api, err := rest.NewHandler(index) if err != nil &#123; log.Fatalf("Invalid API configuration: %s", err) &#125; // Bind the API under /api/ path http.Handle("/api/", http.StripPrefix("/api/", api)) // Serve it log.Print("Serving API on http://localhost:8080") if err := http.ListenAndServe(":8080", nil); err != nil &#123; log.Fatal(err) &#125;&#125; gongular 让开发 API 简单的 go Web 框架。 123456789101112type WelcomeMessage struct &#123; Message string Date time.Time&#125;g := gongular.NewRouter()g.GET("/", func(c *gongular.Context) WelcomeMessage &#123; return WelcomeMessage&#123; Message: "Hello, you are coming from: " + c.Request().RemoteAddr, Date: time.Now(), &#125;&#125;) lessgo 简单、稳定、高效、灵活的 go Web 开发框架。 12345678910111213141516171819import ( "github.com/henrylee2cn/lessgo" "github.com/henrylee2cn/lessgoext/swagger" _ "github.com/henrylee2cn/lessgoext/dbservice/xorm" // _ "github.com/henrylee2cn/lessgoext/dbservice/gorm" _ "github.com/henrylee2cn/lessgo_demo/middleware" _ "github.com/henrylee2cn/lessgo_demo/router")func main() &#123; // 开启自动api文档，通过config/apidoc_allow.myconfig进行配置 swagger.Reg() // 指定根目录URL lessgo.SetHome("/home") // 开启网络服务 lessgo.Run()&#125; neo 极其简单，快速，微内核的 go Web 框架。 123456789101112131415package mainimport ( "github.com/ivpusic/neo")func main() &#123; app := neo.App() app.Get("/", func(ctx *neo.Ctx) (int, error) &#123; return 200, ctx.Res.Text("I am Neo Programmer") &#125;) app.Start()&#125; gondola 快速开发 go Web 框架。 12345678910111213141516package mainimport ( "gnd.la/app" "gnd.la/config")var ( App *app.App)func init() &#123; config.MustParse() App = app.New() App.HandleNamed("^/$", MainHandler, "main")&#125; golf 快速简单高性能轻量的 go Web 开发框架。 123456789101112131415161718package mainimport "github.com/dinever/golf"func mainHandler(ctx *golf.Context) &#123; ctx.Send("Hello World!")&#125;func pageHandler(ctx *golf.Context) &#123; ctx.Send("Page: " + ctx.Param("page"))&#125;func main() &#123; app := golf.New() app.Get("/", mainHandler) app.Get("/p/:page/", pageHandler) app.Run(":9000")&#125; go-relax 专为 RESTful API 开发的 go Web 框架。 123456789101112131415161718package mainimport ( "github.com/codehack/go-relax")type Hello stringfunc (h *Hello) Index(ctx *relax.Context) &#123; ctx.Respond(h)&#125;func main() &#123; h := Hello("hello world!") svc := relax.NewService("http://api.company.com/") svc.Resource(&amp;h) svc.Run()&#125; gem 高性能，易用，REST支持，HTTP/2支持的 go Web 框架。 123456789101112131415161718192021222324package mainimport ( "log" "github.com/go-gem/gem")func index(ctx *gem.Context) &#123; ctx.HTML(200, "hello world")&#125;func main() &#123; // Create server. srv := gem.New(":8080") // Create router. router := gem.NewRouter() // Register handler router.GET("/", index) // Start server. log.Println(srv.ListenAndServe(router.Handler()))&#125; goat 极简JSON API go Web 框架，REST支持。 123456789101112131415161718192021package mainimport ( "net/http" "github.com/bahlo/goat")func helloHandler(w http.ResponseWriter, r *http.Request, p goat.Params) &#123; goat.WriteJSON(w, map[string]string&#123; "hello": p["name"], &#125;)&#125;func main() &#123; r := goat.New() r.Get("/hello/:name", "hello_url", helloHandler) r.Run(":8080")&#125; rex 现在化 go Web 开发框架。 12345678910111213141516package mainimport ( "io" "net/http" "github.com/goanywhere/rex")func main() &#123; app := rex.New() app.Get("/", func(w http.ResponseWriter, r *http.Request) &#123; io.WriteString(w, "Hello World") &#125;) app.Run()&#125; air 一个理想的 RESTful API go Web 开发框架。 12345678910111213package mainimport "github.com/sheng/air"func main() &#123; a := air.New() a.GET("/", homeHandler) a.Serve()&#125;func homeHandler(c *air.Context) error &#123; return c.String("Hello, 世界")&#125; yarf 又一个 高性能 REST 开发框架。 1234567891011121314151617181920212223242526package mainimport ( "github.com/yarf-framework/yarf")// Define a simple resourcetype Hello struct &#123; yarf.Resource&#125;// Implement the GET methodfunc (h *Hello) Get(c *yarf.Context) error &#123; c.Render("Hello world!") return nil&#125;// Run app server on http://localhost:8080func main() &#123; y := yarf.New() y.Add("/", new(Hello)) y.Start(":8080")&#125; 适合开发 REST API 的框架 github收藏较多的框架 echo gin iris go-json-rest martini macaron 小众但可能好用的框架 tango neo go-relax gem goat air yarf faygo traffic rest-layer gongular lessgo go-tigertonic]]></content>
      <categories>
        <category>Web开发</category>
        <category>golang</category>
      </categories>
      <tags>
        <tag>golang</tag>
        <tag>Web开发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown语法总结]]></title>
    <url>%2Fposts%2F4%2F</url>
    <content type="text"><![CDATA[简介Markdown 的目标是实现「易读易写」。Markdown 的语法全由一些符号所组成，这些符号经过精挑细选，其作用一目了然。比如：在文字两旁加上星号，看起来就像*强调*。Markdown的列表看起来，就是列表。Markdown 的区块引用看起来就真的像是引用一段文字。并且兼容 HTML 语法标签，可以在markdown文件里直接使用HTML标签如: &lt;div&gt;、&lt;table&gt;、&lt;pre&gt;、&lt;p&gt;、&lt;span&gt;、&lt;cite&gt;、&lt;del&gt;。并且现在有很多工具可以把markdown文件转换为pdf、html等格式，非常便于阅读，分享。 基本语法区块元素段落和换行一个 Markdown 段落是由一个或多个连续的文本行组成，它的前后要有一个以上的空行（空行的定义是显示上看起来像是空的，便会被视为空行。比方说，若某一行只包含空格和制表符，则该行也会被视为空行）。普通段落不该用空格或制表符来缩进。 如果你确实想要依赖 Markdown 来插入 &lt;br/&gt; 标签的话，在插入处先按入两个以上的空格然后回车。 标题在行首插入 1 到 6 个 # ，对应到标题 1 到 6 阶，例如：12345# 这是 H1 ### 这是 H2 ##### 这是 H3 ###### 区块引用Markdown 文件中建立一个区块引用，那会看起来像是你自己先断好行，然后在每行的最前面加上 &gt; ：123&gt; 这是一个引用&gt; 这是上一个引用的接着部分&gt; 这是这个引用的最后部分 这是一个引用这是上一个引用的接着部分这是这个引用的最后部分 Markdown 也允许你偷懒只在整个段落的第一行最前面加上 &gt; ：123&gt; 这是一个引用这是上一个引用的接着部分这是这个引用的最后部分 这是一个引用这是上一个引用的接着部分这是这个引用的最后部分 区块引用可以嵌套（例如：引用内的引用），只要根据层次加上不同数量的 &gt; ：123&gt; 这是一个引用&gt; &gt; 这是一个引用里的引用&gt; 这是这个引用的最后部分 这是一个引用 这是一个引用里的引用这是这个引用的最后部分 引用的区块内也可以使用其他的 Markdown 语法，包括标题、列表、代码区块等：1234&gt; #### 这是一个标题&gt; - 这是一个引用&gt; - 这是上一个引用的接着部分&gt; - 这是这个引用的最后部分 这是一个标题 这是一个引用 这是上一个引用的接着部分 这是这个引用的最后部分 列表Markdown 支持有序列表和无序列表。无序列表使用星号、加号或是减号作为列表标记：123456789101112131415* Red* Green* Blue+ Red+ Green+ Blue- Red- Green- Blue1. Bird2. McHale3. Parish Red Green Blue Red Green Blue Red Green Blue Bird McHale Parish 内容用固定的缩进整理：12345* Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus.* Donec sit amet nisl. Aliquam semper ipsum sit amet velit. Suspendisse id sem consectetuer libero luctus adipiscing Lorem ipsum dolor sit amet, consectetuer adipiscing elit.Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi,viverra nec, fringilla in, laoreet vitae, risus. Donec sit amet nisl. Aliquam semper ipsum sit amet velit.Suspendisse id sem consectetuer libero luctus adipiscing 偷懒的做法12345* Lorem ipsum dolor sit amet, consectetuer adipiscing elit.Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi,viverra nec, fringilla in, laoreet vitae, risus.* Donec sit amet nisl. Aliquam semper ipsum sit amet velit.Suspendisse id sem consectetuer libero luctus adipiscing. Lorem ipsum dolor sit amet, consectetuer adipiscing elit.Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi,viverra nec, fringilla in, laoreet vitae, risus. Donec sit amet nisl. Aliquam semper ipsum sit amet velit.Suspendisse id sem consectetuer libero luctus adipiscing. 列表项目可以包含多个段落，每个项目下的段落都必须缩进 4 个空格或是 1 个制表符：123456789101112131415161718191. This is a list item with two paragraphs. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aliquam hendrerit mi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreet vitae, risus. Donec sit amet nisl. Aliquam semper ipsum sit amet velit.2. Suspendisse id sem consectetuer libero luctus adipiscing.* This is a list item with two paragraphs. This is the second paragraph in the list item. You&apos;reonly required to indent the first line. Lorem ipsum dolorsit amet, consectetuer adipiscing elit.* Another item in the same list. This is a list item with two paragraphs. Lorem ipsum dolorsit amet, consectetuer adipiscing elit. Aliquam hendreritmi posuere lectus. Vestibulum enim wisi, viverra nec, fringilla in, laoreetvitae, risus. Donec sit amet nisl. Aliquam semper ipsumsit amet velit. Suspendisse id sem consectetuer libero luctus adipiscing. This is a list item with two paragraphs. This is the second paragraph in the list item. You’reonly required to indent the first line. Lorem ipsum dolorsit amet, consectetuer adipiscing elit. Another item in the same list. 如果要在列表项目内放进引用，那 &gt; 就需要缩进：1234* A list item with a blockquote: &gt; This is a blockquote &gt; inside a list item. A list item with a blockquote: This is a blockquoteinside a list item. 如果要放代码区块的话，该区块就需要缩进两次，也就是 8 个空格或是 2 个制表符：123* 一列表项包含一个列表区块： &lt;代码写在这&gt; 一列表项包含一个列表区块： &lt;代码写在这&gt; 代码区块要在 Markdown 中建立代码区块很简单，只要简单地缩进 4 个空格或是 1 个制表符就可以，例如，下面的输入：123这是一个普通段落： 这是一个代码区块。 这是一个普通段落： 这是一个代码区块。 代码也可以使用3个连续的反引号，后面还可以指明代码语言（部分编辑器支持，非官方语法）: 1echo hello 1echo hello 表格支持12345| 项目 | 价格 | 数量 || -------- | -----: | :----: || 计算机 | \$1600 | 5 || 手机 | \$12 | 12 || 管线 | \$1 | 234 | 项目 价格 数量 计算机 \$1600 5 手机 \$12 12 管线 \$1 234 分隔线你可以在一行中用三个以上的星号、减号、底线来建立一个分隔线，行内不能有其他东西。你也可以在星号或是减号中间插入空格。下面每种写法都可以建立分隔线：123456789* * *********- - ---------------------------------------- 区段元素链接要建立一个行内式的链接，只要在方块括号后面紧接着圆括号并插入网址链接即可，如果你还想要加上链接的 title 文字（鼠标放在链接上时显示的文件），只要在网址后面，用双引号把 title文字包起来即可，例如：123This is [an example](http://example.com/ &quot;Title&quot;) inline link.[This link](http://example.net/) has no title attribute. This is an example inline link. This link has no title attribute. 一个参考式链接的范例：123456I get 10 times more traffic from [Google] [1] than from[Yahoo] [2] or [MSN] [3]. [1]: http://google.com/ &quot;Google&quot; [2]: http://search.yahoo.com/ &quot;Yahoo Search&quot; [3]: http://search.msn.com/ &quot;MSN Search&quot; I get 10 times more traffic from Google than fromYahoo or MSN. 改成用链接名称的方式写：123456I get 10 times more traffic from [Google][] than from[Yahoo][] or [MSN][]. [google]: http://google.com/ &quot;Google&quot; [yahoo]: http://search.yahoo.com/ &quot;Yahoo Search&quot; [msn]: http://search.msn.com/ &quot;MSN Search&quot; I get 10 times more traffic from Google than fromYahoo or MSN. 强调Markdown 使用星号（）和底线（_）作为标记强调字词的符号，被 或 _ 包围的字词会被转成用 &lt;em&gt; 斜体标签包围，用两个 * 或 _ 包起来的话，则会被转成 &lt;strong&gt;加粗，例如：1234567*single asterisks*_single underscores_**double asterisks**__double underscores__ single asterisks single underscores double asterisks double underscores 如果你的 * 和 _ 两边都有空白的话，它们就只会被当成普通的符号。 如果要在文字前后直接插入普通的星号或底线，你可以用反斜线：1\*this text is surrounded by literal asterisks\* *this text is surrounded by literal asterisks* 部分编辑器支持删除线 ~~这是一段错误的文本。~~ 这是一段错误的文本。 代码如果要标记一小段行内代码，你可以用反引号把它包起来（`），例如：1Use the `printf()` function. Use the printf() function. 如果要在代码区段内插入反引号，你可以用多个反引号来开启和结束代码区段：1``There is a literal backtick (`) here.`` 代码区段的起始和结束端都可以放入一个空白，起始端后面一个，结束端前面一个，这样你就可以在区段的一开始就插入反引号：123A single backtick in a code span: `` ` ``A backtick-delimited string in a code span: `` `foo` `` A single backtick in a code span: ` A backtick-delimited string in a code span: `foo` 图片Markdown 使用一种和链接很相似的语法来标记图片，同样也允许两种样式： 行内式和参考式。 行内式的图片语法看起来像是：123![Alt text](http://ojz1mcltu.bkt.clouddn.com/animals-august2015.jpg)![Alt text](http://ojz1mcltu.bkt.clouddn.com/animals-august2015.jpg &quot;docker stack&quot;) 参考式的图片语法则长得像这样：12![Alt text][id][id]: http://ojz1mcltu.bkt.clouddn.com/animals-august2015.jpg &quot;docker stack&quot; ![Alt text][id][id]: http://ojz1mcltu.bkt.clouddn.com/animals-august2015.jpg “docker stack” Markdown 还没有办法指定图片的宽高，如果你需要的话，你可以使用普通的 &lt;img&gt; 标签。 其它自动链接Markdown 支持以比较简短的自动链接形式来处理网址和电子邮件信箱，只要是用方括号包起来，Markdown 就会自动把它转成链接。一般网址的链接文字就和链接地址一样，例如：12&lt;http://example.com/&gt;&lt;address@example.com&gt; http://example.com/&#x61;&#100;&#100;&#x72;&#x65;&#115;&#x73;&#x40;&#101;&#120;&#97;&#x6d;&#x70;&#x6c;&#101;&#x2e;&#99;&#111;&#109; 反斜杠Markdown 可以利用反斜杠来插入一些在语法中有其它意义的符号，例如：如果你想要用星号加在文字旁边的方式来做出强调效果（但不用 标签），你可以在星号的前面加上反斜杠：1\*literal asterisks\* Markdown 支持以下这些符号前面加上反斜杠来帮助插入普通的符号： \ 反斜线 ` 反引号 * 星号 _ 底线 {} 花括号 [] 方括号 () 括弧 # 井字号 + 加号 - 减号 . 英文句点 ! 惊叹号 待办事宜 Todo 列表使用带有[ ]或[x]（未完成或已完成）项的列表语法撰写一个待办事宜列表，并且支持子列表嵌套以及混用Markdown语法，此语法非标准语法，不是所有markdown编辑器都支持。 - [ ] **Cmd Markdown 开发** - [ ] 改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率 - [ ] 支持以 PDF 格式导出文稿 - [x] 新增Todo列表功能 [语法参考](https://github.com/blog/1375-task-lists-in-gfm-issues-pulls-comments) - [x] 改进 LaTex 功能 - [x] 修复 LaTex 公式渲染问题 - [x] 新增 LaTex 公式编号功能 [ ] Cmd Markdown 开发 [ ] 改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率 [ ] 支持以 PDF 格式导出文稿 [x] 新增Todo列表功能 语法参考 [x] 改进 LaTex 功能 [x] 修复 LaTex 公式渲染问题 [x] 新增 LaTex 公式编号功能 语法参考 参考文档 http://www.appinn.com/markdown/ https://www.zybuluo.com/mdeditor?url=https%3A%2F%2Fwww.zybuluo.com%2Fstatic%2Feditor%2Fmd-help.markdown]]></content>
      <categories>
        <category>文档相关</category>
      </categories>
      <tags>
        <tag>doc</tag>
        <tag>markdown</tag>
        <tag>pdf</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[REST接口设计规范总结]]></title>
    <url>%2Fposts%2F3%2F</url>
    <content type="text"><![CDATA[简介Representational State Transfer 简称 REST 描述了一个架构样式的网络系统。REST 指的是一组架构约束条件和原则。满足这些约束条件和原则的应用程序或设计就是 RESTful。 概念: 资源（Resources） REST是”表现层状态转化”，其实它省略了主语。”表现层”其实指的是”资源”的”表现层”。那么什么是资源呢？就是我们平常上网访问的一张图片、一个文档、一个视频等。这些资源我们通过URI来定位，也就是一个URI表示一个资源。 表现层（Representation）资源是做一个具体的实体信息，他可以有多种的展现方式。而把实体展现出来就是表现层，例如一个txt文本信息，他可以输出成html、json、xml等格式，一个图片他可以jpg、png等方式展现，这个就是表现层的意思。URI确定一个资源，但是如何确定它的具体表现形式呢？应该在HTTP请求的头信息中用Accept和Content-Type字段指定，这两个字段才是对”表现层”的描述。 状态转化（State Transfer）访问一个网站，就代表了客户端和服务器的一个互动过程。在这个过程中，肯定涉及到数据和状态的变化。而HTTP协议是无状态的，那么这些状态肯定保存在服务器端，所以如果客户端想要通知服务器端改变数据和状态的变化，肯定要通过某种方式来通知它。 URI格式规范 URI中尽量使用连字符”-“代替下划线”_”的使用 URI中统一使用小写字母 URI中不要包含文件(脚本)的扩展名 资源的原型 文档(Document) 123456文档是资源的单一表现形式，可以理解为一个对象，或者数据库中的一条记录。在请求文档时，要么返回文档对应的数据，要么会返回一个指向另外一个资源(文档)的链接。以下是几个基于文档定义的URI例子：https://api.example.com/users/willhttps://api.example.com/posts/1https://api.example.com/posts/1/comments/1 集合(Collection) 1234集合可以理解为是资源的一个容器(目录)，我们可以向里面添加资源(文档)。例如：https://api.example.com/usershttps://api.example.com/postshttps://api.example.com/posts/1/comments 仓库(Store) 123456仓库是客户端来管理的一个资源库，客户端可以向仓库中新增资源或者删除资源。客户端也可以批量获取到某个仓库下的所有资源。仓库中的资源对外的访问不会提供单独URI的，客户端在创建资源时候的URI除外。例如：PUT /users/1234/favorites/posts/1 上面的例子我们可以理解为，我们向一个id是1234的用户的仓库(收藏夹)中，添加了一个id为1的post资源。通俗点儿说：就是用户收藏了一个自己喜爱的id为1的文章。 控制器(Controller) 12345678控制器资源模型，可以执行一个方法，支持参数输入，结果返回。 是为了除了标准操作:增删改查(CRUD)以外的一些逻辑操作。控制器(方法)一般定义子URI中末尾，并且不会有子资源(控制器)。例如：向用户重发ID为245743的消息POST /alerts/245743/resend 发布ID为1的文章POST /posts/1/publish 把动作转换成资源 123456789把动作转换成可以执行 CRUD 操作的资源， github 就是用了这种方法。比如“喜欢”一个 gist，就增加一个 /gists/:id/star 子资源，然后对其进行操作：“喜欢”使用 PUT /gists/:id/star，“取消喜欢”使用 DELETE /gists/:id/star或者使用 POST /gists/:id/unstar另外一个例子是 Fork，这也是一个动作，但是在 gist 下面增加 forks资源，就能把动作变成 CRUD 兼容的：POST /gists/:id/forks 可以执行用户 fork 的动作。 URI命名规范 文档(Document)类型的资源用名词(短语)单数命名 集合(Collection)类型的资源用名词(短语)复数命名 仓库(Store)类型的资源用名词(短语)复数命名 控制器(Controller)类型的资源用动词(短语)命名 URI中有些字段可以是变量，在实际使用中可以按需替换 123例如一个资源URI可以这样定义：https://api.example.com/posts/&#123;postId&#125;/comments/&#123;commentId&#125;postId,commentId 是变量(数字，字符串都类型都可以)。 CRUD的操作不要体现在URI中，HTTP协议中的操作符已经对CRUD做了映射。 123456789CRUD是创建，读取，更新，删除这四个经典操作的简称 例如删除的操作用REST规范执行的话，应该是这个样子：DELETE /users/1234以下是几个错误的示例：GET /deleteUser?id=1234 GET /deleteUser/1234 DELETE /deleteUser/1234 POST /users/1234/delete URI的query字段在REST中,query字段一般作为查询的参数补充，也可以帮助标示一个唯一的资源。但需要注意的是，作为一个提供查询功能的URI，无论是否有query条件，我们都应该保证结果的唯一性，一个URI对应的返回数据是不应该被改变的(在资源没有修改的情况下)。HTTP中的缓存也可能缓存查询结果。 Query参数可以作为Collection或Store类型资源的过滤条件来使用 例如： 123GET /users //返回所有用户列表 GET /users?role=admin //返回权限为admin的用户列表GET /search/users?q=&#123;query&#125;&#123;&amp;page,per_page,sort,order&#125; //根据多条件查询用户 Query参数可以作为Collection或Store资源列表分页标示使用 1234567891011121314如果是一个简单的列表操作，可以这样设计：GET /users?pageSize=25&amp;pageStartIndex=50 如果是一个复杂的列表或查询操作的话，我们可以为资源设计一个Collection，因为复杂查询可能会涉及比较多的参数，建议使用Post的方式传入，例如这样：POST /users/search相关的分页信息还可以存放到 Link 头部，这样客户端可以直接得到诸如下一页、最后一页、上一页等内容的 url 地址Status: 200 OKLink: &lt;https://api.github.com/resource?page=2&gt;; rel=&quot;previous&quot;, &lt;https://api.github.com/resource?page=2&gt;; rel=&quot;next&quot;, &lt;https://api.github.com/resource?page=5&gt;; rel=&quot;last&quot;X-RateLimit-Limit: 20X-RateLimit-Remaining: 19 HTTP请求方法的使用 GET方法用来获取资源 PUT方法可用来新增/更新Store类型的资源 PUT方法可用来更新一个资源的全部属性，使用时传递所有属性的值，即使有的值没有改变 PATCH方法更新资源的部分属性。因为 PATCH 比较新，而且规范比较复杂，所以真正实现的比较少，一般都是用 POST 替代 POST方法可用来创建一个资源 POST方法可用来触发执行一个Controller类型资源 DELETE方法用于删除资源 HTTP响应状态码的使用 200 (“OK”) 用于一般性的成功返回 200 (“OK”) 不可用于请求错误返回 201 (“Created”) 资源被创建 202 (“Accepted”) 用于Controller控制类资源异步处理的返回，仅表示请求已经收到。对于耗时比较久的处理，一般用异步处理来完成 204 (“No Content”) 此状态可能会出现在PUT、POST、DELETE的请求中，一般表示资源存在，但消息体中不会返回任何资源相关的状态或信息。 301 (“Moved Permanently”) 资源的URI被转移，需要使用新的URI访问 302 (“Found”) 不推荐使用，此代码在HTTP1.1协议中被303/307替代。我们目前对302的使用和最初HTTP1.0定义的语意是有出入的，应该只有在GET/HEAD方法下，客户端才能根据Location执行自动跳转，而我们目前的客户端基本上是不会判断原请求方法的，无条件的执行临时重定向 303 (“See Other”) 返回一个资源地址URI的引用，但不强制要求客户端获取该地址的状态(访问该地址) 304 (“Not Modified”) 有一些类似于204状态，服务器端的资源与客户端最近访问的资源版本一致，并无修改，不返回资源消息体。可以用来降低服务端的压力 307 (“Temporary Redirect”) 目前URI不能提供当前请求的服务，临时性重定向到另外一个URI。在HTTP1.1中307是用来替代早期HTTP1.0中使用不当的302 400 (“Bad Request”) 用于客户端一般性错误返回, 在其它4xx错误以外的错误，也可以使用400，具体错误信息可以放在body中 401 (“Unauthorized”) 在访问一个需要验证的资源时，验证错误 403 (“Forbidden”) 一般用于非验证性资源访问被禁止，例如对于某些客户端只开放部分API的访问权限，而另外一些API可能无法访问时，可以给予403状态 404 (“Not Found”) 找不到URI对应的资源 405 (“Method Not Allowed”) HTTP的方法不支持，例如某些只读资源，可能不支持POST/DELETE。但405的响应header中必须声明该URI所支持的方法 406 (“Not Acceptable”) 客户端所请求的资源数据格式类型不被支持，例如客户端请求数据格式为application/xml，但服务器端只支持application/json 409 (“Conflict”) 资源状态冲突，例如客户端尝试删除一个非空的Store资源 412 (“Precondition Failed”) 用于有条件的操作不被满足时 415 (“Unsupported Media Type”) 客户所支持的数据类型，服务端无法满足 429 (“Too Many Requests”) 客户端在规定的时间里发送了太多请求，在进行限流的时候会用到 500 (“Internal Server Error”) 服务器端的接口错误，此错误于客户端无关 HTTP Headers Content-Type 标示body的数据格式 Content-Length body 数据体的大小，客户端可以根据此标示检验读取到的数据是否完整，也可以通过Header判断是否需要下载可能较大的数据体 Last-Modified 用于服务器端的响应，是一个资源最后被修改的时间戳，客户端(缓存)可以根据此信息判断是否需要重新获取该资源 ETag 服务器端资源版本的标示，客户端(缓存)可以根据此信息判断是否需要重新获取该资源，需要注意的是，ETag如果通过服务器随机生成，可能会存在多个主机对同一个资源产生不同ETag的问题 Store类型的资源要支持有条件的PUT请求 1234567891011121314假设有两个客户端client#1/#2都向一个Store资源提交PUT请求，服务端是无法清楚的判断是要insert还是要update的，所以我们要在header中加入条件标示if-Match，If-Unmodified-Since来明确是本次调用API的意图。例如：client#1第一次向服务端发起一个请求 PUT /objects/2113 此时2113资源还不存在，那服务端会认为本次请求是一个insert操作，完成后，会返回 201 (“Created”)client#2再一次向服务端发起同一个请求 PUT /objects/2113 时，因2113资源已存在，服务端会返回 409 (“Conflict”)为了能让client#2的请求成功，或者说我们要清楚的表明本次操作是一次update操作，我们必须在header中加入一些条件标示，例如 if-Match。我们需要给出资源的ETag(if-Match:Etag)，来表明我们希望更新资源的版本，如果服务端版本一致，会返回200 (“OK”) 或者 204 (“No Content”)。如果服务端发现指定的版本与当前资源版本不一致，会返回 412 (“Precondition Failed”) Location 在响应header中使用，一般为客户端感兴趣的资源URI,例如在成功创建一个资源后，我们可以把新的资源URI放在Location中，如果是一个异步创建资源的请求，接口在响应202 (“Accepted”)的同时可以给予客户端一个异步状态查询的地址 Cache-Control, Expires, Date 通过缓存机制提升接口响应性能,同时根据实际需要也可以禁止客户端对接口请求做缓存。对于REST接口来说，如果某些接口实时性要求不高的情况下，我们可以使用max-age来指定一个小的缓存时间，这样对客户端和服务器端双方都是有利的。一般来说只对GET方法且返回200的情况下使用缓存，在某些情况下我们也可以对返回3xx或者4xx的情况下做缓存，可以防范错误访问带来的负载。 我们可以自定义一些头信息，作为客户端和服务器间的通信使用，但不能改变HTTP方法的性质。自定义头尽量简单明了，不要用body中的信息对其作补充说明。 API 地址和版本在 url 中指定 API 的版本是个很好地做法。如果 API 变化比较大，可以把 API 设计为子域名，比如 https://api.github.com/v3；也可以简单地把版本放在路径中，比如 https://example.com/api/v1。另一种做法是，将版本号放在HTTP头信息中。 限流 rate limit如果对访问的次数不加控制，很可能会造成 API 被滥用，甚至被 DDos 攻击。根据使用者不同的身份对其进行限流，可以防止这些情况，减少服务器的压力。 对用户的请求限流之后，要有方法告诉用户它的请求使用情况，Github API 使用的三个相关的头部： X-RateLimit-Limit: 用户每个小时允许发送请求的最大值 X-RateLimit-Remaining：当前时间窗口剩下的可用请求数目 X-RateLimit-Rest: 时间窗口重置的时候，到这个时间点可用的请求数量就会变成 X-RateLimit-Limit 的值 对于超过流量的请求，可以返回 429 Too many requests 状态码，并附带错误信息。 参考文档 http://cizixs.com/2016/12/12/restful-api-design-guide http://wangwei.info/about-rest-api/ http://www.ruanyifeng.com/blog/2011/09/restful.html http://www.ruanyifeng.com/blog/2014/05/restful_api.html https://zh.wikipedia.org/wiki/REST https://developer.github.com/v3 http://novoland.github.io/%E8%AE%BE%E8%AE%A1/2015/08/17/Restful%20API%20%E7%9A%84%E8%AE%BE%E8%AE%A1%E8%A7%84%E8%8C%83.html]]></content>
      <categories>
        <category>Web开发</category>
      </categories>
      <tags>
        <tag>REST</tag>
        <tag>API</tag>
        <tag>Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[codis3系列版本安装]]></title>
    <url>%2Fposts%2F2%2F</url>
    <content type="text"><![CDATA[简介Codis 是 Wandoujia Infrastructure Team 开发的一个分布式 Redis 服务,用户可以看成是一个无限内存的 Redis 服务, 有动态扩/缩容的能力. 对偏存储型的业务更实用,如果你需要 SUBPUB 之类的指令, Codis 是不支持的. 时刻记住 Codis 是一个分布式存储的项目.对于海量的 key, value不太大( &lt;= 1M ), 随着业务扩展缓存也要随之扩展的业务场景有特效. Codis 3.x 由以下组件组成： Codis Server：基于 redis-3.2.8 分支开发。增加了额外的数据结构，以支持 slot 有关的操作以及数据迁移指令。具体的修改可以参考文档 redis 的修改。 Codis Proxy：客户端连接的 Redis 代理服务, 实现了 Redis 协议。 除部分命令不支持以外(不支持的命令列表)，表现的和原生的 Redis 没有区别（就像 Twemproxy）。 对于同一个业务集群而言，可以同时部署多个 codis-proxy 实例； 不同 codis-proxy 之间由 codis-dashboard 保证状态同步。 Codis Dashboard：集群管理工具，支持 codis-proxy、codis-server 的添加、删除，以及据迁移等操作。在集群状态发生改变时，codis-dashboard 维护集群下所有codis-proxy 的状态的一致性。 对于同一个业务集群而言，同一个时刻 codis-dashboard 只能有 0个或者1个； 所有对集群的修改都必须通过 codis-dashboard 完成。 Codis Admin：集群管理的命令行工具。 可用于控制 codis-proxy、codis-dashboard 状态以及访问外部存储。 Codis FE：集群管理界面。 多个集群实例共享可以共享同一个前端展示页面； 通过配置文件管理后端 codis-dashboard 列表，配置文件可自动更新。 Codis HA：为集群提供高可用。 依赖 codis-dashboard 实例，自动抓取集群各个组件的状态； 会根据当前集群状态自动生成主从切换策略，并在需要时通过 codis-dashboard 完成主从切换。 Storage：为集群状态提供外部存储。 提供 Namespace 概念，不同集群的会按照不同 product name 进行组织； 目前仅提供了 Zookeeper、Etcd、Fs 三种实现，但是提供了抽象的 interface 可自行扩展。 安装配置 zookeeper(单机启动，生产环境需要集群)1.安装zookeeper1234567yum install -y java-1.8.0-openjdkcd /server/softwarewget http://mirrors.hust.edu.cn/apache/zookeeper/zookeeper-3.5.1-alpha/zookeeper-3.5.1-alpha.tar.gztar xf zookeeper-3.5.1-alpha.tar.gzmv zookeeper-3.5.1-alpha zookeepermv zookeeper /usr/local/chown root.root /usr/local/zookeeper -R 2.配置zookeeper123456789101112mkdir -pv /data/zookeeper/&#123;data,log&#125;cat &gt;/usr/local/zookeeper/conf/zoo.cfg&lt;&lt;EOFclientPort=2181maxClientCnxns=1024tickTime=2000initLimit=20syncLimit=10dataDir=/data/zookeeper/datadataLogDir=/data/zookeeper/logEOF 3.启动zookeeper12345#启动/usr/local/zookeeper/bin/zkServer.sh start#查看节点的状态/usr/local/zookeeper/bin/zkServer.sh status 安装 codis下载二进制包123456789cd /server/softwarewget https://github.com/CodisLabs/codis/releases/download/3.1.3/codis3.1.3-go1.7.4-linux.tar.gztar xf codis3.1.3-go1.7.4-linux.tar.gzcd codis3.1.3-go1.7.4-linuxmkdir bin etcmv codis-* redis-* bin/cd ..mv codis3.1.3-go1.7.4-linux /usr/local/codischown root.root /usr/local/codis -R 启动 Codis Dashboard 配置 123456cd /usr/local/codis# 生成默认的配置文件./bin/codis-dashboard --default-config &gt; etc/dashboard.toml# 修改配置参数 启动 12nohup ./bin/codis-dashboard --ncpu=4 --config=etc/dashboard.toml \ --log=dashboard.log --log-level=WARN &amp; 启动 Codis Proxy 配置 123456cd /usr/local/codis# 生成默认的配置文件./bin/codis-proxy --default-config &gt; etc/proxy.toml# 修改配置参数 启动 12nohup ./bin/codis-proxy --ncpu=4 --config=etc/proxy.toml \ --log=proxy.log --log-level=WARN &amp; 启动 Codis Server 配置 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849mkdir -pv /data/redis/6379cat &gt;/usr/local/codis/etc/redis_6379.conf&lt;&lt;EOFdaemonize yesbind `ifconfig eth1 | grep "inet "| head -n 1 | awk -F'[: ]+' '&#123;print $3&#125;'`port 6379timeout 300loglevel noticelogfile "/data/redis/6379/redis.log"databases 16dbfilename dump.rdb#save 900 1#save 300 10#save 60 10000dir "/data/redis/6379"maxclients 10000#下面两项一般配置使用maxmemory 1024MB#内存不足时的清楚策略maxmemory-policy allkeys-lru#开启日志记录，相当于MySQL的binlog#appendonly yes #appendfilename "appendonly.aof"#appendfsync everysecEOFmkdir -pv /data/redis/6380cat &gt;/usr/local/codis/etc/redis_6380.conf&lt;&lt;EOFdaemonize yesbind `ifconfig eth1 | grep "inet "| head -n 1 | awk -F'[: ]+' '&#123;print $3&#125;'`port 6380timeout 300loglevel noticelogfile "/data/redis/6380/redis.log"databases 16dbfilename dump.rdb#save 900 1#save 300 10#save 60 10000dir "/data/redis/6380"maxclients 10000#下面两项一般配置使用maxmemory 1024MB#内存不足时的清楚策略maxmemory-policy allkeys-lru#开启日志记录，相当于MySQL的binlog#appendonly yes #appendfilename "appendonly.aof"#appendfsync everysecEOF 启动 1234567# 启动/usr/local/codis/bin/codis-server /usr/local/codis/etc/redis_6379.conf/usr/local/codis/bin/codis-server /usr/local/codis/etc/redis_6380.conf#测试ps -ef | grep codis-servernetstat -tunlp | grep 63 启动 Codis FE（可选组件） 配置 1234cd /usr/local/codis# 生成配置文件./bin/codis-admin --dashboard-list --zookeeper=lab1:2181 &gt; etc/codis.json 启动 1234cd /usr/local/codisnohup ./bin/codis-fe --ncpu=4 --log=fe.log --log-level=WARN \ --dashboard-list=etc/codis.json --assets-dir=/usr/local/codis/assets \ --listen=192.168.12.211:8090 &amp; 启动 Codis HA（可选组件） 启动 1nohup ./bin/codis-ha --log=ha.log --log-level=WARN --dashboard=192.168.12.211:18080 &amp; Codis Admin（命令行工具） codis-dashboard 异常退出的修复 1./bin/codis-admin --remove-lock --product=codis-famulei --zookeeper=lab1:2181 codis-proxy 异常退出的修复 12# 确认 codis-proxy 进程已经退出（很重要）./bin/codis-admin --dashboard=127.0.0.1:18080 --remove-proxy --addr=127.0.0.1:11080 --force 添加启动配置的 codis-server web页面方式添加 12# 访问如下页面http://192.168.12.211:8090/ 添加完成后还需要点击如下图标生成主从关系 命令行添加 123456789101112131415# 添加组./bin/codis-admin --dashboard=127.0.0.1:18080 --create-group --gid=2./bin/codis-admin --dashboard=127.0.0.1:18080 --create-group --gid=3# 把 codis-server 添加到指定组./bin/codis-admin --dashboard=127.0.0.1:18080 --group-add --gid=2 --addr=192.168.12.212:6379./bin/codis-admin --dashboard=127.0.0.1:18080 --group-add --gid=2 --addr=192.168.12.212:6380./bin/codis-admin --dashboard=127.0.0.1:18080 --group-add --gid=3 --addr=192.168.12.213:6379./bin/codis-admin --dashboard=127.0.0.1:18080 --group-add --gid=3 --addr=192.168.12.213:6380# 设置同步状态./bin/codis-admin --dashboard=127.0.0.1:18080 --sync-action --create \--addr=192.168.12.212:6379./bin/codis-admin --dashboard=127.0.0.1:18080 --sync-action --create \--addr=192.168.12.212:6380 分配 slots 到 group web页面方式添加 12# 访问如下页面http://192.168.12.211:8090/ 也可以直接使用如下按键快速分配 命令行添加 1./bin/codis-admin --dashboard=127.0.0.1:18080 --slot-action --create-range --beg=600 --end=1023 --gid=3 上线 proxyproxy启动之后需要上线才能使用 web页面方式添加 12# 访问如下页面http://192.168.12.211:8090/ 命令行添加 1./bin/codis-admin --dashboard=127.0.0.1:18080 --online-proxy --addr=192.168.88.211:11080 测试 基本测试 1./bin/redis-cli -h 192.168.12.211 -p 19000 性能测试 1./bin/redis-benchmark -h 192.168.12.211 -p 19000 参考文档 https://github.com/CodisLabs/codis/blob/release3.1/doc/tutorial_zh.md]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>codis</tag>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pika安装配置]]></title>
    <url>%2Fposts%2F1%2F</url>
    <content type="text"><![CDATA[简介Pika是一个可持久化的大容量redis存储服务，兼容string、hash、list、zset、set的绝大接口(兼容详情)，解决redis由于存储数据量巨大而导致内存不够用的容量瓶颈，并且可以像redis一样，通过slaveof命令进行主从备份，支持全同步和部分同步，pika还可以用在twemproxy或者codis中来实现静态数据分片（pika已经可以支持codis的动态迁移slot功能） 编译安装12345678910111213141516171819202122232425262728293031# 安装依赖yum install -y snappy-devel bz2 libzip-dev libsnappy-dev libprotobuf-dev \libevent-dev protobuf-compiler libgoogle-glog-dev protobuf-devel \libevent-devel bzip2-devel libbz2-dev zlib-devel gcc-c++# 查看gcc版本gcc -v# 如果不4.8需要先安装切换到4.8sudo rpm --import http://ftp.scientificlinux.org/linux/scientific/5x/x86_64/RPM-GPG-KEYs/RPM-GPG-KEY-cernsudo wget http://people.centos.org/tru/devtools-2/devtools-2.repo -O /etc/yum.repos.d/devtools-2.reposudo yum install -y devtoolset-2-gcc devtoolset-2-binutils devtoolset-2-gcc-c++scl enable devtoolset-2 bash# 下载源码git clone --recursive https://github.com/Qihoo360/pika.git &amp;&amp; cd pika# 编译make __REL=1 -j4# 若编译过程中，提示有依赖的库没有安装，则有提示安装后再重新编译# 安装cp -r output /usr/local/pika# 配置库echo '/usr/local/pika/lib' &gt; /etc/ld.so.conf.d/pika.confldconfig -v# 测试/usr/local/pika/bin/pika -v 配置运行12345678910111213141516171819202122232425262728293031323334353637383940414243# 配置mkdir -pv /data/pikamv /usr/local/pika/conf/pika.conf /usr/local/pika/conf/pika.conf.oricat &gt;/usr/local/pika/conf/pika.conf&lt;&lt;EOFport : 9221thread-num : 1sync-thread-num : 6sync-buffer-size : 10log-path : /data/pika/log/loglevel : infodb-path : /data/pika/db/write-buffer-size : 268435456timeout : 60requirepass :masterauth :userpass :userblacklist :dump-prefix :daemonize : yesdump-path : /data/pika/dump/pidfile : /data/pika/pika.pidmaxclients : 20000target-file-size-base : 20971520expire-logs-days : 7expire-logs-nums : 10root-connection-num : 2slowlog-log-slower-than : 10000slave-read-only : 0db-sync-path : /data/pika/dbsync/db-sync-speed : -1binlog-file-size : 104857600compression : snappymax-background-flushes : 1max-background-compactions : 2max-cache-files : 5000max-bytes-for-level-multiplier : 10EOF# 启动/usr/local/pika/bin/pika -c /usr/local/pika/conf/pika.conf# 检测端口netstat -tunlp | grep 9221 测试123456789# 基本测试yum install -y redisredis-cli -h 127.0.0.1 -p 9221set will mgxget will# 性能测试redis-benchmark -h 127.0.0.1 -p 9221 -n 1000000 -t set,get \-r 10000000000 -c 120 -d 200 参考文档 https://github.com/Qihoo360/pika/blob/master/README_CN.md https://github.com/Qihoo360/pika/wiki/pika-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%B4%E6%98%8E]]></content>
      <categories>
        <category>运维</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>NoSQL</tag>
        <tag>pika</tag>
      </tags>
  </entry>
</search>
